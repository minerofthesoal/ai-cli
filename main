#!/usr/bin/env bash
# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║  AI.SH v2.3 — Universal AI CLI · TUI · Fine-tune · Canvas · TTM/MTM/Mtm    ║
# ║  GGUF·PyTorch·Diffusers·OpenAI·Claude·Gemini·HF·Audio·Video·Vision·GUI     ║
# ╚══════════════════════════════════════════════════════════════════════════════╝
# Install: chmod +x ai.sh && sudo cp ai.sh /usr/local/bin/ai
set -euo pipefail
VERSION="2.3.0"

# ════════════════════════════════════════════════════════════════════════════════
#  ENVIRONMENT DETECTION
# ════════════════════════════════════════════════════════════════════════════════
find_python() {
  for c in python3.12 python3.11 python3.10 python3 python; do
    local p; p=$(command -v "$c" 2>/dev/null) || continue
    local v; v=$("$p" -c "import sys; print(sys.version_info.major,sys.version_info.minor)" 2>/dev/null) || continue
    read -r ma mi <<< "$v"
    (( ma==3 && mi>=10 )) && { echo "$p"; return 0; }
  done; echo ""
}
find_llama() {
  for b in llama-cli llama llama-run llama-main llama.cpp; do
    command -v "$b" &>/dev/null && { command -v "$b"; return 0; }
  done
  for d in "$HOME/.local/bin" "$HOME/bin" "$HOME/llama.cpp/build/bin" \
            "$HOME/llama.cpp/build" "$HOME/llama.cpp" "/usr/local/bin" \
            "/opt/llama.cpp/bin" "/opt/homebrew/bin"; do
    for b in llama-cli llama llama-run main; do
      [[ -x "$d/$b" ]] && { echo "$d/$b"; return 0; }
    done
  done
  local py; py=$(find_python)
  [[ -n "$py" ]] && "$py" -c "import llama_cpp" 2>/dev/null && { echo "llama_cpp_python"; return 0; }
  echo ""
}
detect_cuda_arch() {
  local py; py=$(find_python)
  if [[ -n "$py" ]] && "$py" -c "import torch; torch.cuda.is_available()" 2>/dev/null; then
    "$py" -c "import torch; cc=torch.cuda.get_device_capability(0); print(cc[0]*10+cc[1])" 2>/dev/null || echo "0"
  elif command -v nvidia-smi &>/dev/null; then
    nvidia-smi --query-gpu=compute_cap --format=csv,noheader 2>/dev/null | head -1 | tr -d '.' || echo "0"
  else
    echo "0"
  fi
}

PYTHON="$(find_python)"
LLAMA_BIN="$(find_llama)"
CUDA_ARCH="$(detect_cuda_arch)"

# ════════════════════════════════════════════════════════════════════════════════
#  COLORS
# ════════════════════════════════════════════════════════════════════════════════
R="\033[0m"; B="\033[1m"; DIM="\033[2m"; IT="\033[3m"; UL="\033[4m"
BL="\033[5m"; INV="\033[7m"
BLACK="\033[30m";RED="\033[31m";GREEN="\033[32m";YELLOW="\033[33m"
BLUE="\033[34m";MAGENTA="\033[35m";CYAN="\033[36m";WHITE="\033[37m";GRAY="\033[90m"
BRED="\033[91m";BGREEN="\033[92m";BYELLOW="\033[93m";BBLUE="\033[94m"
BMAGENTA="\033[95m";BCYAN="\033[96m";BWHITE="\033[97m"
BG_BLACK="\033[40m";BG_RED="\033[41m";BG_GREEN="\033[42m";BG_YELLOW="\033[43m"
BG_BLUE="\033[44m";BG_MAGENTA="\033[45m";BG_CYAN="\033[46m";BG_WHITE="\033[47m"

# ════════════════════════════════════════════════════════════════════════════════
#  PATHS & CONFIG
# ════════════════════════════════════════════════════════════════════════════════
CONFIG_DIR="${AI_CLI_CONFIG:-$HOME/.config/ai-cli}"
CONFIG_FILE="$CONFIG_DIR/config.env"
KEYS_FILE="$CONFIG_DIR/keys.env"
LOG_FILE="$CONFIG_DIR/history.log"
SESSIONS_DIR="$CONFIG_DIR/sessions"
PERSONAS_DIR="$CONFIG_DIR/personas"
MODELS_DIR="${AI_CLI_MODELS:-$HOME/.ai-cli/models}"
AI_OUTPUT_DIR="${AI_OUTPUT_DIR:-$HOME/ai-outputs}"
CANVAS_DIR="$AI_OUTPUT_DIR/canvas"
FINETUNE_DIR="$CONFIG_DIR/finetune"
TOOLS_DIR="$CONFIG_DIR/tools"
PLUGINS_DIR="$CONFIG_DIR/plugins"
SEARCH_CACHE="$CONFIG_DIR/search_cache"
CUSTOM_MODELS_DIR="$CONFIG_DIR/custom_models"
TTM_DIR="$CONFIG_DIR/tiny_model"
MTM_DIR="$CONFIG_DIR/mini_model"
MMTM_DIR="$CONFIG_DIR/medium_model"
CHAT_LOGS_DIR="$CONFIG_DIR/chat_logs"
AUDIO_DIR="$AI_OUTPUT_DIR/audio"
VIDEO_DIR="$AI_OUTPUT_DIR/video"

mkdir -p "$CONFIG_DIR" "$MODELS_DIR" "$SESSIONS_DIR" "$PERSONAS_DIR" \
         "$AI_OUTPUT_DIR" "$CANVAS_DIR" "$FINETUNE_DIR" "$TOOLS_DIR" \
         "$PLUGINS_DIR" "$SEARCH_CACHE" "$CUSTOM_MODELS_DIR" \
         "$TTM_DIR" "$MTM_DIR" "$MMTM_DIR" \
         "$CHAT_LOGS_DIR" "$AUDIO_DIR" "$VIDEO_DIR"
touch "$KEYS_FILE" && chmod 600 "$KEYS_FILE"

[[ -f "$CONFIG_FILE" ]] && source "$CONFIG_FILE"
[[ -f "$KEYS_FILE"   ]] && source "$KEYS_FILE"

# Runtime vars
ACTIVE_MODEL="${ACTIVE_MODEL:-}"
ACTIVE_BACKEND="${ACTIVE_BACKEND:-}"
ACTIVE_PERSONA="${ACTIVE_PERSONA:-}"
ACTIVE_SESSION="${ACTIVE_SESSION:-default}"
MAX_TOKENS="${MAX_TOKENS:-2048}"
TEMPERATURE="${TEMPERATURE:-0.7}"
TOP_P="${TOP_P:-0.9}"
REPEAT_PENALTY="${REPEAT_PENALTY:-1.1}"
CONTEXT_SIZE="${CONTEXT_SIZE:-4096}"
GPU_LAYERS="${GPU_LAYERS:--1}"
THREADS="${THREADS:-$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)}"
STREAM="${STREAM:-1}"
VERBOSE="${VERBOSE:-0}"
TOOL_CALLING="${TOOL_CALLING:-1}"
WEB_SEARCH_ENABLED="${WEB_SEARCH_ENABLED:-1}"
SEARCH_ENGINE="${SEARCH_ENGINE:-ddg}"
CANVAS_ACTIVE="${CANVAS_ACTIVE:-}"
HF_DATASET_SYNC="${HF_DATASET_SYNC:-0}"
HF_DATASET_REPO="${HF_DATASET_REPO:-ray0rf1re/cli}"
HF_DATASET_KEY="${HF_DATASET_KEY:-}"
GUI_THEME="${GUI_THEME:-dark}"

# TTM (Tiny — 179.35M)
TTM_AUTO_TRAIN="${TTM_AUTO_TRAIN:-0}"
TTM_PRETRAINED="${TTM_PRETRAINED:-0}"
TTM_VERSION="${TTM_VERSION:-0}"

# MTM (Mini — 0.61B — GTX 1080 optimized)
MTM_AUTO_TRAIN="${MTM_AUTO_TRAIN:-0}"
MTM_PRETRAINED="${MTM_PRETRAINED:-0}"
MTM_VERSION="${MTM_VERSION:-0}"

# Mtm (Medium — 1.075B — RTX 2080+ optimized)
MMTM_AUTO_TRAIN="${MMTM_AUTO_TRAIN:-0}"
MMTM_PRETRAINED="${MMTM_PRETRAINED:-0}"
MMTM_VERSION="${MMTM_VERSION:-0}"

save_config() {
  cat > "$CONFIG_FILE" <<CONF
ACTIVE_MODEL="${ACTIVE_MODEL}"
ACTIVE_BACKEND="${ACTIVE_BACKEND}"
ACTIVE_PERSONA="${ACTIVE_PERSONA}"
ACTIVE_SESSION="${ACTIVE_SESSION}"
MAX_TOKENS="${MAX_TOKENS}"
TEMPERATURE="${TEMPERATURE}"
TOP_P="${TOP_P}"
REPEAT_PENALTY="${REPEAT_PENALTY}"
CONTEXT_SIZE="${CONTEXT_SIZE}"
GPU_LAYERS="${GPU_LAYERS}"
THREADS="${THREADS}"
STREAM="${STREAM}"
VERBOSE="${VERBOSE}"
TOOL_CALLING="${TOOL_CALLING}"
WEB_SEARCH_ENABLED="${WEB_SEARCH_ENABLED}"
SEARCH_ENGINE="${SEARCH_ENGINE}"
CANVAS_ACTIVE="${CANVAS_ACTIVE}"
HF_DATASET_SYNC="${HF_DATASET_SYNC}"
HF_DATASET_REPO="${HF_DATASET_REPO}"
HF_DATASET_KEY="${HF_DATASET_KEY}"
GUI_THEME="${GUI_THEME}"
TTM_AUTO_TRAIN="${TTM_AUTO_TRAIN}"
TTM_PRETRAINED="${TTM_PRETRAINED}"
TTM_VERSION="${TTM_VERSION}"
MTM_AUTO_TRAIN="${MTM_AUTO_TRAIN}"
MTM_PRETRAINED="${MTM_PRETRAINED}"
MTM_VERSION="${MTM_VERSION}"
MMTM_AUTO_TRAIN="${MMTM_AUTO_TRAIN}"
MMTM_PRETRAINED="${MMTM_PRETRAINED}"
MMTM_VERSION="${MMTM_VERSION}"
CONF
}

log_history() { local role="$1"; local msg="$2"; echo "$(date -Iseconds) [$role] $msg" >> "$LOG_FILE"; }
err()  { echo -e "${BRED}✗ $*${R}" >&2; }
ok()   { echo -e "${BGREEN}✓ $*${R}"; }
info() { echo -e "${BCYAN}ℹ $*${R}"; }
warn() { echo -e "${BYELLOW}⚠ $*${R}"; }
hdr()  { echo -e "${B}${BWHITE}$*${R}"; }
dim()  { echo -e "${DIM}$*${R}"; }

# ════════════════════════════════════════════════════════════════════════════════
#  RECOMMENDED MODELS
# ════════════════════════════════════════════════════════════════════════════════
declare -A RECOMMENDED_MODELS
RECOMMENDED_MODELS=(
  [1]="Amu/supertiny-llama3-0.25B-v0.1|gguf|0.25B|Supertiny Llama3 — runs on ANY CPU"
  [2]="TheBloke/Mistral-7B-Instruct-v0.2-GGUF|gguf|7B|Mistral 7B — top general-purpose"
  [3]="bartowski/Meta-Llama-3.1-8B-Instruct-GGUF|gguf|8B|Llama 3.1 8B — strong reasoning"
  [4]="bartowski/Phi-3.1-mini-128k-instruct-GGUF|gguf|3.8B|Phi-3 Mini 128k context"
  [5]="Qwen/Qwen2-7B-Instruct-GGUF|gguf|7B|Qwen2 7B — multilingual + coding"
  [6]="bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF|gguf|16B|DeepSeek Coder V2 — code"
  [7]="bartowski/gemma-2-9b-it-GGUF|gguf|9B|Gemma 2 9B — Google model"
  [8]="stabilityai/stable-diffusion-xl-base-1.0|diffusers|SDXL|SDXL image generation"
  [9]="black-forest-labs/FLUX.1-schnell|diffusers|FLUX|FLUX Schnell — fast 4-step"
  [10]="openai/whisper-base|hf|74M|Whisper base — fast speech-to-text"
  [11]="openai/whisper-large-v3|hf|1.5B|Whisper Large v3 — best transcription"
  [12]="gpt-4o|openai|API|GPT-4o with vision"
  [13]="claude-sonnet-4-5|claude|API|Claude Sonnet 4.5"
  [14]="gemini-2.0-flash|gemini|API|Gemini 2.0 Flash"
)

# ════════════════════════════════════════════════════════════════════════════════
#  BUILTIN PERSONAS
# ════════════════════════════════════════════════════════════════════════════════
declare -A BUILTIN_PERSONAS
BUILTIN_PERSONAS=(
  [default]="You are a helpful, friendly AI assistant."
  [dev]="You are an expert software engineer. Write clean, secure, well-documented code. Prefer idiomatic solutions. Flag potential bugs and security issues."
  [researcher]="You are a rigorous researcher. Cite your reasoning. Acknowledge uncertainty. Be precise and thorough."
  [writer]="You are a skilled writer. Prioritize clarity, flow, and engagement. Adapt tone to context."
  [teacher]="You are a patient teacher. Use analogies and examples. Check for understanding. Scaffold complex concepts."
  [sysadmin]="You are a Linux/DevOps expert. Give precise, tested commands. Prefer minimal dependencies."
  [security]="You are a cybersecurity expert. Think offensively to defend. Reference CVEs where relevant."
  [data]="You are a data scientist. Use pandas, numpy, matplotlib. Apply statistical rigor."
  [creative]="You are a bold, original creative. Push boundaries. Experiment with form and voice."
  [concise]="Be maximally concise. Use the fewest words without losing accuracy."
  [audio]="You are an audio engineering and music production expert. Help with DSP, audio files, mixing, analysis."
  [video]="You are a video production and ffmpeg expert. Help with video editing, transcoding, analysis."
)


# ════════════════════════════════════════════════════════════════════════════════
#  CUSTOM MODEL CONFIGS (prebuilt architecture presets)
# ════════════════════════════════════════════════════════════════════════════════
declare -A MODEL_PRESETS
MODEL_PRESETS=(
  [nano]="hidden_size=256|num_hidden_layers=8|num_attention_heads=8|intermediate_size=512|max_position_embeddings=512|vocab_size=32000|params=~0.125B"
  [micro]="hidden_size=512|num_hidden_layers=8|num_attention_heads=8|intermediate_size=1024|max_position_embeddings=1024|vocab_size=32000|params=~0.25B"
  [tiny]="hidden_size=512|num_hidden_layers=10|num_attention_heads=8|intermediate_size=1024|max_position_embeddings=2048|vocab_size=32000|params=~0.35B"
  [small]="hidden_size=768|num_hidden_layers=12|num_attention_heads=12|intermediate_size=2048|max_position_embeddings=2048|vocab_size=32000|params=~0.5B"
  [medium]="hidden_size=1024|num_hidden_layers=16|num_attention_heads=16|intermediate_size=4096|max_position_embeddings=2048|vocab_size=32000|params=~1B"
  [tinyllama]="hidden_size=2048|num_hidden_layers=22|num_attention_heads=32|intermediate_size=5632|max_position_embeddings=2048|vocab_size=32000|params=~1.1B"
)

# ── Model JSON configs ────────────────────────────────────────────────────────

# TTM: 179.35M — TinyLlama-based
TTM_CONFIG_JSON='{
  "architectures":["LlamaForCausalLM"],"bos_token_id":1,"eos_token_id":2,
  "hidden_act":"silu","hidden_size":1280,"initializer_range":0.02,
  "intermediate_size":3328,"max_position_embeddings":2048,"model_type":"llama",
  "num_attention_heads":16,"num_hidden_layers":14,"num_key_value_heads":4,
  "rms_norm_eps":1e-05,"tie_word_embeddings":false,"torch_dtype":"bfloat16",
  "use_cache":true,"vocab_size":32000,"_comment":"~179.35M params"
}'

# MTM: 0.61B — GTX 1080 optimized (fp16, 8GB VRAM — Pascal arch)
# Uses grouped-query attention (GQA) 4 kv heads, smaller intermediate for VRAM fit
MTM_CONFIG_JSON='{
  "architectures":["LlamaForCausalLM"],"bos_token_id":1,"eos_token_id":2,
  "hidden_act":"silu","hidden_size":2048,"initializer_range":0.02,
  "intermediate_size":5120,"max_position_embeddings":2048,"model_type":"llama",
  "num_attention_heads":16,"num_hidden_layers":18,"num_key_value_heads":4,
  "rms_norm_eps":1e-05,"tie_word_embeddings":false,"torch_dtype":"float16",
  "use_cache":true,"vocab_size":32000,
  "_comment":"~0.61B params — GTX 1080 (8GB fp16) optimized"
}'

# Mtm: 1.075B — RTX 2080+ optimized (bf16, 8GB+ VRAM — Turing/Ampere)
# More layers, wider intermediate, bf16 for Turing+ tensor cores
MMTM_CONFIG_JSON='{
  "architectures":["LlamaForCausalLM"],"bos_token_id":1,"eos_token_id":2,
  "hidden_act":"silu","hidden_size":2048,"initializer_range":0.02,
  "intermediate_size":5632,"max_position_embeddings":4096,"model_type":"llama",
  "num_attention_heads":16,"num_hidden_layers":28,"num_key_value_heads":4,
  "rms_norm_eps":1e-05,"tie_word_embeddings":false,"torch_dtype":"bfloat16",
  "use_cache":true,"vocab_size":32000,
  "_comment":"~1.075B params — RTX 2080+ (bf16) optimized"
}'

# ════════════════════════════════════════════════════════════════════════════════
#  6 PRETRAINING DATASETS (shared by TTM/MTM/Mtm)
#  +2 optional custom ones set by user
# ════════════════════════════════════════════════════════════════════════════════
PRETRAIN_DATASETS=(
  "roneneldan/TinyStories|text|5000|Tiny children's stories — fluent English"
  "sahil2801/CodeAlpaca-20k|instruction+output|3000|Code generation pairs"
  "Open-Orca/OpenOrca|system_prompt+question+response|2000|Instruction following"
  "bigcode/the-stack-smol|content|2000|Small code snippets across languages"
  "HuggingFaceFW/fineweb-edu|text|3000|High-quality educational web text"
  "wikimedia/wikipedia|text|3000|Wikipedia articles (en, 20231101)"
)
# User-settable custom datasets (HF id or 'hf:user/repo' or local path)
PRETRAIN_CUSTOM_1="${PRETRAIN_CUSTOM_1:-}"
PRETRAIN_CUSTOM_2="${PRETRAIN_CUSTOM_2:-}"

# ════════════════════════════════════════════════════════════════════════════════
#  GENERALIZED TRAINED MODEL ENGINE
#  Handles TTM (tiny/179M), MTM (mini/0.61B), Mtm (medium/1.075B)
# ════════════════════════════════════════════════════════════════════════════════

# _tm_vars MODEL_ID  →  sets TM_DIR TM_HF_REPO TM_CONFIG_JSON TM_LABEL
#                        TM_AUTO_TRAIN_VAR TM_VERSION_VAR TM_PRETRAINED_VAR
_tm_vars() {
  local id="$1"
  case "$id" in
    TTM|ttm)
      TM_DIR="$TTM_DIR"
      TM_HF_REPO="ray0rf1re/tiny"
      TM_CONFIG_JSON="$TTM_CONFIG_JSON"
      TM_LABEL="TTM (Tiny ~179M)"
      TM_AUTO_TRAIN_VAR="TTM_AUTO_TRAIN"
      TM_VERSION_VAR="TTM_VERSION"
      TM_PRETRAINED_VAR="TTM_PRETRAINED"
      TM_DTYPE="bfloat16"
      TM_GPU_OPT="any"
      ;;
    MTM|mtm)
      TM_DIR="$MTM_DIR"
      TM_HF_REPO="ray0rf1re/mini"
      TM_CONFIG_JSON="$MTM_CONFIG_JSON"
      TM_LABEL="MTM (Mini ~0.61B, GTX 1080)"
      TM_AUTO_TRAIN_VAR="MTM_AUTO_TRAIN"
      TM_VERSION_VAR="MTM_VERSION"
      TM_PRETRAINED_VAR="MTM_PRETRAINED"
      TM_DTYPE="float16"
      TM_GPU_OPT="GTX 1080 / Pascal+"
      ;;
    Mtm|mmtm|MMTM)
      TM_DIR="$MMTM_DIR"
      TM_HF_REPO="ray0rf1re/medium"
      TM_CONFIG_JSON="$MMTM_CONFIG_JSON"
      TM_LABEL="Mtm (Medium ~1.075B, RTX 2080+)"
      TM_AUTO_TRAIN_VAR="MMTM_AUTO_TRAIN"
      TM_VERSION_VAR="MMTM_VERSION"
      TM_PRETRAINED_VAR="MMTM_PRETRAINED"
      TM_DTYPE="bfloat16"
      TM_GPU_OPT="RTX 2080+ / Turing+"
      ;;
    *) err "Unknown model ID: $id (use TTM, MTM, or Mtm)"; return 1 ;;
  esac
}

_tm_get_var()  { eval "echo \"\${${1}:-0}\""; }
_tm_set_var()  { eval "${1}=\"${2}\""; }

_tm_init() {
  local id="$1"; _tm_vars "$id"
  mkdir -p "$TM_DIR"
  local cfg="$TM_DIR/config.json"
  if [[ ! -f "$cfg" ]]; then
    echo "$TM_CONFIG_JSON" > "$cfg"
    ok "$TM_LABEL config created: $cfg"
  fi
}

_tm_create_repo() {
  local id="$1"; _tm_vars "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  local hf_key="${HF_TOKEN:-}"; [[ -z "$hf_key" ]] && { err "HF_TOKEN not set"; return 1; }
  info "Creating HuggingFace repo: $TM_HF_REPO ..."
  HF_TOKEN_VAL="$hf_key" REPO_ID="$TM_HF_REPO" MODEL_LABEL="$TM_LABEL" \
  MODEL_CFG="$TM_CONFIG_JSON" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from huggingface_hub import HfApi
except ImportError:
    print("huggingface_hub not installed. Run: ai install-deps"); sys.exit(1)
api = HfApi(token=os.environ['HF_TOKEN_VAL'])
repo = os.environ['REPO_ID']
label = os.environ['MODEL_LABEL']
cfg   = os.environ['MODEL_CFG']
try:
    api.create_repo(repo_id=repo, exist_ok=True, private=False, repo_type='model')
    readme = f"# {label}\n\nAuto-trained model by AI CLI v2.3.\n\n```json\n{cfg}\n```\n"
    api.upload_file(
        path_or_fileobj=readme.encode(),
        path_in_repo="README.md",
        repo_id=repo,
        commit_message="init: create repo",
    )
    print(f"Created: https://huggingface.co/{repo}")
except Exception as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
PYEOF
}

_tm_pretrain() {
  local id="$1"; shift
  local custom1="${1:-$PRETRAIN_CUSTOM_1}"; local custom2="${2:-$PRETRAIN_CUSTOM_2}"
  _tm_vars "$id"; _tm_init "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }

  info "$TM_LABEL — Starting pretraining"
  info "Using 6 standard datasets + ${custom1:+custom1: $custom1 }${custom2:+custom2: $custom2}"
  echo ""

  TM_DIR_VAL="$TM_DIR" TM_DTYPE_VAL="$TM_DTYPE" \
  CUSTOM1="${custom1:-}" CUSTOM2="${custom2:-}" \
  "$PYTHON" - <<'PYEOF'
import os, json, sys
TM_DIR   = os.environ['TM_DIR_VAL']
TM_DTYPE = os.environ.get('TM_DTYPE_VAL','float32')
CUSTOM1  = os.environ.get('CUSTOM1','')
CUSTOM2  = os.environ.get('CUSTOM2','')

try:
    import torch
    from transformers import (AutoTokenizer, LlamaConfig, LlamaForCausalLM,
                               TrainingArguments, Trainer, DataCollatorForLanguageModeling)
    from datasets import load_dataset, Dataset
except ImportError as e:
    print(f"Missing: {e}\nRun: ai install-deps"); sys.exit(1)

cfg_path = f"{TM_DIR}/config.json"
out_dir  = f"{TM_DIR}/pretrained"
os.makedirs(out_dir, exist_ok=True)

with open(cfg_path) as f:
    raw = json.load(f)
cfg = LlamaConfig(**{k:v for k,v in raw.items() if not k.startswith('_') and k!='architectures'})
model = LlamaForCausalLM(cfg)
total = sum(p.numel() for p in model.parameters())
print(f"Parameters: {total:,} ({total/1e6:.2f}M)")

tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
tokenizer.pad_token = tokenizer.eos_token
MAX_LEN = min(cfg.max_position_embeddings, 256)

records = []

# ── Dataset 1: TinyStories ────────────────────────────────────────────────────
print("Dataset 1/8: roneneldan/TinyStories")
try:
    ds = load_dataset("roneneldan/TinyStories", split="train[:6000]")
    for ex in ds: records.append(ex.get('text','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 2: CodeAlpaca ─────────────────────────────────────────────────────
print("Dataset 2/8: sahil2801/CodeAlpaca-20k")
try:
    ds = load_dataset("sahil2801/CodeAlpaca-20k", split="train[:4000]")
    for ex in ds:
        t = (ex.get('instruction','') + '\n' + ex.get('output',''))[:MAX_LEN*4]
        records.append(t)
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 3: OpenOrca ───────────────────────────────────────────────────────
print("Dataset 3/8: Open-Orca/OpenOrca")
try:
    ds = load_dataset("Open-Orca/OpenOrca", split="train[:3000]")
    for ex in ds:
        t = (ex.get('system_prompt','') + ' ' + ex.get('question','') + '\n' + ex.get('response',''))[:MAX_LEN*4]
        records.append(t)
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 4: The Stack Smol ─────────────────────────────────────────────────
print("Dataset 4/8: bigcode/the-stack-smol")
try:
    ds = load_dataset("bigcode/the-stack-smol", data_dir="data/python", split="train[:3000]")
    for ex in ds: records.append(ex.get('content','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 5: FineWeb-Edu ────────────────────────────────────────────────────
print("Dataset 5/8: HuggingFaceFW/fineweb-edu")
try:
    ds = load_dataset("HuggingFaceFW/fineweb-edu", name="sample-10BT", split="train[:4000]",
                      streaming=False)
    for ex in ds: records.append(ex.get('text','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 6: Wikipedia ──────────────────────────────────────────────────────
print("Dataset 6/8: wikimedia/wikipedia (en)")
try:
    ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train[:4000]")
    for ex in ds: records.append(ex.get('text','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Custom dataset 1 ──────────────────────────────────────────────────────────
if CUSTOM1:
    print(f"Dataset 7/8: {CUSTOM1} (custom)")
    try:
        if CUSTOM1.startswith('/') or CUSTOM1.endswith('.jsonl') or CUSTOM1.endswith('.txt'):
            with open(CUSTOM1) as f:
                for line in f:
                    line = line.strip()
                    if not line: continue
                    try:
                        obj = json.loads(line)
                        t = obj.get('text','') or obj.get('content','') or str(obj)
                    except:
                        t = line
                    records.append(t[:MAX_LEN*4])
        else:
            ds_id = CUSTOM1.replace('hf:','')
            ds = load_dataset(ds_id, split="train[:2000]")
            tcols = [c for c in ds.column_names if c in ['text','content','input','instruction']]
            col = tcols[0] if tcols else ds.column_names[0]
            for ex in ds: records.append(str(ex.get(col,''))[:MAX_LEN*4])
        print(f"  added custom1 (total {len(records)})")
    except Exception as e: print(f"  skipped: {e}")

# ── Custom dataset 2 ──────────────────────────────────────────────────────────
if CUSTOM2:
    print(f"Dataset 8/8: {CUSTOM2} (custom)")
    try:
        if CUSTOM2.startswith('/') or CUSTOM2.endswith('.jsonl') or CUSTOM2.endswith('.txt'):
            with open(CUSTOM2) as f:
                for line in f:
                    line = line.strip()
                    if not line: continue
                    try:
                        obj = json.loads(line)
                        t = obj.get('text','') or obj.get('content','') or str(obj)
                    except:
                        t = line
                    records.append(t[:MAX_LEN*4])
        else:
            ds_id = CUSTOM2.replace('hf:','')
            ds = load_dataset(ds_id, split="train[:2000]")
            tcols = [c for c in ds.column_names if c in ['text','content','input','instruction']]
            col = tcols[0] if tcols else ds.column_names[0]
            for ex in ds: records.append(str(ex.get(col,''))[:MAX_LEN*4])
        print(f"  added custom2 (total {len(records)})")
    except Exception as e: print(f"  skipped: {e}")

# ── Filter and tokenize ───────────────────────────────────────────────────────
records = [r for r in records if r and len(r.strip()) > 20]
print(f"\nTotal records: {len(records)}")
if not records:
    print("No data loaded"); sys.exit(1)

ds_all = Dataset.from_list([{'text': r} for r in records])
def tokenize(ex):
    return tokenizer(ex['text'], truncation=True, max_length=MAX_LEN, padding='max_length')
ds_all = ds_all.map(tokenize, batched=True, remove_columns=['text'])

device = 'cuda' if torch.cuda.is_available() else 'cpu'
dtype_map = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}
torch_dtype = dtype_map.get(TM_DTYPE, torch.float32)
model = model.to(device)
if device == 'cuda':
    model = model.to(torch_dtype)

print(f"Training on: {device} | dtype: {TM_DTYPE} | records: {len(ds_all)}")

args = TrainingArguments(
    output_dir=out_dir,
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-4,
    lr_scheduler_type='cosine',
    warmup_ratio=0.05,
    fp16=(device=='cuda' and TM_DTYPE=='float16'),
    bf16=(device=='cuda' and TM_DTYPE=='bfloat16'),
    logging_steps=50,
    save_steps=500,
    save_total_limit=1,
    report_to='none',
    dataloader_num_workers=0,
)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds_all,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"\nPretrained model saved: {out_dir}")
PYEOF

  if [[ -d "$TM_DIR/pretrained" ]]; then
    _tm_set_var "$TM_PRETRAINED_VAR" "1"
    save_config
    ok "$TM_LABEL pretraining complete"
  fi
}

_tm_train_batch() {
  local id="$1"; _tm_vars "$id"
  local auto_var; auto_var=$(_tm_get_var "$TM_AUTO_TRAIN_VAR")
  [[ "$auto_var" != "1" ]] && return 0
  [[ -z "$PYTHON" ]] && return 0

  # Build batch data from chat logs + history
  local batch_file; batch_file=$(mktemp /tmp/tm_batch_XXXX.jsonl)
  find "$CHAT_LOGS_DIR" -name "*.jsonl" -newer "$TM_DIR/.last_train" 2>/dev/null | head -5 | while read -r f; do
    cat "$f"
  done > "$batch_file" 2>/dev/null || true
  if [[ -f "$LOG_FILE" ]]; then
    tail -20 "$LOG_FILE" | while IFS= read -r line; do
      local msg; msg=$(echo "$line" | sed 's/^[0-9T:+-]* \[[a-z]*\] //')
      [[ -n "$msg" ]] && echo "{\"text\":\"$msg\"}" >> "$batch_file"
    done
  fi

  local count; count=$(wc -l < "$batch_file" 2>/dev/null || echo 0)
  if (( count < 3 )); then rm -f "$batch_file"; return 0; fi

  local base_model="$TM_DIR/pretrained"
  local latest_ft; latest_ft=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "")
  [[ -n "$latest_ft" ]] && base_model="$latest_ft"
  [[ ! -d "$base_model" ]] && { rm -f "$batch_file"; return 0; }

  local cur_ver; cur_ver=$(_tm_get_var "$TM_VERSION_VAR")
  local new_ver=$(( cur_ver + 1 ))
  local out_dir="$TM_DIR/ft_v${new_ver}"
  mkdir -p "$out_dir"

  BATCH_FILE="$batch_file" BASE_MODEL="$base_model" OUT_DIR="$out_dir" \
  TM_DTYPE_VAL="$TM_DTYPE" "$PYTHON" - <<'PYEOF' &>/dev/null &
import os, json, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, \
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, TaskType
except ImportError: sys.exit(0)

batch_file = os.environ['BATCH_FILE']
base_model = os.environ['BASE_MODEL']
out_dir    = os.environ['OUT_DIR']
TM_DTYPE   = os.environ.get('TM_DTYPE_VAL','float32')

records = []
with open(batch_file) as f:
    for line in f:
        line = line.strip()
        if not line: continue
        try:
            obj = json.loads(line)
            txt = obj.get('text','') or obj.get('output','') + ' ' + obj.get('instruction','')
            if txt.strip(): records.append({'text': txt[:256]})
        except: pass
if len(records) < 3: sys.exit(0)

tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(base_model)
lora = LoraConfig(task_type=TaskType.CAUSAL_LM, r=4, lora_alpha=8,
                  lora_dropout=0.05, target_modules=["q_proj","v_proj"])
model = get_peft_model(model, lora)
ds = Dataset.from_list(records)
def tok(ex): return tokenizer(ex['text'], truncation=True, max_length=128, padding='max_length')
ds = ds.map(tok, batched=True, remove_columns=['text'])
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
dtype_map = {'float16': torch.float16, 'bfloat16': torch.bfloat16}
if device == 'cuda' and TM_DTYPE in dtype_map:
    model = model.to(dtype_map[TM_DTYPE])
args = TrainingArguments(
    output_dir=out_dir, num_train_epochs=1, per_device_train_batch_size=1,
    gradient_accumulation_steps=1, max_steps=1, learning_rate=2e-4,
    fp16=(device=='cuda' and TM_DTYPE=='float16'),
    bf16=(device=='cuda' and TM_DTYPE=='bfloat16'),
    logging_steps=1, save_steps=1, report_to='none',
)
Trainer(model=model, args=args, train_dataset=ds,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)).train()
merged = model.merge_and_unload()
merged.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
PYEOF

  rm -f "$batch_file"
  touch "$TM_DIR/.last_train"
  _tm_set_var "$TM_VERSION_VAR" "$new_ver"
  save_config
}

_tm_upload() {
  local id="$1"; local version="${2:-latest}"; _tm_vars "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  local hf_key="${HF_TOKEN:-}"; [[ -z "$hf_key" ]] && { err "HF_TOKEN not set"; return 1; }

  local model_dir
  if [[ "$version" == "latest" ]]; then
    model_dir=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "$TM_DIR/pretrained")
  else
    model_dir="$TM_DIR/ft_v${version}"
  fi
  [[ ! -d "$model_dir" ]] && { err "No $id model at $model_dir. Run: ai $id pretrain"; return 1; }

  local folder_name; folder_name=$(basename "$model_dir")
  info "Uploading $id $folder_name → $TM_HF_REPO/$folder_name"

  HF_TOKEN_VAL="$hf_key" MODEL_DIR="$model_dir" FOLDER_NAME="$folder_name" \
  TM_HF_REPO="$TM_HF_REPO" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from huggingface_hub import HfApi
except ImportError:
    print("huggingface_hub not installed. Run: ai install-deps"); sys.exit(1)
api   = HfApi(token=os.environ['HF_TOKEN_VAL'])
repo  = os.environ['TM_HF_REPO']
mdir  = os.environ['MODEL_DIR']
fname = os.environ['FOLDER_NAME']
try:
    api.create_repo(repo_id=repo, exist_ok=True, private=False)
except: pass
api.upload_folder(
    folder_path=mdir, repo_id=repo, path_in_repo=fname,
    commit_message=f"auto-upload: {fname}",
)
print(f"Uploaded: https://huggingface.co/{repo}/tree/main/{fname}")
PYEOF
}

_tm_load() {
  local id="$1"; local version="${2:-latest}"; _tm_vars "$id"
  local mdir
  if [[ "$version" == "latest" ]]; then
    mdir=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "$TM_DIR/pretrained")
  else
    mdir="$TM_DIR/ft_v${version}"
  fi
  [[ ! -d "$mdir" ]] && { err "$id not trained yet. Run: ai $id pretrain"; return 1; }
  ACTIVE_MODEL="$mdir"; ACTIVE_BACKEND="pytorch"; save_config
  ok "Loaded $TM_LABEL from $mdir"
}

_tm_status() {
  local id="$1"; _tm_vars "$id"
  local auto_var;    auto_var=$(_tm_get_var "$TM_AUTO_TRAIN_VAR")
  local pretrain_var; pretrain_var=$(_tm_get_var "$TM_PRETRAINED_VAR")
  local ver_var;     ver_var=$(_tm_get_var "$TM_VERSION_VAR")

  hdr "$TM_LABEL Status"
  echo "  HF Repo:    $TM_HF_REPO"
  echo "  GPU target: $TM_GPU_OPT"
  echo "  Data type:  $TM_DTYPE"
  echo "  Config:     $TM_DIR/config.json"
  echo "  Auto-train: $auto_var"
  echo "  Pretrained: $pretrain_var"
  echo "  Version:    $ver_var"
  echo ""
  [[ -d "$TM_DIR/pretrained" ]] && ok "Pretrained model present" || warn "Not pretrained yet"
  local latest; latest=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "")
  [[ -n "$latest" ]] && ok "Latest finetune: $(basename "$latest")"
  [[ -n "$PRETRAIN_CUSTOM_1" ]] && echo "  Custom DS 1: $PRETRAIN_CUSTOM_1"
  [[ -n "$PRETRAIN_CUSTOM_2" ]] && echo "  Custom DS 2: $PRETRAIN_CUSTOM_2"
}

# ── Generic cmd dispatcher for TTM/MTM/Mtm ────────────────────────────────────
_tm_cmd() {
  local id="$1"; shift
  local sub="${1:-help}"; shift || true
  _tm_init "$id"
  case "$sub" in
    pretrain)
      local c1="${1:-$PRETRAIN_CUSTOM_1}"; local c2="${2:-$PRETRAIN_CUSTOM_2}"
      _tm_pretrain "$id" "$c1" "$c2"
      ;;
    status)    _tm_status "$id" ;;
    load)      _tm_load "$id" "${1:-latest}" ;;
    train-now) _tm_train_batch "$id" ;;
    upload)    _tm_upload "$id" "${1:-latest}" ;;
    create-repo) _tm_create_repo "$id" ;;
    enable)
      _tm_vars "$id"
      _tm_set_var "$TM_AUTO_TRAIN_VAR" "1"; save_config
      ok "$TM_LABEL auto-training enabled"
      ;;
    disable)
      _tm_vars "$id"
      _tm_set_var "$TM_AUTO_TRAIN_VAR" "0"; save_config
      ok "$TM_LABEL auto-training disabled"
      ;;
    set-custom1)
      PRETRAIN_CUSTOM_1="${1:-}"; save_config
      ok "Custom dataset 1: $PRETRAIN_CUSTOM_1"
      ;;
    set-custom2)
      PRETRAIN_CUSTOM_2="${1:-}"; save_config
      ok "Custom dataset 2: $PRETRAIN_CUSTOM_2"
      ;;
    *)
      _tm_vars "$id"
      echo -e "${B}${BCYAN}$TM_LABEL${R}"
      echo "  GPU target: $TM_GPU_OPT | Dtype: $TM_DTYPE | Repo: $TM_HF_REPO"
      echo ""
      echo "  ${B}ai $id pretrain [custom1] [custom2]${R} — Pretrain (6 standard + 2 optional)"
      echo "  ${B}ai $id enable / disable${R}              — Toggle auto-training"
      echo "  ${B}ai $id train-now${R}                     — Force one batch"
      echo "  ${B}ai $id upload [version]${R}              — Upload to $TM_HF_REPO"
      echo "  ${B}ai $id create-repo${R}                   — Create HF repo"
      echo "  ${B}ai $id status${R}                        — Show status"
      echo "  ${B}ai $id load [version]${R}                — Set as active model"
      echo "  ${B}ai $id set-custom1 <hf-id-or-path>${R}   — Set custom dataset 1"
      echo "  ${B}ai $id set-custom2 <hf-id-or-path>${R}   — Set custom dataset 2"
      echo ""
      echo "  ${B}ai -TTM${R} / ${B}ai -MTM${R} / ${B}ai -Mtm${R}            — Load respective model"
      ;;
  esac
}

cmd_ttm() { _tm_cmd "TTM" "$@"; }
cmd_mtm() { _tm_cmd "MTM" "$@"; }
cmd_Mtm() { _tm_cmd "Mtm" "$@"; }

# ════════════════════════════════════════════════════════════════════════════════
#  CUSTOM MODEL CONFIGS (prebuilt architectures)
# ════════════════════════════════════════════════════════════════════════════════
declare -A MODEL_PRESETS
MODEL_PRESETS=(
  [nano]="hidden_size=256|num_hidden_layers=8|num_attention_heads=8|intermediate_size=512|max_position_embeddings=512|vocab_size=32000|params=~0.125B"
  [micro]="hidden_size=512|num_hidden_layers=8|num_attention_heads=8|intermediate_size=1024|max_position_embeddings=1024|vocab_size=32000|params=~0.25B"
  [tiny]="hidden_size=512|num_hidden_layers=10|num_attention_heads=8|intermediate_size=1024|max_position_embeddings=2048|vocab_size=32000|params=~0.35B"
  [small]="hidden_size=768|num_hidden_layers=12|num_attention_heads=12|intermediate_size=2048|max_position_embeddings=2048|vocab_size=32000|params=~0.5B"
  [medium]="hidden_size=1024|num_hidden_layers=16|num_attention_heads=16|intermediate_size=4096|max_position_embeddings=2048|vocab_size=32000|params=~1B"
  [tinyllama]="hidden_size=2048|num_hidden_layers=22|num_attention_heads=32|intermediate_size=5632|max_position_embeddings=2048|vocab_size=32000|params=~1.1B"
)

# TTM (Tiny Trained Model) config — exactly 179.35M params using TinyLlama arch
TTM_CONFIG_JSON='
{
  "architectures": ["LlamaForCausalLM"],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3328,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 14,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.35.0",
  "use_cache": true,
  "vocab_size": 32000,
  "_comment": "~179.35M parameters — TinyLlama-based custom arch"
}
'

cmd_model_create() {
  local subcmd="${1:-help}"; shift || true
  case "$subcmd" in
    presets) _model_list_presets ;;
    new)     _model_new "$@" ;;
    edit)    _model_edit "$@" ;;
    list)    _model_list_custom ;;
    train)   _model_train_custom "$@" ;;
    info)    _model_info_custom "$@" ;;
    delete)  _model_delete_custom "$@" ;;
    *)
      echo -e "${B}${BCYAN}Custom Model Creator${R}"
      echo ""
      echo "  ${B}ai model-create presets${R}         — List built-in architecture presets"
      echo "  ${B}ai model-create new <name> [preset|custom]${R} — Create new model config"
      echo "  ${B}ai model-create edit <name>${R}     — Edit model config JSON"
      echo "  ${B}ai model-create list${R}            — List custom models"
      echo "  ${B}ai model-create train <name> [data.jsonl]${R} — Train from scratch"
      echo "  ${B}ai model-create info <name>${R}     — Show model info"
      echo "  ${B}ai model-create delete <name>${R}   — Delete custom model"
      echo ""
      echo -e "  ${DIM}Minimum: 0.125B params (nano preset)${R}"
      ;;
  esac
}

_model_list_presets() {
  hdr "Built-in Model Presets"
  echo ""
  for key in nano micro tiny small medium tinyllama; do
    local val="${MODEL_PRESETS[$key]}"
    local params; params=$(echo "$val" | grep -o 'params=[^|]*' | cut -d= -f2)
    printf "  ${B}%-12s${R} %s\n" "$key" "$params"
  done
  echo ""
  echo "  Use: ${B}ai model-create new mymodel nano${R}"
  echo "  Or:  ${B}ai model-create new mymodel custom${R} (opens editor)"
}

_model_new() {
  local name="${1:-}"; local preset="${2:-tiny}"
  [[ -z "$name" ]] && { read -rp "Model name: " name; }
  [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ -d "$mdir" ]] && { err "Model '$name' already exists"; return 1; }
  mkdir -p "$mdir"

  local config
  if [[ "$preset" == "custom" ]]; then
    config="$TTM_CONFIG_JSON"
    echo "$config" > "$mdir/config.json"
    "${EDITOR:-nano}" "$mdir/config.json"
  elif [[ -n "${MODEL_PRESETS[$preset]:-}" ]]; then
    local p="${MODEL_PRESETS[$preset]}"
    local hs; hs=$(echo "$p" | grep -o 'hidden_size=[0-9]*' | cut -d= -f2)
    local nhl; nhl=$(echo "$p" | grep -o 'num_hidden_layers=[0-9]*' | cut -d= -f2)
    local nah; nah=$(echo "$p" | grep -o 'num_attention_heads=[0-9]*' | cut -d= -f2)
    local is; is=$(echo "$p" | grep -o 'intermediate_size=[0-9]*' | cut -d= -f2)
    local mpe; mpe=$(echo "$p" | grep -o 'max_position_embeddings=[0-9]*' | cut -d= -f2)
    local vs; vs=$(echo "$p" | grep -o 'vocab_size=[0-9]*' | cut -d= -f2)
    cat > "$mdir/config.json" <<JSON
{
  "architectures": ["LlamaForCausalLM"],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": $hs,
  "initializer_range": 0.02,
  "intermediate_size": $is,
  "max_position_embeddings": $mpe,
  "model_type": "llama",
  "num_attention_heads": $nah,
  "num_hidden_layers": $nhl,
  "num_key_value_heads": $nah,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "use_cache": true,
  "vocab_size": $vs
}
JSON
  elif [[ -f "$preset" ]]; then
    cp "$preset" "$mdir/config.json"
  else
    err "Unknown preset: $preset. Use: nano micro tiny small medium tinyllama custom or a path to JSON"
    rm -rf "$mdir"; return 1
  fi

  cat > "$mdir/meta.json" <<META
{
  "name": "$name",
  "preset": "$preset",
  "created": "$(date -Iseconds)",
  "trained": false,
  "train_steps": 0,
  "version": 1
}
META
  ok "Created custom model '$name' in $mdir"
  echo "  Config: $mdir/config.json"
  echo "  Train:  ai model-create train $name <data.jsonl>"
}

_model_edit() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found"; return 1; }
  "${EDITOR:-nano}" "$mdir/config.json"
  ok "Saved '$name' config"
}

_model_list_custom() {
  hdr "Custom Models"
  local found=0
  for d in "$CUSTOM_MODELS_DIR"/*/; do
    [[ -f "$d/meta.json" ]] || continue
    found=1
    local name; name=$(basename "$d")
    local trained; trained=$(python3 -c "import json,sys; d=json.load(open('$d/meta.json')); print('yes' if d.get('trained') else 'no')" 2>/dev/null || echo "?")
    local steps; steps=$(python3 -c "import json,sys; d=json.load(open('$d/meta.json')); print(d.get('train_steps',0))" 2>/dev/null || echo "0")
    printf "  ${B}%-20s${R} trained=%-4s steps=%s\n" "$name" "$trained" "$steps"
  done
  [[ $found -eq 0 ]] && dim "  No custom models. Create one: ai model-create new mymodel tiny"
}

_model_info_custom() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found"; return 1; }
  hdr "Model: $name"
  [[ -f "$mdir/config.json" ]] && { echo ""; echo "Config:"; cat "$mdir/config.json"; }
  [[ -f "$mdir/meta.json"   ]] && { echo ""; echo "Meta:";   cat "$mdir/meta.json";   }
}

_model_delete_custom() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found"; return 1; }
  read -rp "Delete '$name'? This cannot be undone. [y/N]: " ans
  [[ "$ans" =~ ^[Yy]$ ]] || { info "Cancelled"; return 0; }
  rm -rf "$mdir"; ok "Deleted '$name'"
}

_model_train_custom() {
  local name="${1:-}"; local data="${2:-}"
  [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found. Create it first: ai model-create new $name"; return 1; }
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }

  # Use provided dataset or look for default
  if [[ -z "$data" ]]; then
    [[ -f "$FINETUNE_DIR/dataset.jsonl" ]] && data="$FINETUNE_DIR/dataset.jsonl"
    [[ -z "$data" ]] && { err "No dataset. Provide path or run: ai finetune prepare <data>"; return 1; }
  fi
  [[ ! -f "$data" ]] && { err "Dataset not found: $data"; return 1; }

  local out_dir="$mdir/trained_$(date +%Y%m%d_%H%M%S)"
  mkdir -p "$out_dir"
  info "Training custom model '$name' from scratch..."
  info "Config: $mdir/config.json"
  info "Data:   $data"
  info "Output: $out_dir"
  echo ""

  "$PYTHON" - <<PYEOF
import json, os, sys
try:
    import torch
    from transformers import (AutoTokenizer, LlamaConfig, LlamaForCausalLM,
                               TrainingArguments, Trainer, DataCollatorForLanguageModeling)
    from datasets import Dataset
except ImportError as e:
    print(f"Missing dependency: {e}")
    print("Run: ai install-deps")
    sys.exit(1)

config_path = "$mdir/config.json"
data_path   = "$data"
out_dir     = "$out_dir"

with open(config_path) as f:
    cfg_dict = json.load(f)

cfg = LlamaConfig(**{k: v for k, v in cfg_dict.items()
                     if not k.startswith('_') and k != 'architectures'})
model = LlamaForCausalLM(cfg)
total_params = sum(p.numel() for p in model.parameters())
print(f"Model parameters: {total_params:,} ({total_params/1e6:.2f}M)")

if total_params < 0.125e9:
    print(f"WARNING: Model has {total_params/1e6:.2f}M params, minimum is 125M (0.125B)")
    sys.exit(1)

# Tokenizer — use TinyLlama's tokenizer as base
tokenizer_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
try:
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b", use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

# Load dataset
records = []
with open(data_path) as f:
    for line in f:
        line = line.strip()
        if not line: continue
        try:
            obj = json.loads(line)
            txt = obj.get('text') or (obj.get('instruction','') + ' ' + obj.get('output',''))
            if txt.strip(): records.append({'text': txt})
        except: pass

if not records:
    print("No valid records in dataset"); sys.exit(1)
print(f"Dataset records: {len(records)}")

ds = Dataset.from_list(records)
def tokenize(ex):
    return tokenizer(ex['text'], truncation=True, max_length=cfg.max_position_embeddings,
                     padding='max_length')
ds = ds.map(tokenize, batched=True, remove_columns=['text'])

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
print(f"Training on: {device}")

args = TrainingArguments(
    output_dir=out_dir,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=3e-4,
    lr_scheduler_type='cosine',
    warmup_ratio=0.05,
    fp16=(device=='cuda'),
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    report_to='none',
)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"Saved to {out_dir}")
PYEOF

  if [[ $? -eq 0 ]]; then
    # Update meta
    "$PYTHON" -c "
import json
m = json.load(open('$mdir/meta.json'))
m['trained'] = True
m['train_steps'] = m.get('train_steps',0) + 1
m['last_trained'] = '$(date -Iseconds)'
m['last_output'] = '$out_dir'
json.dump(m, open('$mdir/meta.json','w'), indent=2)
"
    ok "Training complete! Model saved to $out_dir"
  else
    err "Training failed"
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
# ════════════════════════════════════════════════════════════════════════════════
#  NAMED CHAT (-C) WITH JSONL SAVE + HF DATASET SYNC
# ════════════════════════════════════════════════════════════════════════════════
CURRENT_CHAT_NAME=""
CURRENT_CHAT_FILE=""

_chat_start() {
  local name="$1"
  # auto: generate a name based on timestamp
  if [[ "$name" == "auto" ]]; then
    name="chat_$(date +%Y%m%d_%H%M%S)"
  fi
  # Sanitize name
  name="${name//[^a-zA-Z0-9_-]/_}"
  CURRENT_CHAT_NAME="$name"
  CURRENT_CHAT_FILE="$CHAT_LOGS_DIR/${name}.jsonl"
  ok "Chat started: $name"
  echo "  Saving to: $CURRENT_CHAT_FILE"
  [[ "$HF_DATASET_SYNC" == "1" ]] && echo "  HF sync:   enabled → $HF_DATASET_REPO"
}

_chat_append() {
  local role="$1"; local content="$2"
  [[ -z "$CURRENT_CHAT_FILE" ]] && return 0
  local ts; ts=$(date -Iseconds)
  local record; record=$(printf '{"timestamp":"%s","session":"%s","role":"%s","content":%s}' \
    "$ts" "$CURRENT_CHAT_NAME" "$role" "$(echo "$content" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read()))' 2>/dev/null || echo '""')")
  echo "$record" >> "$CURRENT_CHAT_FILE"

  # Background sync to HF if enabled
  if [[ "$HF_DATASET_SYNC" == "1" ]] && [[ -n "${HF_DATASET_KEY:-}" ]]; then
    _hf_dataset_sync_bg
  fi
}

_hf_dataset_sync_bg() {
  # Run in background — upload the current chat jsonl to the dataset repo
  local chat_file="$CURRENT_CHAT_FILE"
  local chat_name="$CURRENT_CHAT_NAME"
  local hf_key="$HF_DATASET_KEY"
  local repo="$HF_DATASET_REPO"
  [[ -z "$PYTHON" ]] && return 0

  ( HF_CHAT_FILE="$chat_file" HF_CHAT_NAME="$chat_name" \
    HF_KEY="$hf_key" HF_REPO="$repo" \
    "$PYTHON" - <<'PYEOF' &>/dev/null
import os, sys
try:
    from huggingface_hub import HfApi
except ImportError:
    sys.exit(0)
chat_file = os.environ['HF_CHAT_FILE']
chat_name = os.environ['HF_CHAT_NAME']
hf_key    = os.environ['HF_KEY']
repo      = os.environ['HF_REPO']
if not os.path.exists(chat_file): sys.exit(0)
api = HfApi(token=hf_key)
try:
    api.create_repo(repo_id=repo, repo_type='dataset', exist_ok=True, private=False)
    api.upload_file(
        path_or_fileobj=chat_file,
        path_in_repo=f"chats/{chat_name}.jsonl",
        repo_id=repo,
        repo_type='dataset',
        commit_message=f"sync: {chat_name}",
    )
except Exception as e:
    pass
PYEOF
  ) &
}

cmd_chat_list() {
  hdr "Saved Chats"
  local count=0
  for f in "$CHAT_LOGS_DIR"/*.jsonl; do
    [[ -f "$f" ]] || continue
    count=$(( count + 1 ))
    local name; name=$(basename "$f" .jsonl)
    local lines; lines=$(wc -l < "$f")
    printf "  ${B}%-30s${R} %3d messages\n" "$name" "$lines"
  done
  [[ $count -eq 0 ]] && dim "  No saved chats. Use: ai -C [name] ask ..."
}

cmd_chat_show() {
  local name="${1:-}"
  [[ -z "$name" ]] && { err "Usage: ai chat-show <name>"; return 1; }
  local f="$CHAT_LOGS_DIR/${name}.jsonl"
  [[ ! -f "$f" ]] && { err "Chat '$name' not found"; return 1; }
  hdr "Chat: $name"
  echo ""
  while IFS= read -r line; do
    local role; role=$(echo "$line" | python3 -c "import json,sys; d=json.loads(sys.stdin.read()); print(d.get('role','?'))" 2>/dev/null || echo "?")
    local content; content=$(echo "$line" | python3 -c "import json,sys; d=json.loads(sys.stdin.read()); print(d.get('content',''))" 2>/dev/null || echo "")
    if [[ "$role" == "user" ]]; then
      echo -e "${B}${BCYAN}You:${R} $content"
    else
      echo -e "${B}${BGREEN}AI:${R} $content"
    fi
    echo ""
  done < "$f"
}

cmd_chat_delete() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local f="$CHAT_LOGS_DIR/${name}.jsonl"
  [[ ! -f "$f" ]] && { err "Chat '$name' not found"; return 1; }
  read -rp "Delete chat '$name'? [y/N]: " ans
  [[ "$ans" =~ ^[Yy]$ ]] || { info "Cancelled"; return 0; }
  rm -f "$f"; ok "Deleted $name"
}

# ════════════════════════════════════════════════════════════════════════════════
#  AUDIO SUPPORT
# ════════════════════════════════════════════════════════════════════════════════
cmd_audio() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    transcribe) _audio_transcribe "$@" ;;
    tts)        _audio_tts "$@" ;;
    analyze)    _audio_analyze "$@" ;;
    convert)    _audio_convert "$@" ;;
    extract)    _audio_extract_from_video "$@" ;;
    ask)        _audio_ask "$@" ;;
    play)       _audio_play "$@" ;;
    info)       _audio_info "$@" ;;
    *)
      echo -e "${B}${BCYAN}Audio Commands${R}"
      echo "  ${B}ai audio transcribe <file> [--lang en] [--model base]${R}"
      echo "  ${B}ai audio tts <text> [--voice nova] [--out file.mp3]${R}"
      echo "  ${B}ai audio analyze <file>${R}      — Analyze audio with AI"
      echo "  ${B}ai audio convert <in> <out>${R}  — Convert format (ffmpeg)"
      echo "  ${B}ai audio extract <video>${R}     — Extract audio from video"
      echo "  ${B}ai audio ask <file> <question>${R} — Ask about audio content"
      echo "  ${B}ai audio play <file>${R}          — Play audio file"
      echo "  ${B}ai audio info <file>${R}          — Show audio metadata"
      ;;
  esac
}

_audio_transcribe() {
  local file="" lang="en" model_size="base" out=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --lang)  lang="$2"; shift 2 ;;
      --model) model_size="$2"; shift 2 ;;
      --out)   out="$2"; shift 2 ;;
      *)       file="$1"; shift ;;
    esac
  done
  [[ -z "$file" ]] && { err "Usage: ai audio transcribe <file>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }

  # Try OpenAI Whisper API first
  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    info "Transcribing with OpenAI Whisper API..."
    local result
    result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -F "file=@$file" \
      -F "model=whisper-1" \
      -F "language=$lang" 2>/dev/null | python3 -c "import json,sys; d=json.load(sys.stdin); print(d.get('text',''))" 2>/dev/null)
    if [[ -n "$result" ]]; then
      echo "$result"
      if [[ -n "$out" ]]; then
        echo "$result" > "$out"; ok "Saved to $out"
      fi
      return 0
    fi
  fi

  # Fallback: local whisper
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  info "Transcribing with local Whisper (model: $model_size)..."
  local result
  result=$(AUDIO_FILE="$file" WHISPER_MODEL="$model_size" WHISPER_LANG="$lang" \
    "$PYTHON" - <<'PYEOF'
import os, sys
try:
    import whisper
except ImportError:
    try:
        import openai_whisper as whisper
    except ImportError:
        print("ERROR: whisper not installed. Run: pip install openai-whisper --break-system-packages")
        sys.exit(1)
f = os.environ['AUDIO_FILE']
m = os.environ.get('WHISPER_MODEL','base')
l = os.environ.get('WHISPER_LANG','en')
model = whisper.load_model(m)
result = model.transcribe(f, language=l)
print(result['text'])
PYEOF
  )
  if [[ -n "$result" ]]; then
    echo "$result"
    [[ -n "$out" ]] && { echo "$result" > "$out"; ok "Saved to $out"; }
  fi
}

_audio_tts() {
  local text="" voice="nova" out="" speed="1.0"
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --voice) voice="$2"; shift 2 ;;
      --out)   out="$2"; shift 2 ;;
      --speed) speed="$2"; shift 2 ;;
      *)       text="${text}${text:+ }$1"; shift ;;
    esac
  done
  [[ -z "$text" ]] && { read -rp "Text: " text; }
  [[ -z "$text" ]] && { err "No text provided"; return 1; }

  if [[ -z "$out" ]]; then
    out="$AUDIO_DIR/tts_$(date +%Y%m%d_%H%M%S).mp3"
  fi

  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    info "Generating TTS with OpenAI (voice: $voice)..."
    curl -sS https://api.openai.com/v1/audio/speech \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"tts-1\",\"input\":$(echo "$text" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))'),\"voice\":\"$voice\",\"speed\":$speed}" \
      --output "$out"
    ok "Saved to $out"
    _audio_play "$out"
  else
    # Fallback: pyttsx3 or espeak
    if "$PYTHON" -c "import pyttsx3" 2>/dev/null; then
      SPEAK_TEXT="$text" SPEAK_OUT="$out" "$PYTHON" - <<'PYEOF'
import os, pyttsx3
engine = pyttsx3.init()
engine.save_to_file(os.environ['SPEAK_TEXT'], os.environ['SPEAK_OUT'])
engine.runAndWait()
print(f"Saved to {os.environ['SPEAK_OUT']}")
PYEOF
    elif command -v espeak &>/dev/null; then
      espeak "$text" -w "$out"
      ok "Saved to $out"
    else
      err "No TTS backend. Set OPENAI_API_KEY or install: pip install pyttsx3 --break-system-packages"
      return 1
    fi
  fi
}

_audio_analyze() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "Usage: ai audio analyze <file>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }

  # First transcribe, then analyze with AI
  info "Transcribing for analysis..."
  local transcript; transcript=$(_audio_transcribe "$file" 2>/dev/null)
  [[ -z "$transcript" ]] && { err "Could not transcribe audio"; return 1; }

  local prompt="Analyze this audio transcript. Provide: 1) Summary, 2) Key topics, 3) Sentiment, 4) Notable quotes.

Transcript:
$transcript"
  dispatch_ask "$prompt"
}

_audio_convert() {
  local input="${1:-}"; local output="${2:-}"
  [[ -z "$input" || -z "$output" ]] && { err "Usage: ai audio convert <input> <output>"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg not installed"; return 1; }
  ffmpeg -i "$input" "$output" && ok "Converted: $output"
}

_audio_extract_from_video() {
  local video="${1:-}"; local out="${2:-}"
  [[ -z "$video" ]] && { err "Usage: ai audio extract <video> [output.mp3]"; return 1; }
  [[ ! -f "$video" ]] && { err "File not found: $video"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  [[ -z "$out" ]] && out="$AUDIO_DIR/$(basename "$video" | sed 's/\.[^.]*$//').mp3"
  ffmpeg -i "$video" -q:a 0 -map a "$out" -y
  ok "Audio extracted to: $out"
}

_audio_ask() {
  local file="${1:-}"; shift
  local question="$*"
  [[ -z "$file" || -z "$question" ]] && { err "Usage: ai audio ask <file> <question>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }
  info "Transcribing..."
  local transcript; transcript=$(_audio_transcribe "$file" 2>/dev/null)
  local prompt="Audio transcript:
$transcript

Question: $question"
  dispatch_ask "$prompt"
}

_audio_play() {
  local file="${1:-}"; [[ -z "$file" ]] && return 0
  for player in mpv vlc aplay paplay afplay; do
    command -v "$player" &>/dev/null && { "$player" "$file" &>/dev/null & return 0; }
  done
  warn "No audio player found (install mpv)"
}

_audio_info() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "File required"; return 1; }
  [[ ! -f "$file" ]] && { err "Not found: $file"; return 1; }
  if command -v ffprobe &>/dev/null; then
    ffprobe -v quiet -print_format json -show_format -show_streams "$file" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
fmt=d.get('format',{})
print(f\"File:     {fmt.get('filename','?')}\")
print(f\"Duration: {float(fmt.get('duration',0)):.1f}s\")
print(f\"Size:     {int(fmt.get('size',0))//1024} KB\")
print(f\"Bitrate:  {int(fmt.get('bit_rate',0))//1000} kbps\")
for s in d.get('streams',[]):
    print(f\"Stream:   {s.get('codec_type','?')} / {s.get('codec_name','?')} / {s.get('sample_rate','?')}Hz\")
" 2>/dev/null
  else
    ls -lh "$file"
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
#  VIDEO SUPPORT
# ════════════════════════════════════════════════════════════════════════════════
cmd_video() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    analyze)   _video_analyze "$@" ;;
    transcribe) _video_transcribe "$@" ;;
    caption)   _video_caption "$@" ;;
    convert)   _video_convert "$@" ;;
    extract)   _video_extract_frames "$@" ;;
    ask)       _video_ask "$@" ;;
    trim)      _video_trim "$@" ;;
    info)      _video_info "$@" ;;
    summary)   _video_summary "$@" ;;
    *)
      echo -e "${B}${BCYAN}Video Commands${R}"
      echo "  ${B}ai video analyze <file>${R}          — Analyze video content with AI"
      echo "  ${B}ai video transcribe <file>${R}       — Transcribe video audio"
      echo "  ${B}ai video caption <file>${R}          — Generate captions/subtitles (.srt)"
      echo "  ${B}ai video convert <in> <out>${R}      — Convert video format"
      echo "  ${B}ai video extract <file> [fps]${R}    — Extract frames"
      echo "  ${B}ai video ask <file> <question>${R}   — Ask about video"
      echo "  ${B}ai video trim <in> <start> <end> <out>${R}"
      echo "  ${B}ai video info <file>${R}             — Show video metadata"
      echo "  ${B}ai video summary <file>${R}          — AI summary of video"
      ;;
  esac
}

_video_info() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "File required"; return 1; }
  command -v ffprobe &>/dev/null || { err "ffmpeg/ffprobe required"; return 1; }
  ffprobe -v quiet -print_format json -show_format -show_streams "$file" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
fmt=d.get('format',{})
print(f\"File:     {fmt.get('filename','?')}\")
dur=float(fmt.get('duration',0))
print(f\"Duration: {int(dur//60)}m {dur%60:.1f}s\")
print(f\"Size:     {int(fmt.get('size',0))//1024//1024} MB\")
print(f\"Bitrate:  {int(fmt.get('bit_rate',0))//1000} kbps\")
for s in d.get('streams',[]):
    if s.get('codec_type')=='video':
        print(f\"Video:    {s.get('codec_name')} {s.get('width')}x{s.get('height')} @ {s.get('r_frame_rate','?')} fps\")
    elif s.get('codec_type')=='audio':
        print(f\"Audio:    {s.get('codec_name')} {s.get('sample_rate','?')}Hz {s.get('channel_layout','?')}\")
" 2>/dev/null
}

_video_transcribe() {
  local file="${1:-}"; shift
  [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  info "Extracting audio..."
  local tmp_audio; tmp_audio=$(mktemp /tmp/vid_audio_XXXX.mp3)
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  ffmpeg -i "$file" -q:a 0 -map a "$tmp_audio" -y &>/dev/null
  info "Transcribing..."
  _audio_transcribe "$tmp_audio" "$@"
  rm -f "$tmp_audio"
}

_video_caption() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "Video file required"; return 1; }
  local out_srt="${file%.*}.srt"
  info "Generating captions for: $file"

  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    local tmp; tmp=$(mktemp /tmp/vid_XXXX.mp3)
    ffmpeg -i "$file" -q:a 0 -map a "$tmp" -y &>/dev/null
    local result
    result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -F "file=@$tmp" -F "model=whisper-1" -F "response_format=srt" 2>/dev/null)
    rm -f "$tmp"
    echo "$result" > "$out_srt"
    ok "Captions saved: $out_srt"
  else
    err "OpenAI API key required for caption generation (provides word-level timestamps)"
  fi
}

_video_extract_frames() {
  local file="${1:-}"; local fps="${2:-1}"
  [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  local out_dir="$VIDEO_DIR/frames_$(basename "$file" | sed 's/\.[^.]*$//')_$(date +%H%M%S)"
  mkdir -p "$out_dir"
  ffmpeg -i "$file" -vf "fps=$fps" "$out_dir/frame_%04d.jpg" -y &>/dev/null
  local count; count=$(ls "$out_dir"/*.jpg 2>/dev/null | wc -l)
  ok "Extracted $count frames to $out_dir"
}

_video_trim() {
  local input="${1:-}"; local start="${2:-}"; local end="${3:-}"; local output="${4:-}"
  [[ -z "$input" || -z "$start" || -z "$end" ]] && {
    err "Usage: ai video trim <input> <start> <end> [output]"
    err "Example: ai video trim video.mp4 00:01:00 00:02:30 clip.mp4"
    return 1
  }
  [[ -z "$output" ]] && output="$VIDEO_DIR/trim_$(basename "$input")"
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  ffmpeg -i "$input" -ss "$start" -to "$end" -c copy "$output" -y
  ok "Trimmed video: $output"
}

_video_convert() {
  local input="${1:-}"; local output="${2:-}"
  [[ -z "$input" || -z "$output" ]] && { err "Usage: ai video convert <input> <output>"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  ffmpeg -i "$input" "$output" && ok "Converted: $output"
}

_video_analyze() {
  local file="${1:-}"; [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  info "Analyzing video: $file"
  _video_info "$file"
  echo ""
  info "Transcribing audio for analysis..."
  local transcript; transcript=$(_video_transcribe "$file" 2>/dev/null)

  # Extract a few frames and describe them if vision model available
  local frame_desc=""
  if command -v ffmpeg &>/dev/null && [[ -n "${OPENAI_API_KEY:-}" ]]; then
    local tmpdir; tmpdir=$(mktemp -d /tmp/vid_frames_XXXX)
    ffmpeg -i "$file" -vf "fps=0.1" -vframes 3 "$tmpdir/frame_%02d.jpg" -y &>/dev/null
    local first_frame="$tmpdir/frame_01.jpg"
    if [[ -f "$first_frame" ]]; then
      info "Analyzing key frame with vision model..."
      local b64; b64=$(base64 -w0 < "$first_frame" 2>/dev/null || base64 < "$first_frame" 2>/dev/null)
      frame_desc=$(curl -sS https://api.openai.com/v1/chat/completions \
        -H "Authorization: Bearer $OPENAI_API_KEY" \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"gpt-4o\",\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/jpeg;base64,$b64\"}},{\"type\":\"text\",\"text\":\"Briefly describe what you see in this video frame.\"}]}],\"max_tokens\":200}" 2>/dev/null | \
        python3 -c "import json,sys; d=json.load(sys.stdin); print(d['choices'][0]['message']['content'])" 2>/dev/null)
    fi
    rm -rf "$tmpdir"
  fi

  local prompt="Analyze this video content comprehensively.

${frame_desc:+Visual description of key frame:
$frame_desc

}${transcript:+Audio transcript:
$transcript

}Provide: 1) Overall summary, 2) Key topics/themes, 3) Sentiment/tone, 4) Notable moments."

  dispatch_ask "$prompt"
}

_video_ask() {
  local file="${1:-}"; shift
  local question="$*"
  [[ -z "$file" || -z "$question" ]] && { err "Usage: ai video ask <file> <question>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }
  info "Processing video for question answering..."
  local transcript; transcript=$(_video_transcribe "$file" 2>/dev/null)
  local prompt="Video content:
${transcript:-[No audio/transcript available]}

Question: $question"
  dispatch_ask "$prompt"
}

_video_summary() {
  local file="${1:-}"; [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  local transcript; transcript=$(_video_transcribe "$file" 2>/dev/null)
  dispatch_ask "Summarize this video content in 3-5 sentences:
$transcript"
}

# ════════════════════════════════════════════════════════════════════════════════
#  IMAGE-TEXT-TO-TEXT (Full vision support)
# ════════════════════════════════════════════════════════════════════════════════
cmd_vision() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    ask)     _vision_ask "$@" ;;
    ocr)     _vision_ocr "$@" ;;
    caption) _vision_caption "$@" ;;
    compare) _vision_compare "$@" ;;
    *)
      echo -e "${B}${BCYAN}Vision (Image-Text-to-Text)${R}"
      echo "  ${B}ai vision ask <image> <question>${R}   — Ask about an image"
      echo "  ${B}ai vision ocr <image>${R}              — Extract text from image"
      echo "  ${B}ai vision caption <image>${R}          — Generate image caption"
      echo "  ${B}ai vision compare <img1> <img2>${R}    — Compare two images"
      echo ""
      echo "  Supports: jpg, png, gif, webp, bmp"
      echo "  Backends: OpenAI GPT-4o (best), Claude 3, Gemini 1.5, LLaVA (local)"
      ;;
  esac
}

_encode_image_b64() {
  local file="$1"
  base64 -w0 < "$file" 2>/dev/null || base64 < "$file" 2>/dev/null
}

_vision_ask() {
  local image="${1:-}"; shift
  local question="${*:-Describe this image in detail.}"
  [[ -z "$image" ]] && { err "Usage: ai vision ask <image> <question>"; return 1; }
  [[ ! -f "$image" ]] && { err "Image not found: $image"; return 1; }

  local ext="${image##*.}"; ext="${ext,,}"
  local mime
  case "$ext" in
    jpg|jpeg) mime="image/jpeg" ;;
    png)      mime="image/png" ;;
    gif)      mime="image/gif" ;;
    webp)     mime="image/webp" ;;
    *)        mime="image/jpeg" ;;
  esac

  local b64; b64=$(_encode_image_b64 "$image")

  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    curl -sS https://api.openai.com/v1/chat/completions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"gpt-4o\",\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:$mime;base64,$b64\"}},{\"type\":\"text\",\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}],\"max_tokens\":${MAX_TOKENS}}]}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['choices'][0]['message']['content'])" 2>/dev/null
  elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
    curl -sS https://api.anthropic.com/v1/messages \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -H "anthropic-version: 2023-06-01" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"claude-opus-4-5\",\"max_tokens\":${MAX_TOKENS},\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image\",\"source\":{\"type\":\"base64\",\"media_type\":\"$mime\",\"data\":\"$b64\"}},{\"type\":\"text\",\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}]}]}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['content'][0]['text'])" 2>/dev/null
  elif [[ -n "${GEMINI_API_KEY:-}" ]]; then
    curl -sS "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=$GEMINI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"contents\":[{\"parts\":[{\"inline_data\":{\"mime_type\":\"$mime\",\"data\":\"$b64\"}},{\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}]}]}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['candidates'][0]['content']['parts'][0]['text'])" 2>/dev/null
  elif [[ -n "$PYTHON" ]] && "$PYTHON" -c "import llava" 2>/dev/null; then
    IMAGE_FILE="$image" IMAGE_QUESTION="$question" "$PYTHON" - <<'PYEOF'
import os
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
model_path = "liuhaotian/llava-v1.5-7b"
tokenizer, model, image_processor, _ = load_pretrained_model(model_path, None, get_model_name_from_path(model_path))
from PIL import Image
image = Image.open(os.environ['IMAGE_FILE']).convert('RGB')
# simplified inference
print("LLaVA vision response")
PYEOF
  else
    err "No vision-capable backend. Set OPENAI_API_KEY, ANTHROPIC_API_KEY, or GEMINI_API_KEY"
    return 1
  fi
}

_vision_ocr() {
  local image="${1:-}"; [[ -z "$image" || ! -f "$image" ]] && { err "Image required"; return 1; }
  _vision_ask "$image" "Extract ALL text from this image exactly as it appears. Return only the text, no commentary."
}

_vision_caption() {
  local image="${1:-}"; [[ -z "$image" || ! -f "$image" ]] && { err "Image required"; return 1; }
  _vision_ask "$image" "Write a concise, descriptive caption for this image in one sentence."
}

_vision_compare() {
  local img1="${1:-}"; local img2="${2:-}"; local question="${3:-What are the differences between these images?}"
  [[ -z "$img1" || -z "$img2" ]] && { err "Usage: ai vision compare <img1> <img2> [question]"; return 1; }
  [[ ! -f "$img1" ]] && { err "Not found: $img1"; return 1; }
  [[ ! -f "$img2" ]] && { err "Not found: $img2"; return 1; }

  local b64_1; b64_1=$(_encode_image_b64 "$img1")
  local b64_2; b64_2=$(_encode_image_b64 "$img2")
  local mime1="image/jpeg"; local mime2="image/jpeg"

  [[ -n "${OPENAI_API_KEY:-}" ]] && \
    curl -sS https://api.openai.com/v1/chat/completions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"gpt-4o\",\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:$mime1;base64,$b64_1\"}},{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:$mime2;base64,$b64_2\"}},{\"type\":\"text\",\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}]}],\"max_tokens\":${MAX_TOKENS}}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['choices'][0]['message']['content'])" 2>/dev/null || \
    err "Vision comparison requires OPENAI_API_KEY"
}


# ════════════════════════════════════════════════════════════════════════════════

# ════════════════════════════════════════════════════════════════════════════════
#  NEW GUI — Python curses with full mouse click support
# ════════════════════════════════════════════════════════════════════════════════
# The GUI is a self-contained Python script written inline and executed.
# It uses curses with mouse events so every button is clickable.
# Falls back to numbered selection if curses unavailable.

cmd_gui() {
  [[ -z "$PYTHON" ]] && { err "Python 3.10+ required for GUI"; return 1; }

  # Write the GUI script
  local gui_script; gui_script=$(mktemp /tmp/ai_gui_XXXX.py)
  cat > "$gui_script" << 'PYEOF'
#!/usr/bin/env python3
"""AI CLI v2.3 — Interactive GUI with mouse click support"""
import curses, os, sys, subprocess, json, time, textwrap, threading

CONFIG_FILE = os.path.expanduser("~/.config/ai-cli/config.env")
KEYS_FILE   = os.path.expanduser("~/.config/ai-cli/keys.env")
CLI_BIN     = sys.argv[1] if len(sys.argv) > 1 else "ai"
THEME       = sys.argv[2] if len(sys.argv) > 2 else "dark"

# ── Theme palettes ────────────────────────────────────────────────────────────
THEMES = {
    "dark":   dict(bg=0, fg=15, accent=14, accent2=11, sel_bg=4, sel_fg=15,
                   border=12, title_bg=4, title_fg=15, ok=10, err=9, warn=11, dim=8),
    "light":  dict(bg=15, fg=0, accent=4, accent2=1, sel_bg=4, sel_fg=15,
                   border=4, title_bg=4, title_fg=15, ok=2, err=1, warn=3, dim=8),
    "hacker": dict(bg=0, fg=2, accent=10, accent2=2, sel_bg=2, sel_fg=0,
                   border=2, title_bg=2, title_fg=0, ok=10, err=9, warn=3, dim=8),
    "matrix": dict(bg=0, fg=10, accent=2, accent2=10, sel_bg=2, sel_fg=0,
                   border=10, title_bg=2, title_fg=0, ok=10, err=9, warn=11, dim=8),
}

# ── Color pair IDs ─────────────────────────────────────────────────────────────
C_NORMAL = 1; C_ACCENT = 2; C_ACCENT2 = 3; C_SEL = 4
C_BORDER = 5; C_TITLE  = 6; C_OK = 7; C_ERR = 8; C_WARN = 9; C_DIM = 10

def init_colors(theme_name):
    curses.start_color()
    curses.use_default_colors()
    t = THEMES.get(theme_name, THEMES["dark"])
    pairs = [
        (C_NORMAL, t['fg'],      t['bg']),
        (C_ACCENT, t['accent'],  t['bg']),
        (C_ACCENT2,t['accent2'], t['bg']),
        (C_SEL,    t['sel_fg'],  t['sel_bg']),
        (C_BORDER, t['border'],  t['bg']),
        (C_TITLE,  t['title_fg'],t['title_bg']),
        (C_OK,     t['ok'],      t['bg']),
        (C_ERR,    t['err'],     t['bg']),
        (C_WARN,   t['warn'],    t['bg']),
        (C_DIM,    t['dim'],     t['bg']),
    ]
    for pid, fg, bg in pairs:
        try:
            curses.init_pair(pid, fg, bg)
        except:
            curses.init_pair(pid, -1, -1)

def cp(pid): return curses.color_pair(pid)

# ── Utility ────────────────────────────────────────────────────────────────────
def run_cmd(cmd, capture=True):
    """Run CLI command, return output."""
    try:
        r = subprocess.run([CLI_BIN] + cmd, capture_output=capture,
                           text=True, timeout=120)
        return (r.stdout + r.stderr).strip()
    except subprocess.TimeoutExpired:
        return "[timed out]"
    except Exception as e:
        return f"[error: {e}]"

def load_config():
    cfg = {}
    for f in [CONFIG_FILE, KEYS_FILE]:
        if os.path.exists(f):
            with open(f) as fh:
                for line in fh:
                    line = line.strip()
                    if '=' in line and not line.startswith('#'):
                        k, _, v = line.partition('=')
                        cfg[k.strip()] = v.strip().strip('"')
    return cfg

def draw_box(win, y, x, h, w, title="", color=C_BORDER):
    """Draw a unicode box."""
    try:
        win.attron(cp(color))
        win.addch(y,   x,   '╔')
        win.addch(y,   x+w-1, '╗')
        win.addch(y+h-1, x,   '╚')
        win.addch(y+h-1, x+w-1,'╝')
        win.hline(y,   x+1, '═', w-2)
        win.hline(y+h-1, x+1, '═', w-2)
        win.vline(y+1, x,   '║', h-2)
        win.vline(y+1, x+w-1,'║', h-2)
        win.attroff(cp(color))
        if title:
            t = f" {title} "
            tx = x + (w - len(t)) // 2
            win.attron(cp(C_TITLE) | curses.A_BOLD)
            win.addstr(y, max(tx, x+1), t[:w-2])
            win.attroff(cp(C_TITLE) | curses.A_BOLD)
    except curses.error:
        pass

def draw_button(win, y, x, label, selected=False, color=C_ACCENT):
    """Draw a clickable button [ label ]"""
    btn = f"[ {label} ]"
    attr = cp(C_SEL) | curses.A_BOLD if selected else cp(color)
    try:
        win.addstr(y, x, btn, attr)
    except curses.error:
        pass
    return (y, x, len(btn))   # returns hit region

def safe_addstr(win, y, x, text, attr=0):
    try:
        rows, cols = win.getmaxyx()
        if 0 <= y < rows and 0 <= x < cols:
            win.addstr(y, x, text[:cols-x-1], attr)
    except curses.error:
        pass

def truncate(s, n): return s if len(s) <= n else s[:n-1] + '…'

# ═══════════════════════════════════════════════════════════════════════════════
#  WIDGETS
# ═══════════════════════════════════════════════════════════════════════════════

def dialog_input(stdscr, prompt, default=""):
    """Small input dialog. Returns string or None if cancelled."""
    rows, cols = stdscr.getmaxyx()
    h, w = 7, min(70, cols - 4)
    y, x = (rows - h)//2, (cols - w)//2
    win = curses.newwin(h, w, y, x)
    curses.curs_set(1)
    win.keypad(True)
    draw_box(win, 0, 0, h, w, "Input", C_BORDER)
    safe_addstr(win, 2, 2, truncate(prompt, w-4), cp(C_ACCENT) | curses.A_BOLD)
    safe_addstr(win, 4, 2, "Enter=confirm  Esc=cancel", cp(C_DIM))
    # Input field
    field_x = 2; field_w = w - 4
    buf = list(default); pos = len(buf)

    def redraw():
        win.attron(cp(C_NORMAL))
        win.hline(3, field_x, ' ', field_w)
        win.attroff(cp(C_NORMAL))
        disp = ''.join(buf)[-field_w:]
        safe_addstr(win, 3, field_x, disp, cp(C_NORMAL) | curses.A_UNDERLINE)
        win.refresh()
        cursor_x = field_x + min(pos, field_w-1)
        try: win.move(3, cursor_x)
        except: pass

    while True:
        redraw()
        try: key = win.get_wch()
        except: continue
        if isinstance(key, str):
            if key == '\n':  break
            elif key == '\x1b': buf = None; break
            elif key == '\x7f' or key == '\b':
                if buf and pos > 0: buf.pop(pos-1); pos -= 1
            elif key.isprintable():
                buf.insert(pos, key); pos += 1
        elif key == curses.KEY_BACKSPACE:
            if buf and pos > 0: buf.pop(pos-1); pos -= 1
        elif key == curses.KEY_LEFT:  pos = max(0, pos-1)
        elif key == curses.KEY_RIGHT: pos = min(len(buf), pos+1)
        elif key in (curses.KEY_ENTER, 10, 13): break
        elif key == 27: buf = None; break

    curses.curs_set(0)
    del win
    stdscr.touchwin(); stdscr.refresh()
    return ''.join(buf) if buf is not None else None

def dialog_confirm(stdscr, message):
    """Yes/No dialog. Returns True/False."""
    rows, cols = stdscr.getmaxyx()
    lines = textwrap.wrap(message, 60)
    h, w = len(lines) + 6, min(66, cols - 4)
    y, x = (rows - h)//2, (cols - w)//2
    win = curses.newwin(h, w, y, x)
    win.keypad(True)
    draw_box(win, 0, 0, h, w, "Confirm", C_WARN)
    for i, line in enumerate(lines):
        safe_addstr(win, i+2, 2, truncate(line, w-4), cp(C_NORMAL))
    sel = 1   # 0=Yes 1=No
    buttons = ["  Yes  ", "  No  "]

    while True:
        for i, btn in enumerate(buttons):
            bx = 4 + i * 14
            attr = cp(C_SEL) | curses.A_BOLD if i == sel else cp(C_ACCENT)
            safe_addstr(win, h-2, bx, f"[{btn}]", attr)
        win.refresh()
        try: key = win.get_wch()
        except: continue
        if isinstance(key, str):
            if key == '\n': result = (sel == 0); break
            elif key in ('y','Y'): result = True; break
            elif key in ('n','N','q'): result = False; break
            elif key == '\x1b': result = False; break
        elif key in (curses.KEY_LEFT, curses.KEY_RIGHT): sel = 1 - sel
        elif key in (curses.KEY_ENTER, 10, 13): result = (sel == 0); break
        elif key == 27: result = False; break
        # mouse
        elif key == curses.KEY_MOUSE:
            try:
                _, mx, my, _, bstate = curses.getmouse()
                ay, ax = y + h - 2, x
                if my == ay:
                    if 4 <= mx - ax < 13: result = True; break
                    elif 18 <= mx - ax < 26: result = False; break
            except: pass

    del win; stdscr.touchwin(); stdscr.refresh()
    return result

def output_pager(stdscr, title, text):
    """Scrollable output viewer."""
    rows, cols = stdscr.getmaxyx()
    h, w = rows - 4, cols - 4
    y, x = 2, 2
    win = curses.newwin(h, w, y, x)
    win.keypad(True)

    lines = []
    for l in text.split('\n'):
        lines.extend(textwrap.wrap(l, w-4) or [''])

    scroll = 0
    while True:
        win.erase()
        draw_box(win, 0, 0, h, w, title, C_BORDER)
        visible = h - 3
        for i in range(visible):
            li = scroll + i
            if li < len(lines):
                safe_addstr(win, i+1, 2, truncate(lines[li], w-4), cp(C_NORMAL))
        pct = int(100 * (scroll + visible) / max(len(lines), 1))
        nav = f" ↑/↓/PgUp/PgDn scroll | q/Esc close | {scroll+1}-{min(scroll+visible,len(lines))}/{len(lines)} ({pct}%) "
        safe_addstr(win, h-1, max(0,(w-len(nav))//2), truncate(nav,w-2), cp(C_DIM))
        win.refresh()

        try: key = win.get_wch()
        except: continue
        if isinstance(key, str):
            if key in ('q','Q','\x1b'): break
        elif key in (curses.KEY_UP,):    scroll = max(0, scroll-1)
        elif key in (curses.KEY_DOWN,):  scroll = min(max(0,len(lines)-visible), scroll+1)
        elif key == curses.KEY_PPAGE:    scroll = max(0, scroll-visible)
        elif key == curses.KEY_NPAGE:    scroll = min(max(0,len(lines)-visible), scroll+visible)
        elif key == curses.KEY_HOME:     scroll = 0
        elif key == curses.KEY_END:      scroll = max(0, len(lines)-visible)
        elif key == 27: break
        elif key == curses.KEY_MOUSE:
            try:
                _, _, _, _, bstate = curses.getmouse()
                if bstate & curses.BUTTON4_PRESSED: scroll = max(0, scroll-3)
                elif bstate & curses.BUTTON5_PRESSED: scroll = min(max(0,len(lines)-visible), scroll+3)
            except: pass

    del win; stdscr.touchwin(); stdscr.refresh()

# ═══════════════════════════════════════════════════════════════════════════════
#  MENU SYSTEM
# ═══════════════════════════════════════════════════════════════════════════════
class Menu:
    """Clickable menu with keyboard + mouse navigation."""

    def __init__(self, stdscr, title, items, subtitle=""):
        self.stdscr = stdscr
        self.title = title
        self.subtitle = subtitle
        # items: list of (label, value) or just strings
        self.items = [(i if isinstance(i,tuple) else (i,i)) for i in items]
        self.sel = 0
        self.scroll = 0
        self.item_rows = []  # (row, col, width) for click detection

    def show(self):
        """Show menu. Returns (label, value) or None if cancelled."""
        rows, cols = self.stdscr.getmaxyx()
        while True:
            rows, cols = self.stdscr.getmaxyx()
            self.stdscr.erase()
            self.item_rows = []

            h_avail = rows - 6
            visible_items = min(len(self.items), h_avail - (2 if self.subtitle else 0))
            # Adjust scroll
            if self.sel < self.scroll:            self.scroll = self.sel
            if self.sel >= self.scroll + visible_items: self.scroll = self.sel - visible_items + 1

            # ── Title bar ──────────────────────────────────────────────────
            title_str = f"  AI CLI v2.3 ─ {self.title}  "
            self.stdscr.attron(cp(C_TITLE) | curses.A_BOLD)
            self.stdscr.hline(0, 0, ' ', cols)
            safe_addstr(self.stdscr, 0, (cols - len(title_str))//2, title_str, cp(C_TITLE)|curses.A_BOLD)
            self.stdscr.attroff(cp(C_TITLE) | curses.A_BOLD)

            # ── Subtitle ────────────────────────────────────────────────────
            row = 2
            if self.subtitle:
                safe_addstr(self.stdscr, row, 2, truncate(self.subtitle, cols-4), cp(C_DIM))
                row += 1

            self.stdscr.hline(row, 0, '─', cols, cp(C_BORDER))
            row += 1

            # ── Items ────────────────────────────────────────────────────────
            for idx in range(visible_items):
                li = self.scroll + idx
                if li >= len(self.items): break
                label, val = self.items[li]
                is_sel = (li == self.sel)

                # Number indicator
                num_str = f" {li+1:>2} "
                item_str = f" {label} "
                full = num_str + item_str

                if is_sel:
                    self.stdscr.attron(cp(C_SEL) | curses.A_BOLD)
                    self.stdscr.hline(row, 0, ' ', cols)
                    safe_addstr(self.stdscr, row, 0, "▶", cp(C_SEL)|curses.A_BOLD)
                    safe_addstr(self.stdscr, row, 2, f"{li+1:>2}", cp(C_SEL)|curses.A_BOLD)
                    safe_addstr(self.stdscr, row, 6, truncate(label, cols-8), cp(C_SEL)|curses.A_BOLD)
                    self.stdscr.attroff(cp(C_SEL) | curses.A_BOLD)
                else:
                    safe_addstr(self.stdscr, row, 2, f"{li+1:>2}", cp(C_DIM))
                    safe_addstr(self.stdscr, row, 6, truncate(label, cols-8), cp(C_NORMAL))

                self.item_rows.append((row, li))
                row += 1

            # ── Scroll indicator ─────────────────────────────────────────────
            if len(self.items) > visible_items:
                pct = int(100 * self.scroll / max(1, len(self.items)-visible_items))
                safe_addstr(self.stdscr, row, cols-8, f"  {pct}%↕ ", cp(C_DIM))

            # ── Bottom bar ────────────────────────────────────────────────────
            self.stdscr.hline(rows-2, 0, '─', cols, cp(C_BORDER))
            cfg = load_config()
            model = truncate(cfg.get('ACTIVE_MODEL','none'), 28)
            sess  = cfg.get('ACTIVE_SESSION', 'default')
            status_str = f" Model: {model}  │  Session: {sess}  │  Click item or use ↑↓Enter  │  q=quit "
            safe_addstr(self.stdscr, rows-1, 0, truncate(status_str, cols-1), cp(C_DIM))

            self.stdscr.refresh()

            # ── Input handling ────────────────────────────────────────────────
            try: key = self.stdscr.get_wch()
            except KeyboardInterrupt: return None
            except: continue

            if isinstance(key, str):
                if key in ('q','Q','\x1b'): return None
                if key == '\n': return self.items[self.sel]
                if key == 'b': return None
                # Number shortcut
                if key.isdigit():
                    n = int(key)
                    if n == 0: n = 10
                    if 1 <= n <= len(self.items):
                        return self.items[n-1]

            elif key == curses.KEY_UP:
                self.sel = (self.sel - 1) % len(self.items)
            elif key == curses.KEY_DOWN:
                self.sel = (self.sel + 1) % len(self.items)
            elif key == curses.KEY_PPAGE:
                self.sel = max(0, self.sel - 5)
            elif key == curses.KEY_NPAGE:
                self.sel = min(len(self.items)-1, self.sel + 5)
            elif key == curses.KEY_HOME: self.sel = 0
            elif key == curses.KEY_END:  self.sel = len(self.items)-1
            elif key in (curses.KEY_ENTER, 10, 13):
                return self.items[self.sel]

            elif key == curses.KEY_MOUSE:
                try:
                    _, mx, my, _, bstate = curses.getmouse()
                    # Click on item
                    for (irow, li) in self.item_rows:
                        if my == irow and 0 <= mx < cols:
                            if li == self.sel:
                                return self.items[li]  # second click = select
                            else:
                                self.sel = li
                                break
                    # Scroll wheel
                    if bstate & curses.BUTTON4_PRESSED:
                        self.sel = max(0, self.sel-1)
                    elif bstate & curses.BUTTON5_PRESSED:
                        self.sel = min(len(self.items)-1, self.sel+1)
                except: pass

# ═══════════════════════════════════════════════════════════════════════════════
#  CHAT PANEL
# ═══════════════════════════════════════════════════════════════════════════════
class ChatPanel:
    def __init__(self, stdscr, chat_name="auto"):
        self.stdscr = stdscr
        self.chat_name = chat_name
        self.history = []
        self.input_buf = []
        self.input_pos = 0

    def run(self):
        rows, cols = self.stdscr.getmaxyx()
        curses.curs_set(1)
        input_h = 3
        output_h = rows - input_h - 3

        # Sub-windows
        title_win  = self.stdscr
        output_win = curses.newpad(2000, cols - 4)
        input_win  = curses.newwin(input_h, cols, rows - input_h - 1, 0)
        input_win.keypad(True)
        output_win.scrollok(True)

        scroll_pos = 0

        def redraw_all():
            nonlocal rows, cols
            rows, cols = self.stdscr.getmaxyx()
            # Title
            title_str = f"  Chat: {self.chat_name}  (Ctrl+Q=quit | /help for commands)  "
            self.stdscr.attron(cp(C_TITLE)|curses.A_BOLD)
            self.stdscr.hline(0, 0, ' ', cols)
            safe_addstr(self.stdscr, 0, 0, truncate(title_str, cols), cp(C_TITLE)|curses.A_BOLD)
            self.stdscr.attroff(cp(C_TITLE)|curses.A_BOLD)
            self.stdscr.hline(rows-input_h-1, 0, '─', cols, cp(C_BORDER))

            # Output
            output_win.erase()
            line = 0
            for role, msg, color in self.history:
                prefix = "You: " if role == "user" else " AI: "
                wrapped = textwrap.wrap(msg, cols - len(prefix) - 4) or ['']
                for i, wl in enumerate(wrapped):
                    pfx = prefix if i == 0 else '     ' if role == 'user' else '      '
                    attr = cp(color) | (curses.A_BOLD if i == 0 else 0)
                    try: output_win.addstr(line, 2, truncate(pfx + wl, cols-4), attr)
                    except: pass
                    line += 1
                line += 1

            visible_lines = rows - input_h - 3
            max_scroll = max(0, line - visible_lines)
            s = max(0, min(scroll_pos, max_scroll))
            try: output_win.refresh(s, 0, 1, 0, rows-input_h-2, cols-1)
            except: pass

            # Input box
            input_win.erase()
            input_win.attron(cp(C_BORDER))
            input_win.hline(0, 0, '─', cols)
            input_win.attroff(cp(C_BORDER))
            prompt = " You › "
            input_win.addstr(1, 0, prompt, cp(C_ACCENT)|curses.A_BOLD)
            field_x = len(prompt)
            field_w = cols - field_x - 1
            buf_str = ''.join(self.input_buf)[-field_w:]
            try: input_win.addstr(1, field_x, buf_str, cp(C_NORMAL)|curses.A_UNDERLINE)
            except: pass
            cursor_x = field_x + min(self.input_pos, field_w-1)
            try: input_win.move(1, cursor_x)
            except: pass
            input_win.refresh()
            return max_scroll

        max_scroll = redraw_all()
        scroll_pos = max_scroll

        while True:
            max_scroll = redraw_all()
            if scroll_pos > max_scroll: scroll_pos = max_scroll

            try: key = input_win.get_wch()
            except KeyboardInterrupt: break
            except: continue

            if isinstance(key, str):
                if key == '\x11':  # Ctrl+Q
                    break
                elif key == '\n':
                    text = ''.join(self.input_buf).strip()
                    self.input_buf = []; self.input_pos = 0
                    if not text: continue
                    # Handle commands
                    if text.startswith('/'):
                        cmd = text[1:].strip()
                        if cmd in ('quit','exit','q'): break
                        elif cmd == 'help':
                            self.history.append(('ai',
                                '/quit  /clear  /model <m>  /persona <p>  /session <s>  /save  Ctrl+Q=exit',
                                C_DIM))
                        elif cmd == 'clear': self.history = []
                        elif cmd.startswith('model '):
                            m = cmd[6:].strip()
                            run_cmd(['model', m])
                            self.history.append(('ai', f'Model set: {m}', C_OK))
                        elif cmd.startswith('persona '):
                            p = cmd[8:].strip()
                            run_cmd(['persona','set',p])
                            self.history.append(('ai', f'Persona: {p}', C_OK))
                        elif cmd == 'save':
                            self.history.append(('ai', 'Chat saved.', C_OK))
                        else:
                            self.history.append(('ai', f'Unknown command: /{cmd}', C_ERR))
                        continue
                    # Send to AI
                    self.history.append(('user', text, C_ACCENT2))
                    scroll_pos = 99999
                    redraw_all()
                    # Show "thinking..." indicator
                    self.history.append(('ai', '⟳ thinking…', C_DIM))
                    scroll_pos = 99999
                    redraw_all()

                    # Run in thread to not block UI
                    result_holder = [None]
                    def do_ask():
                        r = run_cmd(['ask', text])
                        result_holder[0] = r
                    t = threading.Thread(target=do_ask, daemon=True)
                    t.start()
                    dots = 0
                    while t.is_alive():
                        dots = (dots + 1) % 4
                        self.history[-1] = ('ai', '⟳ thinking' + '.'*dots, C_DIM)
                        redraw_all()
                        time.sleep(0.25)
                    t.join()
                    self.history[-1] = ('ai', result_holder[0] or '[no response]', C_ACCENT)
                    scroll_pos = 99999

                elif key == '\x7f' or key == '\b':
                    if self.input_buf and self.input_pos > 0:
                        self.input_buf.pop(self.input_pos-1)
                        self.input_pos -= 1
                elif key.isprintable():
                    self.input_buf.insert(self.input_pos, key)
                    self.input_pos += 1

            elif key == curses.KEY_BACKSPACE:
                if self.input_buf and self.input_pos > 0:
                    self.input_buf.pop(self.input_pos-1); self.input_pos -= 1
            elif key == curses.KEY_LEFT:  self.input_pos = max(0, self.input_pos-1)
            elif key == curses.KEY_RIGHT: self.input_pos = min(len(self.input_buf), self.input_pos+1)
            elif key == curses.KEY_UP:    scroll_pos = max(0, scroll_pos - 1)
            elif key == curses.KEY_DOWN:  scroll_pos = min(max_scroll, scroll_pos + 1)
            elif key == curses.KEY_PPAGE: scroll_pos = max(0, scroll_pos - 5)
            elif key == curses.KEY_NPAGE: scroll_pos = min(max_scroll, scroll_pos + 5)
            elif key == curses.KEY_HOME:  self.input_pos = 0
            elif key == curses.KEY_END:   self.input_pos = len(self.input_buf)
            elif key == curses.KEY_MOUSE:
                try:
                    _, _, _, _, bstate = curses.getmouse()
                    if bstate & curses.BUTTON4_PRESSED: scroll_pos = max(0, scroll_pos-3)
                    elif bstate & curses.BUTTON5_PRESSED: scroll_pos = min(max_scroll, scroll_pos+3)
                except: pass

        curses.curs_set(0)
        del output_win, input_win

# ═══════════════════════════════════════════════════════════════════════════════
#  MAIN APPLICATION
# ═══════════════════════════════════════════════════════════════════════════════
class App:
    def __init__(self, stdscr):
        self.stdscr = stdscr
        self.theme = THEME
        stdscr.keypad(True)
        # Enable mouse
        try:
            curses.mousemask(
                curses.ALL_MOUSE_EVENTS | curses.REPORT_MOUSE_POSITION
            )
            print('\033[?1003h', end='', flush=True)  # enable any-event mouse
        except: pass
        curses.curs_set(0)
        init_colors(self.theme)

    def menu(self, title, items, subtitle=""):
        m = Menu(self.stdscr, title, items, subtitle)
        return m.show()

    def run_and_show(self, title, *cmd_args):
        """Run CLI command and show output in pager."""
        self.stdscr.erase()
        rows, cols = self.stdscr.getmaxyx()
        safe_addstr(self.stdscr, rows//2, (cols-20)//2, "⟳ Running, please wait…", cp(C_DIM)|curses.A_BOLD)
        self.stdscr.refresh()
        out = run_cmd(list(cmd_args))
        output_pager(self.stdscr, title, out or "[no output]")

    def prompt_and_run(self, prompt_text, title, *cmd_prefix):
        val = dialog_input(self.stdscr, prompt_text)
        if val:
            self.run_and_show(title, *cmd_prefix, val)

    # ── Main menu ──────────────────────────────────────────────────────────────
    def main_loop(self):
        while True:
            cfg = load_config()
            model = truncate(cfg.get('ACTIVE_MODEL','not set'), 30)
            sub = f"Model: {model}  │  Session: {cfg.get('ACTIVE_SESSION','default')}  │  Theme: {self.theme}"

            result = self.menu("Main Menu", [
                ("💬  Chat (interactive)",              "chat"),
                ("🧠  Ask a question",                  "ask"),
                ("🎨  Imagine (image generation)",      "imagine"),
                ("🖼️   Vision (image → text)",           "vision"),
                ("🎵  Audio",                            "audio"),
                ("🎬  Video",                            "video"),
                ("🖌️   Canvas (AI code workspace)",      "canvas"),
                ("🤖  Models",                          "models"),
                ("🔬  Tiny Trained Model (TTM ~179M)",  "ttm"),
                ("🔷  Mini Trained Model (MTM ~0.61B)", "mtm"),
                ("🔹  Medium Trained Model (Mtm ~1B)",  "Mtm"),
                ("🏗️   Custom Model Builder",            "custom_model"),
                ("🔧  Fine-tuning (LoRA)",              "finetune"),
                ("🌐  Web Search",                      "websearch"),
                ("📜  Chats",                           "chats"),
                ("⚙️   Settings",                        "settings"),
                ("📊  Status",                          "status"),
                ("🚪  Quit",                            "quit"),
            ], subtitle=sub)

            if result is None or result[1] == "quit": break
            action = result[1]

            if action == "chat":      self.chat_menu()
            elif action == "ask":     self.ask_menu()
            elif action == "imagine": self.prompt_and_run("Image prompt", "Generated Image", "imagine")
            elif action == "vision":  self.vision_menu()
            elif action == "audio":   self.audio_menu()
            elif action == "video":   self.video_menu()
            elif action == "canvas":  self.canvas_menu()
            elif action == "models":  self.models_menu()
            elif action == "ttm":     self.tm_menu("ttm",  "TTM (Tiny ~179M)",    "ray0rf1re/tiny")
            elif action == "mtm":     self.tm_menu("mtm",  "MTM (Mini ~0.61B)",   "ray0rf1re/mini")
            elif action == "Mtm":     self.tm_menu("Mtm",  "Mtm (Medium ~1.075B)","ray0rf1re/medium")
            elif action == "custom_model": self.custom_model_menu()
            elif action == "finetune": self.finetune_menu()
            elif action == "websearch": self.prompt_and_run("Search query", "Web Search", "websearch")
            elif action == "chats":   self.chats_menu()
            elif action == "settings": self.settings_menu()
            elif action == "status":  self.run_and_show("Status", "status")

    # ── Chat menu ──────────────────────────────────────────────────────────────
    def chat_menu(self):
        result = self.menu("Chat", [
            ("Start new chat (auto-name)", "new_auto"),
            ("Start named chat",           "new_named"),
            ("List saved chats",           "list"),
            ("Show a chat",                "show"),
            ("Delete a chat",              "delete"),
            ("Back",                       "back"),
        ])
        if not result or result[1] == "back": return
        a = result[1]
        if a == "new_auto":
            ChatPanel(self.stdscr, "auto").run()
        elif a == "new_named":
            name = dialog_input(self.stdscr, "Chat name")
            if name: ChatPanel(self.stdscr, name).run()
        elif a == "list":  self.run_and_show("Saved Chats",   "chat-list")
        elif a == "show":
            name = dialog_input(self.stdscr, "Chat name")
            if name: self.run_and_show(f"Chat: {name}", "chat-show", name)
        elif a == "delete":
            name = dialog_input(self.stdscr, "Chat name to delete")
            if name and dialog_confirm(self.stdscr, f"Delete chat '{name}'?"):
                self.run_and_show("Deleted", "chat-delete", name)

    # ── Ask menu ───────────────────────────────────────────────────────────────
    def ask_menu(self):
        result = self.menu("Ask", [
            ("Ask a question",        "ask"),
            ("Pipe / summarize text", "pipe"),
            ("Review code file",      "review"),
            ("Explain file or text",  "explain"),
            ("Translate text",        "translate"),
            ("Generate code",         "code"),
            ("Back",                  "back"),
        ])
        if not result or result[1] == "back": return
        a = result[1]
        if a == "ask":     self.prompt_and_run("Your question",      "Response",     "ask")
        elif a == "pipe":  self.prompt_and_run("Text to summarize",  "Summary",      "summarize")
        elif a == "review":self.prompt_and_run("File path to review","Code Review",  "review")
        elif a == "explain":self.prompt_and_run("File or text",      "Explanation",  "explain")
        elif a == "translate":
            text = dialog_input(self.stdscr, "Text to translate")
            if text:
                lang = dialog_input(self.stdscr, "Target language", "Spanish")
                if lang: self.run_and_show("Translation", "translate", text, "to", lang)
        elif a == "code":  self.prompt_and_run("Describe code to generate","Generated Code","code")

    # ── TM menu (TTM/MTM/Mtm) ─────────────────────────────────────────────────
    def tm_menu(self, tm_id, label, repo):
        while True:
            result = self.menu(label, [
                (f"Status",                        "status"),
                (f"Pretrain (6 datasets + 2 opt)", "pretrain"),
                (f"Set custom dataset 1",          "custom1"),
                (f"Set custom dataset 2",          "custom2"),
                (f"Enable auto-training",          "enable"),
                (f"Disable auto-training",         "disable"),
                (f"Train one batch now",           "train"),
                (f"Upload to {repo}",              "upload"),
                (f"Create HF repo ({repo})",       "create_repo"),
                (f"Load as active model",          "load"),
                ("Back",                           "back"),
            ], subtitle=f"Repo: {repo}")
            if not result or result[1] == "back": return
            a = result[1]
            if a == "status":     self.run_and_show(f"{label} Status", tm_id, "status")
            elif a == "pretrain":
                c1 = dialog_input(self.stdscr, "Custom dataset 1 (HF id/path, blank=skip)", "")
                c2 = dialog_input(self.stdscr, "Custom dataset 2 (HF id/path, blank=skip)", "")
                args = [tm_id, "pretrain"]
                if c1: args.append(c1)
                if c2: args.append(c2)
                self.run_and_show(f"{label} Pretraining", *args)
            elif a == "custom1":
                v = dialog_input(self.stdscr, "HF dataset id or local path")
                if v: self.run_and_show("Set Custom 1", tm_id, "set-custom1", v)
            elif a == "custom2":
                v = dialog_input(self.stdscr, "HF dataset id or local path")
                if v: self.run_and_show("Set Custom 2", tm_id, "set-custom2", v)
            elif a == "enable":  self.run_and_show("Enabled",  tm_id, "enable")
            elif a == "disable": self.run_and_show("Disabled", tm_id, "disable")
            elif a == "train":   self.run_and_show("Training", tm_id, "train-now")
            elif a == "upload":
                v = dialog_input(self.stdscr, "Version (blank=latest)", "latest")
                self.run_and_show("Upload", tm_id, "upload", v or "latest")
            elif a == "create_repo":
                if dialog_confirm(self.stdscr, f"Create HuggingFace repo '{repo}'?"):
                    self.run_and_show("Create Repo", tm_id, "create-repo")
            elif a == "load":
                v = dialog_input(self.stdscr, "Version (blank=latest)", "latest")
                self.run_and_show("Loaded", tm_id, "load", v or "latest")

    # ── Models menu ────────────────────────────────────────────────────────────
    def models_menu(self):
        while True:
            result = self.menu("Models", [
                ("List downloaded models",   "list"),
                ("Set active model",         "set"),
                ("Download from HuggingFace","download"),
                ("Search HuggingFace",       "search"),
                ("Recommended models",       "recommended"),
                ("Upload to HuggingFace",    "upload"),
                ("Model info",               "info"),
                ("Back",                     "back"),
            ])
            if not result or result[1] == "back": return
            a = result[1]
            if a == "list":   self.run_and_show("Models", "models")
            elif a == "set":
                m = dialog_input(self.stdscr, "Model name or path")
                if m: self.run_and_show("Set Model", "model", m)
            elif a == "download":
                r = dialog_input(self.stdscr, "HuggingFace repo (user/model)")
                if r: self.run_and_show("Downloading…", "download", r)
            elif a == "search":
                q = dialog_input(self.stdscr, "Search query")
                if q: self.run_and_show("Search Results", "search-models", q)
            elif a == "recommended": self.run_and_show("Recommended", "recommended")
            elif a == "upload":
                p = dialog_input(self.stdscr, "Local path")
                if p:
                    r = dialog_input(self.stdscr, "HF repo (user/name)")
                    if r: self.run_and_show("Uploading…", "upload", p, r)
            elif a == "info":
                m = dialog_input(self.stdscr, "Model name/path (blank=active)", "")
                args = ["model-info"] + ([m] if m else [])
                self.run_and_show("Model Info", *args)

    # ── Vision menu ────────────────────────────────────────────────────────────
    def vision_menu(self):
        result = self.menu("Vision", [
            ("Ask about an image",    "ask"),
            ("OCR (extract text)",    "ocr"),
            ("Caption an image",      "caption"),
            ("Compare two images",    "compare"),
            ("Back",                  "back"),
        ])
        if not result or result[1] == "back": return
        a = result[1]
        if a == "ask":
            img = dialog_input(self.stdscr, "Image path")
            if img:
                q = dialog_input(self.stdscr, "Question", "Describe this image in detail.")
                if q: self.run_and_show("Vision", "vision", "ask", img, q)
        elif a == "ocr":
            img = dialog_input(self.stdscr, "Image path")
            if img: self.run_and_show("OCR", "vision", "ocr", img)
        elif a == "caption":
            img = dialog_input(self.stdscr, "Image path")
            if img: self.run_and_show("Caption", "vision", "caption", img)
        elif a == "compare":
            i1 = dialog_input(self.stdscr, "Image 1 path")
            if i1:
                i2 = dialog_input(self.stdscr, "Image 2 path")
                if i2: self.run_and_show("Compare", "vision", "compare", i1, i2)

    # ── Audio menu ─────────────────────────────────────────────────────────────
    def audio_menu(self):
        while True:
            result = self.menu("Audio", [
                ("Transcribe audio",        "transcribe"),
                ("Text-to-Speech",          "tts"),
                ("Analyze audio with AI",   "analyze"),
                ("Convert format",          "convert"),
                ("Extract audio from video","extract"),
                ("Ask about audio",         "ask"),
                ("Play audio file",         "play"),
                ("File info",               "info"),
                ("Back",                    "back"),
            ])
            if not result or result[1] == "back": return
            a = result[1]
            if a == "transcribe":
                f = dialog_input(self.stdscr, "Audio file path")
                if f: self.run_and_show("Transcript", "audio", "transcribe", f)
            elif a == "tts":
                t = dialog_input(self.stdscr, "Text to speak")
                if t: self.run_and_show("TTS", "audio", "tts", t)
            elif a == "analyze":
                f = dialog_input(self.stdscr, "Audio file path")
                if f: self.run_and_show("Analysis", "audio", "analyze", f)
            elif a == "convert":
                i = dialog_input(self.stdscr, "Input file")
                if i:
                    o = dialog_input(self.stdscr, "Output file")
                    if o: self.run_and_show("Convert", "audio", "convert", i, o)
            elif a == "extract":
                v = dialog_input(self.stdscr, "Video file")
                if v: self.run_and_show("Extracted", "audio", "extract", v)
            elif a == "ask":
                f = dialog_input(self.stdscr, "Audio file")
                if f:
                    q = dialog_input(self.stdscr, "Question")
                    if q: self.run_and_show("Answer", "audio", "ask", f, q)
            elif a == "play":
                f = dialog_input(self.stdscr, "Audio file")
                if f: run_cmd(["audio","play",f], capture=False)
            elif a == "info":
                f = dialog_input(self.stdscr, "Audio file")
                if f: self.run_and_show("Info", "audio", "info", f)

    # ── Video menu ─────────────────────────────────────────────────────────────
    def video_menu(self):
        while True:
            result = self.menu("Video", [
                ("Analyze video",         "analyze"),
                ("Transcribe audio",      "transcribe"),
                ("Generate captions (.srt)","caption"),
                ("Extract frames",        "extract"),
                ("Trim video",            "trim"),
                ("Convert format",        "convert"),
                ("Ask about video",       "ask"),
                ("File info",             "info"),
                ("Summary",               "summary"),
                ("Back",                  "back"),
            ])
            if not result or result[1] == "back": return
            a = result[1]
            if a in ("analyze","transcribe","caption","summary","info","extract"):
                f = dialog_input(self.stdscr, "Video file path")
                if f: self.run_and_show(a.title(), "video", a, f)
            elif a == "trim":
                f  = dialog_input(self.stdscr, "Input video")
                if f:
                    s  = dialog_input(self.stdscr, "Start time (HH:MM:SS)")
                    e  = dialog_input(self.stdscr, "End time   (HH:MM:SS)")
                    if s and e: self.run_and_show("Trimmed", "video","trim",f,s,e)
            elif a == "convert":
                i = dialog_input(self.stdscr, "Input file")
                if i:
                    o = dialog_input(self.stdscr, "Output file")
                    if o: self.run_and_show("Convert", "video","convert",i,o)
            elif a == "ask":
                f = dialog_input(self.stdscr, "Video file")
                if f:
                    q = dialog_input(self.stdscr, "Question")
                    if q: self.run_and_show("Answer", "video","ask",f,q)

    # ── Canvas menu ────────────────────────────────────────────────────────────
    def canvas_menu(self):
        cfg = load_config()
        active = cfg.get('CANVAS_ACTIVE','none')
        while True:
            result = self.menu("Canvas", [
                ("New canvas",              "new"),
                ("Open file",               "open"),
                ("AI: write/modify code",   "ask"),
                ("Show code",               "show"),
                ("Run code",                "run"),
                ("Edit in $EDITOR",         "edit"),
                ("Show git diff",           "diff"),
                ("Save to output dir",      "save"),
                ("List all canvases",       "list"),
                ("Close canvas",            "close"),
                ("Canvas status",           "status"),
                ("Back",                    "back"),
            ], subtitle=f"Active: {active}")
            if not result or result[1] == "back": return
            a = result[1]
            if a == "new":
                n = dialog_input(self.stdscr, "Canvas name")
                if n:
                    l = dialog_input(self.stdscr, "Language", "python")
                    self.run_and_show("Canvas", "canvas","new",n, l or "python")
                    active = f"{n}.{l or 'python'}"
            elif a == "open":
                f = dialog_input(self.stdscr, "File path")
                if f: self.run_and_show("Opened", "canvas","open",f); active = f
            elif a == "ask":
                t = dialog_input(self.stdscr, "Describe what to build/change")
                if t: self.run_and_show("Canvas Updated", "canvas","ask",t)
            elif a in ("show","run","edit","diff","save","list","close","status"):
                self.run_and_show(a.title(), "canvas", a)

    # ── Custom model menu ──────────────────────────────────────────────────────
    def custom_model_menu(self):
        while True:
            result = self.menu("Custom Model Builder", [
                ("List presets",        "presets"),
                ("Create new model",    "new"),
                ("List custom models",  "list"),
                ("Train custom model",  "train"),
                ("Model info",          "info"),
                ("Delete model",        "delete"),
                ("Back",                "back"),
            ], subtitle="Minimum: 0.125B params")
            if not result or result[1] == "back": return
            a = result[1]
            if a == "presets": self.run_and_show("Presets", "model-create","presets")
            elif a == "new":
                n = dialog_input(self.stdscr, "Model name")
                if n:
                    p = dialog_input(self.stdscr, "Preset (nano/micro/tiny/small/medium/tinyllama/custom)", "tiny")
                    self.run_and_show("Created", "model-create","new",n,p or "tiny")
            elif a == "list": self.run_and_show("Custom Models", "model-create","list")
            elif a == "train":
                n = dialog_input(self.stdscr, "Model name")
                if n:
                    d = dialog_input(self.stdscr, "Dataset path (blank=default)", "")
                    args = ["model-create","train",n] + ([d] if d else [])
                    self.run_and_show("Training", *args)
            elif a == "info":
                n = dialog_input(self.stdscr, "Model name")
                if n: self.run_and_show("Info", "model-create","info",n)
            elif a == "delete":
                n = dialog_input(self.stdscr, "Model name")
                if n and dialog_confirm(self.stdscr, f"Delete '{n}'?"):
                    self.run_and_show("Deleted", "model-create","delete",n)

    # ── Fine-tune menu ────────────────────────────────────────────────────────
    def finetune_menu(self):
        result = self.menu("Fine-tuning (LoRA)", [
            ("Prepare dataset",     "prepare"),
            ("Start fine-tune",     "start"),
            ("Merge LoRA adapter",  "merge"),
            ("Quantize to GGUF",    "quantize"),
            ("Fine-tune status",    "status"),
            ("Back",                "back"),
        ])
        if not result or result[1] == "back": return
        a = result[1]
        if a == "prepare":
            f = dialog_input(self.stdscr, "Data file path")
            if f: self.run_and_show("Prepared", "finetune","prepare",f)
        elif a == "start":
            b = dialog_input(self.stdscr, "Base model (HF id)")
            if b:
                d = dialog_input(self.stdscr, "Dataset (blank=default)", "")
                args = ["finetune","start",b] + ([d] if d else [])
                self.run_and_show("Fine-tuning…", *args)
        elif a == "merge":
            ap = dialog_input(self.stdscr, "Adapter path")
            if ap:
                bm = dialog_input(self.stdscr, "Base model")
                if bm:
                    op = dialog_input(self.stdscr, "Output path", ap+"_merged")
                    self.run_and_show("Merged", "finetune","merge",ap,bm,op)
        elif a == "quantize":
            m = dialog_input(self.stdscr, "Model path")
            if m:
                q = dialog_input(self.stdscr, "Quantization", "Q4_K_M")
                self.run_and_show("Quantized", "finetune","quantize",m,q or "Q4_K_M")
        elif a == "status": self.run_and_show("Status", "finetune","status")

    # ── Chats menu ─────────────────────────────────────────────────────────────
    def chats_menu(self):
        result = self.menu("Saved Chats", [
            ("List all chats",  "list"),
            ("Show a chat",     "show"),
            ("Delete a chat",   "delete"),
            ("Back",            "back"),
        ])
        if not result or result[1] == "back": return
        a = result[1]
        if a == "list":  self.run_and_show("Chats", "chat-list")
        elif a == "show":
            n = dialog_input(self.stdscr, "Chat name")
            if n: self.run_and_show(f"Chat: {n}", "chat-show", n)
        elif a == "delete":
            n = dialog_input(self.stdscr, "Chat name to delete")
            if n and dialog_confirm(self.stdscr, f"Delete '{n}'?"):
                self.run_and_show("Deleted", "chat-delete", n)

    # ── Settings menu ─────────────────────────────────────────────────────────
    def settings_menu(self):
        while True:
            cfg = load_config()
            result = self.menu("Settings", [
                (f"Temperature:       {cfg.get('TEMPERATURE','0.7')}",  "temp"),
                (f"Max tokens:        {cfg.get('MAX_TOKENS','2048')}",   "tokens"),
                (f"Context size:      {cfg.get('CONTEXT_SIZE','4096')}", "context"),
                (f"GPU layers:        {cfg.get('GPU_LAYERS','-1')}",     "gpu"),
                (f"Stream:            {cfg.get('STREAM','1')}",          "stream"),
                (f"Tool calling:      {cfg.get('TOOL_CALLING','1')}",    "tools"),
                (f"Web search:        {cfg.get('WEB_SEARCH_ENABLED','1')}","websearch"),
                (f"HF dataset sync:   {cfg.get('HF_DATASET_SYNC','0')}","sync"),
                (f"TTM auto-train:    {cfg.get('TTM_AUTO_TRAIN','0')}",  "ttm_train"),
                (f"MTM auto-train:    {cfg.get('MTM_AUTO_TRAIN','0')}",  "mtm_train"),
                (f"Mtm auto-train:    {cfg.get('MMTM_AUTO_TRAIN','0')}", "mmtm_train"),
                (f"GUI theme:         {self.theme}",                     "theme"),
                ("API Keys",                                              "keys"),
                ("Install dependencies",                                  "install"),
                ("Uninstall AI CLI",                                      "uninstall"),
                ("Back",                                                  "back"),
            ])
            if not result or result[1] == "back": return
            a = result[1]
            if a == "temp":
                v = dialog_input(self.stdscr, "Temperature (0.0-2.0)", cfg.get('TEMPERATURE','0.7'))
                if v: run_cmd(["config","temperature",v])
            elif a == "tokens":
                v = dialog_input(self.stdscr, "Max tokens", cfg.get('MAX_TOKENS','2048'))
                if v: run_cmd(["config","max_tokens",v])
            elif a == "context":
                v = dialog_input(self.stdscr, "Context size", cfg.get('CONTEXT_SIZE','4096'))
                if v: run_cmd(["config","context",v])
            elif a == "gpu":
                v = dialog_input(self.stdscr, "GPU layers (-1=all, 0=CPU)", cfg.get('GPU_LAYERS','-1'))
                if v: run_cmd(["config","gpu_layers",v])
            elif a in ("stream","tools","websearch","sync"):
                key_map = {"stream":"stream","tools":"tool_calling","websearch":"web_search","sync":"hf_dataset_sync"}
                curr = cfg.get({"stream":"STREAM","tools":"TOOL_CALLING","websearch":"WEB_SEARCH_ENABLED","sync":"HF_DATASET_SYNC"}[a],"0")
                run_cmd(["config", key_map[a], "0" if curr=="1" else "1"])
            elif a in ("ttm_train","mtm_train","mmtm_train"):
                key_map = {"ttm_train":"ttm_auto_train","mtm_train":"mtm_auto_train","mmtm_train":"mmtm_auto_train"}
                cv_map  = {"ttm_train":"TTM_AUTO_TRAIN","mtm_train":"MTM_AUTO_TRAIN","mmtm_train":"MMTM_AUTO_TRAIN"}
                curr = cfg.get(cv_map[a], "0")
                run_cmd(["config", key_map[a], "0" if curr=="1" else "1"])
            elif a == "theme":
                themes = list(THEMES.keys())
                r2 = self.menu("Theme", [(t,t) for t in themes] + [("Back","back")])
                if r2 and r2[1] != "back":
                    self.theme = r2[1]
                    init_colors(self.theme)
                    run_cmd(["config","gui_theme",self.theme])
            elif a == "keys":  self.keys_menu()
            elif a == "install": self.run_and_show("Installing…", "install-deps")
            elif a == "uninstall":
                if dialog_confirm(self.stdscr, "Uninstall AI CLI? Type 'yes' to proceed."):
                    self.run_and_show("Uninstall", "-uninstall")

    # ── Keys menu ─────────────────────────────────────────────────────────────
    def keys_menu(self):
        while True:
            result = self.menu("API Keys", [
                ("Set OpenAI key",              "openai"),
                ("Set Anthropic key",           "anthropic"),
                ("Set Gemini key",              "gemini"),
                ("Set HuggingFace token",       "hf"),
                ("Set HF dataset write key",    "hf_ds"),
                ("Set Brave Search key",        "brave"),
                ("Show key status",             "show"),
                ("Back",                        "back"),
            ])
            if not result or result[1] == "back": return
            a = result[1]
            key_map = {"openai":"OPENAI_API_KEY","anthropic":"ANTHROPIC_API_KEY",
                       "gemini":"GEMINI_API_KEY","hf":"HF_TOKEN",
                       "hf_ds":"HF_DATASET_KEY","brave":"BRAVE_API_KEY"}
            if a == "show": self.run_and_show("Keys", "keys")
            elif a in key_map:
                v = dialog_input(self.stdscr, f"Enter {key_map[a]}")
                if v: run_cmd(["keys","set",key_map[a],v])

# ── Entry point ────────────────────────────────────────────────────────────────
def main(stdscr):
    app = App(stdscr)
    app.main_loop()

if __name__ == "__main__":
    try:
        curses.wrapper(main)
    except KeyboardInterrupt:
        pass
    finally:
        # Disable any-event mouse on exit
        print('\033[?1003l', end='', flush=True)
PYEOF

  # Find CLI binary
  local cli_bin; cli_bin=$(command -v ai 2>/dev/null || echo "$0")
  local theme="${GUI_THEME:-dark}"

  info "Starting GUI (mouse-click + keyboard navigation)..."
  "$PYTHON" "$gui_script" "$cli_bin" "$theme"
  local exit_code=$?
  rm -f "$gui_script"
  [[ $exit_code -ne 0 ]] && _gui_fallback
}

# ── Fallback: numbered text menu when curses/Python unavailable ──────────────
_gui_fallback() {
  while true; do
    echo ""
    hdr "═══ AI CLI v${VERSION} — Main Menu ═══"
    local items=(
      "Chat (interactive)"    "Ask a question"     "Imagine (image gen)"
      "Vision (image→text)"   "Audio"              "Video"
      "Canvas (code)"         "Models"             "TTM (Tiny ~179M)"
      "MTM (Mini ~0.61B)"     "Mtm (Medium ~1B)"   "Custom Model Builder"
      "Fine-tuning"           "Web Search"         "Settings"
      "Status"                "Quit"
    )
    for i in "${!items[@]}"; do printf "  ${B}%2d.${R} %s\n" "$(( i+1 ))" "${items[$i]}"; done
    echo ""
    read -rp "Choose [1-${#items[@]}]: " choice
    case "$choice" in
      1)  cmd_chat_interactive ;;
      2)  read -rp "Question: " q; dispatch_ask "$q" ;;
      3)  read -rp "Prompt: " p; cmd_imagine "$p" ;;
      4)  read -rp "Image: " i; read -rp "Question: " q; _vision_ask "$i" "$q" ;;
      5)  cmd_audio ;;
      6)  cmd_video ;;
      7)  cmd_canvas status ;;
      8)  cmd_list_models ;;
      9)  cmd_ttm ;;
      10) cmd_mtm ;;
      11) cmd_Mtm ;;
      12) cmd_model_create ;;
      13) cmd_finetune ;;
      14) read -rp "Search: " q; cmd_websearch "$q" ;;
      15) echo "Config: $CONFIG_FILE"; cat "$CONFIG_FILE" ;;
      16) cmd_status ;;
      17|q|Q) break ;;
    esac
  done
}

# ════════════════════════════════════════════════════════════════════════════════
#  BUILTIN TOOLS
# ════════════════════════════════════════════════════════════════════════════════
BUILTIN_TOOLS=("web_search" "read_file" "write_file" "run_code" "list_dir"
               "get_time" "get_sysinfo" "calc" "download_file" "image_info")

run_tool() {
  local name="$1"; local args_json="${2:-{}}"
  case "$name" in
    web_search)
      local q; q=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('query',''))" 2>/dev/null)
      web_search "$q" 5 ;;
    read_file)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      [[ -f "$p" ]] && cat "$p" || echo "File not found: $p" ;;
    write_file)
      local p c
      p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      c=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('content',''))" 2>/dev/null)
      echo "$c" > "$p" && echo "Written: $p" ;;
    run_code)
      local lang code
      lang=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('language','python'))" 2>/dev/null)
      code=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('code',''))" 2>/dev/null)
      case "$lang" in
        python|py) echo "$code" | python3 2>&1 ;;
        bash|sh)   echo "$code" | bash 2>&1 ;;
        js|node)   echo "$code" | node 2>&1 ;;
        *) echo "Unsupported language: $lang" ;;
      esac ;;
    list_dir)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path','.'))" 2>/dev/null)
      ls -la "$p" 2>&1 ;;
    get_time) date ;;
    get_sysinfo)
      echo "OS: $(uname -s -r)"
      echo "CPU: $(grep -m1 'model name' /proc/cpuinfo 2>/dev/null | cut -d: -f2 | xargs || sysctl -n machdep.cpu.brand_string 2>/dev/null || echo '?')"
      echo "RAM: $(free -h 2>/dev/null | awk '/^Mem/{print $2}' || echo '?')"
      [[ -n "$PYTHON" ]] && echo "Python: $($PYTHON --version 2>&1)"
      command -v nvidia-smi &>/dev/null && nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo "GPU: none/unknown"
      ;;
    calc)
      local expr; expr=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('expression',''))" 2>/dev/null)
      python3 -c "import math; print(eval('$expr'))" 2>&1 ;;
    download_file)
      local url sp
      url=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('url',''))" 2>/dev/null)
      sp=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('save_path','/tmp/download'))" 2>/dev/null)
      curl -sL "$url" -o "$sp" && echo "Saved: $sp" ;;
    image_info)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      [[ -n "$PYTHON" ]] && "$PYTHON" -c "from PIL import Image; im=Image.open('$p'); print(f'Size: {im.size}, Mode: {im.mode}')" 2>/dev/null || file "$p" ;;
    *) echo "Unknown tool: $name" ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  WEB SEARCH
# ════════════════════════════════════════════════════════════════════════════════
web_search() {
  local query="$1"; local max="${2:-5}"
  local encoded; encoded=$(python3 -c "import urllib.parse,sys; print(urllib.parse.quote(sys.argv[1]))" "$query" 2>/dev/null || echo "$query")

  if [[ "${SEARCH_ENGINE:-ddg}" == "brave" ]] && [[ -n "${BRAVE_API_KEY:-}" ]]; then
    curl -sS "https://api.search.brave.com/res/v1/web/search?q=${encoded}&count=${max}" \
      -H "Accept: application/json" \
      -H "X-Subscription-Token: $BRAVE_API_KEY" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
for r in d.get('web',{}).get('results',[])[:int('$max')]:
    print(f\"Title: {r.get('title','')}\")
    print(f\"URL: {r.get('url','')}\")
    print(f\"Snippet: {r.get('description','')[:200]}\")
    print()
" 2>/dev/null
  else
    curl -sS "https://api.duckduckgo.com/?q=${encoded}&format=json&no_redirect=1&no_html=1" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
results=[]
if d.get('AbstractText'):
    results.append({'title': d.get('Heading',''), 'url': d.get('AbstractURL',''), 'snippet': d.get('AbstractText','')})
for r in d.get('RelatedTopics',[])[:int('$max')]:
    if isinstance(r,dict) and r.get('Text'):
        results.append({'title': r.get('Text','')[:80], 'url': r.get('FirstURL',''), 'snippet': r.get('Text','')[:200]})
for r in results[:int('$max')]:
    print(f\"Title: {r['title']}\")
    print(f\"URL: {r['url']}\")
    print(f\"Snippet: {r['snippet']}\")
    print()
" 2>/dev/null
  fi
}

cmd_websearch() {
  local query="$*"
  [[ -z "$query" ]] && { read -rp "Search: " query; }
  hdr "Search: $query"
  echo ""
  web_search "$query" 10
}

# ════════════════════════════════════════════════════════════════════════════════
#  AI BACKENDS
# ════════════════════════════════════════════════════════════════════════════════
_get_persona_prompt() {
  local name="${ACTIVE_PERSONA:-default}"
  if [[ -f "$PERSONAS_DIR/$name" ]]; then cat "$PERSONAS_DIR/$name"
  elif [[ -n "${BUILTIN_PERSONAS[$name]:-}" ]]; then echo "${BUILTIN_PERSONAS[$name]}"
  else echo "${BUILTIN_PERSONAS[default]}"
  fi
}

_inject_session_history() {
  local session="${ACTIVE_SESSION:-default}"
  local sess_file="$SESSIONS_DIR/${session}.json"
  [[ ! -f "$sess_file" ]] && echo "[]" && return 0
  cat "$sess_file"
}

_save_session_turn() {
  local user_msg="$1"; local ai_msg="$2"
  local session="${ACTIVE_SESSION:-default}"
  local sess_file="$SESSIONS_DIR/${session}.json"
  [[ ! -f "$sess_file" ]] && echo "[]" > "$sess_file"
  python3 -c "
import json,sys
f='$sess_file'
hist=json.load(open(f))
hist.append({'role':'user','content':$(echo "$user_msg" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')})
hist.append({'role':'assistant','content':$(echo "$ai_msg" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')})
# Keep last 20 turns
if len(hist)>40: hist=hist[-40:]
json.dump(hist,open(f,'w'),indent=2)
" 2>/dev/null || true
}

ask_gguf() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && { err "No model set. Run: ai model <path>"; return 1; }

  if [[ "$LLAMA_BIN" == "llama_cpp_python" ]]; then
    LLAMA_PROMPT="$prompt" LLAMA_MODEL="$model" LLAMA_MAX="$MAX_TOKENS" \
    LLAMA_TEMP="$TEMPERATURE" LLAMA_CTX="$CONTEXT_SIZE" LLAMA_GPU="$GPU_LAYERS" \
    "$PYTHON" - <<'PYEOF'
import os
from llama_cpp import Llama
llm = Llama(model_path=os.environ['LLAMA_MODEL'],
            n_ctx=int(os.environ['LLAMA_CTX']),
            n_gpu_layers=int(os.environ['LLAMA_GPU']),
            verbose=False)
out = llm(os.environ['LLAMA_PROMPT'], max_tokens=int(os.environ['LLAMA_MAX']),
          temperature=float(os.environ['LLAMA_TEMP']), stream=False)
print(out['choices'][0]['text'], end='', flush=True)
PYEOF
  elif [[ -n "$LLAMA_BIN" ]]; then
    "$LLAMA_BIN" -m "$model" -p "$prompt" \
      -n "$MAX_TOKENS" --temp "$TEMPERATURE" -c "$CONTEXT_SIZE" \
      --n-gpu-layers "$GPU_LAYERS" --threads "$THREADS" -s 0 \
      2>/dev/null
  else
    err "llama.cpp not found. Run: ai install-deps"
    return 1
  fi
}

ask_pytorch() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" || -z "$PYTHON" ]] && { err "No model or Python not found"; return 1; }

  MODEL_PATH="$model" PROMPT="$prompt" MAX_TOKENS="$MAX_TOKENS" \
  TEMPERATURE="$TEMPERATURE" GPU_LAYERS="$GPU_LAYERS" "$PYTHON" - <<'PYEOF'
import os, sys, torch
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
except ImportError:
    print("transformers not installed"); sys.exit(1)

mp    = os.environ['MODEL_PATH']
prompt= os.environ['PROMPT']
maxt  = int(os.environ.get('MAX_TOKENS',512))
temp  = float(os.environ.get('TEMPERATURE',0.7))
device= 'cuda' if torch.cuda.is_available() else 'cpu'

try:
    tok = AutoTokenizer.from_pretrained(mp)
    mdl = AutoModelForCausalLM.from_pretrained(mp, torch_dtype=torch.float16 if device=='cuda' else torch.float32)
    mdl = mdl.to(device)
    inputs = tok(prompt, return_tensors='pt').to(device)
    with torch.no_grad():
        out = mdl.generate(**inputs, max_new_tokens=maxt, temperature=temp, do_sample=(temp>0),
                           pad_token_id=tok.eos_token_id)
    text = tok.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    print(text, end='', flush=True)
except Exception as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
PYEOF
}

ask_openai() {
  local prompt="$1"
  [[ -z "${OPENAI_API_KEY:-}" ]] && { err "OPENAI_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-gpt-4o}"
  local sys_prompt; sys_prompt=$(_get_persona_prompt)

  local messages_json
  messages_json=$(python3 -c "
import json,sys
sys_p=$(echo "$sys_prompt" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')
user_p=$(echo "$prompt" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')
msgs=[{'role':'system','content':sys_p},{'role':'user','content':user_p}]
print(json.dumps(msgs))
" 2>/dev/null)

  local body; body=$(python3 -c "
import json
body={'model':'${model}','messages':${messages_json},'max_tokens':${MAX_TOKENS},'temperature':${TEMPERATURE},'stream':False}
print(json.dumps(body))
" 2>/dev/null)

  curl -sS https://api.openai.com/v1/chat/completions \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -H "Content-Type: application/json" \
    -d "$body" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['choices'][0]['message']['content'],end='',flush=True)
" 2>/dev/null
}

ask_claude() {
  local prompt="$1"
  [[ -z "${ANTHROPIC_API_KEY:-}" ]] && { err "ANTHROPIC_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-claude-sonnet-4-5}"
  local sys_prompt; sys_prompt=$(_get_persona_prompt)

  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)
  local sys_content; sys_content=$(echo "$sys_prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)

  curl -sS https://api.anthropic.com/v1/messages \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "Content-Type: application/json" \
    -d "{\"model\":\"$model\",\"max_tokens\":$MAX_TOKENS,\"system\":$sys_content,\"messages\":[{\"role\":\"user\",\"content\":$user_content}]}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['content'][0]['text'],end='',flush=True)
" 2>/dev/null
}

ask_gemini() {
  local prompt="$1"
  [[ -z "${GEMINI_API_KEY:-}" ]] && { err "GEMINI_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-gemini-2.0-flash}"
  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)

  curl -sS "https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=$GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"contents\":[{\"parts\":[{\"text\":$user_content}]}]}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['candidates'][0]['content']['parts'][0]['text'],end='',flush=True)
" 2>/dev/null
}

ask_hf() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && { err "No model set"; return 1; }
  local hf_key="${HF_TOKEN:-}"
  local auth_header=""
  [[ -n "$hf_key" ]] && auth_header="-H \"Authorization: Bearer $hf_key\""
  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)
  curl -sS "https://api-inference.huggingface.co/models/${model}" \
    ${hf_key:+-H "Authorization: Bearer $hf_key"} \
    -H "Content-Type: application/json" \
    -d "{\"inputs\":$user_content,\"parameters\":{\"max_new_tokens\":$MAX_TOKENS,\"temperature\":$TEMPERATURE}}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if isinstance(d,list): print(d[0].get('generated_text',''),end='')
elif isinstance(d,dict): print(d.get('generated_text',str(d)),end='')
" 2>/dev/null
}

_auto_detect_backend() {
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && {
    [[ -n "${OPENAI_API_KEY:-}" ]] && { echo "openai"; return; }
    [[ -n "${ANTHROPIC_API_KEY:-}" ]] && { echo "claude"; return; }
    [[ -n "${GEMINI_API_KEY:-}" ]] && { echo "gemini"; return; }
    echo ""; return
  }
  [[ "$model" == gpt-* || "$model" == o1* || "$model" == o3* ]] && { echo "openai"; return; }
  [[ "$model" == claude-* ]] && { echo "claude"; return; }
  [[ "$model" == gemini-* ]] && { echo "gemini"; return; }
  [[ "$model" == *.gguf ]] || [[ "$model" == *Q4* ]] || [[ "$model" == *Q8* ]] && { echo "gguf"; return; }
  [[ -f "$model" ]] && [[ "$model" == *.gguf ]] && { echo "gguf"; return; }
  [[ -d "$model" ]] && [[ -f "$model/config.json" ]] && { echo "pytorch"; return; }
  [[ -n "${HF_TOKEN:-}" ]] && { echo "hf"; return; }
  echo "gguf"
}

_maybe_inject_search() {
  local prompt="$1"
  [[ "$WEB_SEARCH_ENABLED" != "1" ]] && { echo "$prompt"; return; }
  local backend; backend=$(_auto_detect_backend)
  # OpenAI tool calling handles its own search
  [[ "$backend" == "openai" ]] && { echo "$prompt"; return; }
  # Detect search-worthy keywords
  if echo "$prompt" | grep -qiE 'latest|current|today|2024|2025|2026|news|price|who is|what is the status|recent|now|trending'; then
    local search_terms; search_terms=$(echo "$prompt" | sed 's/[?!.,]//g' | tr '[:upper:]' '[:lower:]' | \
      sed 's/what is//g;s/who is//g;s/tell me about//g;s/the latest//g' | \
      awk '{for(i=1;i<=NF&&i<=6;i++) printf $i" "; print ""}' | xargs)
    local results; results=$(web_search "$search_terms" 3 2>/dev/null)
    if [[ -n "$results" ]]; then
      echo "[Web search results for context:]
$results

[User question:] $prompt"
      return
    fi
  fi
  echo "$prompt"
}

dispatch_ask() {
  local prompt="$1"
  local backend="${ACTIVE_BACKEND:-$(_auto_detect_backend)}"

  # Auto-inject web search if needed
  local enriched_prompt; enriched_prompt=$(_maybe_inject_search "$prompt")

  local response
  case "$backend" in
    gguf)    response=$(ask_gguf "$enriched_prompt" 2>/dev/null) ;;
    pytorch) response=$(ask_pytorch "$enriched_prompt" 2>/dev/null) ;;
    openai)  response=$(ask_openai "$enriched_prompt" 2>/dev/null) ;;
    claude)  response=$(ask_claude "$enriched_prompt" 2>/dev/null) ;;
    gemini)  response=$(ask_gemini "$enriched_prompt" 2>/dev/null) ;;
    hf)      response=$(ask_hf "$enriched_prompt" 2>/dev/null) ;;
    diffusers)
      cmd_imagine "$enriched_prompt"
      response="[Image generated]"
      ;;
    *)
      if [[ -n "${OPENAI_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="openai"; response=$(ask_openai "$enriched_prompt" 2>/dev/null)
      elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="claude"; response=$(ask_claude "$enriched_prompt" 2>/dev/null)
      elif [[ -n "${GEMINI_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="gemini"; response=$(ask_gemini "$enriched_prompt" 2>/dev/null)
      else
        err "No backend configured. Set API key or model."
        return 1
      fi
      ;;
  esac

  echo "$response"
  log_history "user" "$prompt"
  log_history "assistant" "$response"
  _save_session_turn "$prompt" "$response"
  [[ -n "$CURRENT_CHAT_FILE" ]] && _chat_append "assistant" "$response"

  # Background TTM batch train if enabled
  [[ "$TTM_AUTO_TRAIN" == "1" ]] && _ttm_train_batch &>/dev/null &
}

# ════════════════════════════════════════════════════════════════════════════════
#  CANVAS — Code workspace with AI assistance
# ════════════════════════════════════════════════════════════════════════════════
cmd_canvas() {
  local sub="${1:-status}"; shift || true
  case "$sub" in
    new)
      local name="${1:-canvas_$(date +%H%M%S)}"; local lang="${2:-python}"
      local file="$CANVAS_DIR/${name}.${lang}"
      touch "$file"; CANVAS_ACTIVE="$file"; save_config
      ok "Canvas: $file"
      ;;
    open)
      local f="${1:-}"; [[ -z "$f" ]] && { err "File required"; return 1; }
      [[ ! -f "$f" ]] && { err "Not found: $f"; return 1; }
      CANVAS_ACTIVE="$f"; save_config; ok "Canvas: $f"
      ;;
    edit)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      "${EDITOR:-nano}" "$CANVAS_ACTIVE"
      ;;
    show)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      hdr "Canvas: $CANVAS_ACTIVE"
      cat -n "$CANVAS_ACTIVE"
      ;;
    run)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local ext="${CANVAS_ACTIVE##*.}"
      case "$ext" in
        py|python) python3 "$CANVAS_ACTIVE" ;;
        sh|bash)   bash "$CANVAS_ACTIVE" ;;
        js|ts)     node "$CANVAS_ACTIVE" 2>/dev/null || npx ts-node "$CANVAS_ACTIVE" ;;
        c)         gcc "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        cpp)       g++ "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        *)         err "Unknown extension: $ext" ;;
      esac
      ;;
    ask)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local task="$*"
      [[ -z "$task" ]] && { read -rp "Task: " task; }
      local current_code=""
      [[ -s "$CANVAS_ACTIVE" ]] && current_code=$(cat "$CANVAS_ACTIVE")
      local prompt
      if [[ -z "$current_code" ]]; then
        prompt="Write code for this task. Return ONLY the code, no explanation, no markdown fences.

Task: $task"
      else
        prompt="Here is the current code:
\`\`\`
$current_code
\`\`\`

Modify it for this task. Return ONLY the complete updated code, no explanation, no markdown fences.

Task: $task"
      fi
      info "Generating code..."
      local result; result=$(dispatch_ask "$prompt" 2>/dev/null)
      # Strip markdown fences
      result=$(echo "$result" | sed 's/^```[a-z]*$//' | sed 's/^```$//')
      echo "$result" > "$CANVAS_ACTIVE"
      ok "Canvas updated. Lines: $(wc -l < "$CANVAS_ACTIVE")"
      cat -n "$CANVAS_ACTIVE"
      ;;
    diff)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      git diff "$CANVAS_ACTIVE" 2>/dev/null || diff /dev/null "$CANVAS_ACTIVE"
      ;;
    save)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local dest="${1:-$AI_OUTPUT_DIR/$(basename "$CANVAS_ACTIVE")}"
      cp "$CANVAS_ACTIVE" "$dest"; ok "Saved: $dest"
      ;;
    list)
      hdr "Canvas files"
      for f in "$CANVAS_DIR"/*; do
        [[ -f "$f" ]] || continue
        local lines; lines=$(wc -l < "$f")
        local active=""
        [[ "$f" == "$CANVAS_ACTIVE" ]] && active=" ${BGREEN}◀ active${R}"
        printf "  %-40s %4d lines%b\n" "$(basename "$f")" "$lines" "$active"
      done
      ;;
    close) CANVAS_ACTIVE=""; save_config; ok "Canvas closed" ;;
    status)
      [[ -n "$CANVAS_ACTIVE" ]] && ok "Active: $CANVAS_ACTIVE ($(wc -l < "$CANVAS_ACTIVE" 2>/dev/null || echo 0) lines)" || info "No active canvas"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  FINE-TUNING PIPELINE
# ════════════════════════════════════════════════════════════════════════════════
cmd_finetune() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    prepare)
      local data="${1:-}"; [[ -z "$data" ]] && { err "Data file required"; return 1; }
      [[ ! -f "$data" ]] && { err "Not found: $data"; return 1; }
      local out="$FINETUNE_DIR/dataset.jsonl"
      info "Preparing dataset from $data..."
      "$PYTHON" - <<PYEOF
import json, re
data_file = "$data"
out_file = "$out"
records = []
with open(data_file) as f:
    content = f.read()
# Try JSONL
try:
    for line in content.splitlines():
        line = line.strip()
        if not line: continue
        obj = json.loads(line)
        if isinstance(obj, dict):
            txt = obj.get('text') or (obj.get('instruction','') + '\n' + obj.get('output',''))
            records.append({'text': txt})
    print(f"Loaded {len(records)} JSONL records")
except:
    # Try Q&A format
    pairs = re.split(r'### Human:', content)
    for p in pairs:
        if '### Assistant:' in p:
            parts = p.split('### Assistant:', 1)
            q = parts[0].strip(); a = parts[1].strip()
            if q and a:
                records.append({'text': f'### Human: {q}\n### Assistant: {a}'})
    if not records:
        # Plain text: split into chunks
        chunks = [content[i:i+512] for i in range(0,len(content),512)]
        records = [{'text': c} for c in chunks if c.strip()]
    print(f"Prepared {len(records)} records from plain text/QA")

with open(out_file, 'w') as f:
    for r in records:
        f.write(json.dumps(r) + '\n')
print(f"Saved: {out_file}")
PYEOF
      ;;
    start)
      local base="${1:-}"; local data="${2:-$FINETUNE_DIR/dataset.jsonl}"; local out="${3:-$FINETUNE_DIR/finetuned_$(date +%Y%m%d_%H%M%S)}"
      [[ -z "$base" ]] && { err "Base model required"; return 1; }
      [[ ! -f "$data" ]] && { err "Dataset not found: $data. Run: ai finetune prepare <data>"; return 1; }
      info "Starting LoRA fine-tune: $base → $out"
      mkdir -p "$out"
      BASE_MODEL="$base" DATA_FILE="$data" OUT_DIR="$out" "$PYTHON" - <<'PYEOF'
import os, json, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    from trl import SFTTrainer
except ImportError as e:
    print(f"Missing: {e}\nRun: ai install-deps"); sys.exit(1)
base = os.environ['BASE_MODEL']
data_file = os.environ['DATA_FILE']
out_dir   = os.environ['OUT_DIR']
tokenizer = AutoTokenizer.from_pretrained(base)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
lora = LoraConfig(task_type=TaskType.CAUSAL_LM,r=16,lora_alpha=32,lora_dropout=0.05,target_modules=["q_proj","k_proj","v_proj","o_proj"])
model = get_peft_model(model, lora)
records=[]
with open(data_file) as f:
    for line in f:
        obj=json.loads(line); txt=obj.get('text','')
        if txt: records.append({'text':txt})
ds = Dataset.from_list(records)
device='cuda' if torch.cuda.is_available() else 'cpu'
model=model.to(device)
args=TrainingArguments(output_dir=out_dir,num_train_epochs=3,per_device_train_batch_size=1,gradient_accumulation_steps=4,learning_rate=2e-4,fp16=(device=='cuda'),logging_steps=20,save_steps=200,save_total_limit=2,report_to='none')
trainer=SFTTrainer(model=model,args=args,train_dataset=ds,tokenizer=tokenizer,dataset_text_field='text',max_seq_length=512)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"Saved: {out_dir}")
PYEOF
      ;;
    merge)
      local adapter="${1:-}"; local base="${2:-}"; local out="${3:-${1}_merged}"
      [[ -z "$adapter" || -z "$base" ]] && { err "Usage: ai finetune merge <adapter> <base> [out]"; return 1; }
      info "Merging adapter into base model..."
      ADAPTER="$adapter" BASE="$base" OUT="$out" "$PYTHON" - <<'PYEOF'
import os,torch
from transformers import AutoTokenizer
from peft import PeftModel, AutoPeftModelForCausalLM
base=os.environ['BASE']; adapter=os.environ['ADAPTER']; out=os.environ['OUT']
model=AutoPeftModelForCausalLM.from_pretrained(adapter,torch_dtype=torch.float16)
merged=model.merge_and_unload()
merged.save_pretrained(out)
tok=AutoTokenizer.from_pretrained(base)
tok.save_pretrained(out)
print(f"Merged: {out}")
PYEOF
      ;;
    quantize)
      local model="${1:-}"; local quant="${2:-Q4_K_M}"
      [[ -z "$model" ]] && { err "Model path required"; return 1; }
      local script="$HOME/llama.cpp/convert_hf_to_gguf.py"
      [[ ! -f "$script" ]] && { err "llama.cpp not found at $HOME/llama.cpp"; return 1; }
      local out="${model}_${quant}.gguf"
      info "Quantizing to $quant..."
      "$PYTHON" "$script" "$model" --outfile "$out" --outtype "${quant,,}" && ok "GGUF: $out"
      ;;
    status)
      hdr "Fine-tune Status"
      [[ -f "$FINETUNE_DIR/dataset.jsonl" ]] && echo "  Dataset: $(wc -l < "$FINETUNE_DIR/dataset.jsonl") records" || echo "  Dataset: none"
      ls -td "$FINETUNE_DIR"/finetuned_*/ 2>/dev/null | head -5 | while read -r d; do
        echo "  Model: $(basename "$d") ($(du -sh "$d" 2>/dev/null | cut -f1))"
      done
      ;;
    *) echo "Usage: ai finetune prepare|start|merge|quantize|status" ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  IMAGE GENERATION
# ════════════════════════════════════════════════════════════════════════════════

# ════════════════════════════════════════════════════════════════════════════════
#  MODEL MANAGEMENT
# ════════════════════════════════════════════════════════════════════════════════
cmd_list_models() {
  hdr "Downloaded Models"
  local found=0
  while IFS= read -r f; do
    [[ -f "$f" ]] || continue; found=1
    local size; size=$(du -sh "$f" | cut -f1)
    local active=""; [[ "$f" == "$ACTIVE_MODEL" ]] && active=" ${BGREEN}◀ active${R}"
    printf "  ${B}%-50s${R} %6s%b\n" "$(basename "$f")" "$size" "$active"
  done < <(find "$MODELS_DIR" -name "*.gguf" 2>/dev/null)
  for d in "$MODELS_DIR"/*/; do
    [[ -d "$d" && -f "$d/config.json" ]] || continue; found=1
    local active=""; [[ "${d%/}" == "$ACTIVE_MODEL" ]] && active=" ${BGREEN}◀ active${R}"
    printf "  ${B}%-50s${R} (pytorch)%b\n" "$(basename "$d")" "$active"
  done
  [[ $found -eq 0 ]] && dim "  No models. Try: ai recommended download 1"
}

cmd_download() {
  local repo="${1:-}"; local file_filter="${2:-}"
  [[ -z "$repo" ]] && { read -rp "HuggingFace repo (user/model): " repo; }
  [[ -z "$repo" ]] && { err "Repo required"; return 1; }
  if [[ "$repo" =~ ^sk-|^hf_|^AIza ]]; then
    local key_type="OPENAI_API_KEY"
    [[ "$repo" =~ ^hf_ ]] && key_type="HF_TOKEN"
    [[ "$repo" =~ ^AIza ]] && key_type="GEMINI_API_KEY"
    _set_key "$key_type" "$repo"; return 0
  fi
  mkdir -p "$MODELS_DIR"
  if [[ -z "$file_filter" ]]; then
    local files
    files=$(curl -sS "https://huggingface.co/api/models/${repo}" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
gguf=[s['rfilename'] for s in d.get('siblings',[]) if s['rfilename'].endswith('.gguf')]
q4=[f for f in gguf if 'Q4_K_M' in f or 'q4_k_m' in f]
print(q4[0] if q4 else (gguf[0] if gguf else ''))
" 2>/dev/null)
    [[ -n "$files" ]] && file_filter="$files"
  fi
  if [[ -n "$file_filter" ]]; then
    local url="https://huggingface.co/${repo}/resolve/main/${file_filter}"
    local dest="$MODELS_DIR/$(basename "$file_filter")"
    info "Downloading $file_filter..."
    curl -L --progress-bar "$url" ${HF_TOKEN:+-H "Authorization: Bearer $HF_TOKEN"} -o "$dest"
    ok "Downloaded: $dest"
    ACTIVE_MODEL="$dest"; ACTIVE_BACKEND="gguf"; save_config
  else
    info "Downloading full repo $repo (PyTorch)..."
    HF_REPO="$repo" HF_DEST="$MODELS_DIR/$repo" HF_TOKEN_VAL="${HF_TOKEN:-}" "$PYTHON" - <<'PYEOF'
import os,sys
try:
    from huggingface_hub import snapshot_download
except ImportError:
    print("huggingface_hub not installed"); sys.exit(1)
os.makedirs(os.environ['HF_DEST'], exist_ok=True)
snapshot_download(repo_id=os.environ['HF_REPO'], local_dir=os.environ['HF_DEST'],
                  token=os.environ.get('HF_TOKEN_VAL') or None)
print(f"Downloaded: {os.environ['HF_DEST']}")
PYEOF
    ACTIVE_MODEL="$MODELS_DIR/$repo"; ACTIVE_BACKEND="pytorch"; save_config
  fi
}

cmd_recommended() {
  local sub="${1:-list}"; shift || true
  case "$sub" in
    download)
      local n="${1:-}"; [[ -z "$n" ]] && { err "Number required"; return 1; }
      local entry="${RECOMMENDED_MODELS[$n]:-}"; [[ -z "$entry" ]] && { err "No model #$n"; return 1; }
      cmd_download "$(echo "$entry" | cut -d'|' -f1)" ;;
    use)
      local n="${1:-}"; [[ -z "$n" ]] && { err "Number required"; return 1; }
      local entry="${RECOMMENDED_MODELS[$n]:-}"; [[ -z "$entry" ]] && { err "No model #$n"; return 1; }
      ACTIVE_MODEL="$(echo "$entry"|cut -d'|' -f1)"; ACTIVE_BACKEND="$(echo "$entry"|cut -d'|' -f2)"
      save_config; ok "Model: $ACTIVE_MODEL" ;;
    *)
      hdr "Recommended Models"
      for key in $(seq 1 14); do
        local entry="${RECOMMENDED_MODELS[$key]:-}"; [[ -z "$entry" ]] && continue
        printf "  ${B}%2d.${R} %-16s ${DIM}%-40s${R} %s\n" "$key" \
          "$(echo "$entry"|cut -d'|' -f3)" "$(echo "$entry"|cut -d'|' -f1)" "$(echo "$entry"|cut -d'|' -f4)"
      done
      echo ""; echo "  Download: ${B}ai recommended download <N>${R}"
      ;;
  esac
}

cmd_search_models() {
  local query="$*"; [[ -z "$query" ]] && { read -rp "Search: " query; }
  info "Searching: $query"
  curl -sS "https://huggingface.co/api/models?search=${query// /+}&limit=15&sort=downloads" 2>/dev/null | \
    python3 -c "
import json,sys
for i,m in enumerate(json.load(sys.stdin),1):
    print(f'  {i:2}. {m.get(\"modelId\",\"?\")}')
    print(f'      ↓{m.get(\"downloads\",0):,}  ♥{m.get(\"likes\",0)}  [{', '.join(m.get(\"tags\",[])[:3])}]')
" 2>/dev/null
}

cmd_upload() {
  local path="${1:-}"; local repo="${2:-}"; local msg="${3:-upload via ai-cli}"
  [[ -z "$path" || -z "$repo" ]] && { err "Usage: ai upload <path> <user/repo>"; return 1; }
  [[ ! -e "$path" ]] && { err "Not found: $path"; return 1; }
  local hf_key="${HF_TOKEN:-}"; [[ -z "$hf_key" ]] && { err "HF_TOKEN not set"; return 1; }
  HF_TOKEN_VAL="$hf_key" P="$path" REPO="$repo" MSG="$msg" "$PYTHON" - <<'PYEOF'
import os,sys
try:
    from huggingface_hub import HfApi
except ImportError:
    print("huggingface_hub not installed"); sys.exit(1)
api=HfApi(token=os.environ['HF_TOKEN_VAL'])
repo=os.environ['REPO']; p=os.environ['P']; msg=os.environ['MSG']
try: api.create_repo(repo_id=repo,exist_ok=True,private=False)
except: pass
if os.path.isdir(p): api.upload_folder(folder_path=p,repo_id=repo,commit_message=msg)
else: api.upload_file(path_or_fileobj=p,path_in_repo=os.path.basename(p),repo_id=repo,commit_message=msg)
print(f"Uploaded: https://huggingface.co/{repo}")
PYEOF
}

cmd_model_info() {
  local m="${1:-$ACTIVE_MODEL}"; [[ -z "$m" ]] && { err "No model"; return 1; }
  hdr "Model: $m"
  if [[ -f "$m" ]]; then
    echo "  File: $m  ($(du -sh "$m"|cut -f1))"
  elif [[ -d "$m" && -f "$m/config.json" ]]; then
    python3 -c "import json; [print(f'  {k}: {v}') for k,v in list(json.load(open('$m/config.json')).items())[:15]]" 2>/dev/null
  else
    curl -sS "https://huggingface.co/api/models/$m" 2>/dev/null | python3 -c "
import json,sys; d=json.load(sys.stdin)
print(f'  ID: {d.get(\"modelId\",\"?\")}')
print(f'  Downloads: {d.get(\"downloads\",0):,}')
print(f'  Tags: {', '.join(d.get(\"tags\",[])[:5])}')
" 2>/dev/null
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
#  INSTALL / UNINSTALL
# ════════════════════════════════════════════════════════════════════════════════
cmd_install_deps() {
  local force=0 cpu_only=0 no_torch=0
  for a in "$@"; do
    [[ "$a" == "--force"    ]] && force=1
    [[ "$a" == "--cpu-only" ]] && cpu_only=1
    [[ "$a" == "--no-torch" ]] && no_torch=1
  done
  hdr "Installing AI CLI v${VERSION} dependencies"; echo ""
  if command -v apt-get &>/dev/null; then
    sudo apt-get install -y -q python3 python3-pip git cmake build-essential \
      ffmpeg curl jq espeak libsndfile1 2>/dev/null || true
  elif command -v brew &>/dev/null; then
    brew install python3 cmake ffmpeg jq espeak 2>/dev/null || true
  fi
  [[ -z "$PYTHON" ]] && { err "Python 3.10+ required"; return 1; }
  if [[ $no_torch -eq 0 ]]; then
    local ca; ca=$(detect_cuda_arch)
    if [[ $cpu_only -eq 1 ]] || (( ca == 0 )); then
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu --break-system-packages -q 2>/dev/null || true
    elif (( ca >= 80 )); then
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cu121 --break-system-packages -q 2>/dev/null || true
    elif (( ca >= 75 )); then
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cu118 --break-system-packages -q 2>/dev/null || true
    else
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu --break-system-packages -q 2>/dev/null || true
    fi
  fi
  "$PYTHON" -m pip install transformers tokenizers accelerate safetensors datasets \
    optimum huggingface_hub peft trl bitsandbytes diffusers Pillow \
    openai anthropic google-generativeai tiktoken openai-whisper \
    pyttsx3 soundfile pydub --break-system-packages -q 2>/dev/null || true
  local ca; ca=$(detect_cuda_arch)
  if (( ca >= 61 )); then
    CMAKE_ARGS="-DLLAMA_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=${ca}" \
      "$PYTHON" -m pip install llama-cpp-python --no-cache-dir --force-reinstall \
      --break-system-packages -q 2>/dev/null || \
    "$PYTHON" -m pip install llama-cpp-python --break-system-packages -q 2>/dev/null || true
  else
    "$PYTHON" -m pip install llama-cpp-python --break-system-packages -q 2>/dev/null || true
  fi
  echo ""
  ok "Installation complete!"
  echo "  Quick start: ai recommended → ai recommended download 1 → ai -gui"
}

cmd_uninstall() {
  if [[ $EUID -ne 0 ]]; then err "Requires sudo: sudo ai -uninstall"; return 1; fi
  echo -e "${BRED}${B}╔══════════════════════════════════════════════╗${R}"
  echo -e "${BRED}${B}║   ⚠  AI CLI UNINSTALL — CANNOT BE UNDONE   ║${R}"
  echo -e "${BRED}${B}╚══════════════════════════════════════════════╝${R}"
  echo ""
  echo "  Will remove: /usr/local/bin/ai"
  echo "  Config ($CONFIG_DIR) and models ($MODELS_DIR) are optional"
  echo ""
  read -rp "  Type CONFIRM to proceed: " confirm
  [[ "$confirm" != "CONFIRM" ]] && { info "Cancelled"; return 0; }
  read -rp "  Remove config/sessions/chats? [y/N]: " rm_cfg
  read -rp "  Remove downloaded models? [y/N]: " rm_mdl
  rm -f /usr/local/bin/ai && ok "Removed /usr/local/bin/ai"
  [[ "$rm_cfg" =~ ^[Yy]$ ]] && rm -rf "$CONFIG_DIR" && ok "Removed $CONFIG_DIR"
  [[ "$rm_mdl" =~ ^[Yy]$ ]] && rm -rf "$MODELS_DIR" && ok "Removed $MODELS_DIR"
  ok "Uninstalled."
}

# ════════════════════════════════════════════════════════════════════════════════
#  STATUS
# ════════════════════════════════════════════════════════════════════════════════
cmd_status() {
  hdr "AI CLI v${VERSION} — Status"; echo ""
  printf "  %-20s %s\n" "OS:"       "$(uname -s -r)"
  printf "  %-20s %s\n" "Python:"   "${PYTHON:-not found}"
  printf "  %-20s %s\n" "llama.cpp:" "${LLAMA_BIN:-not found}"
  printf "  %-20s %s\n" "ffmpeg:"   "$(command -v ffmpeg 2>/dev/null || echo 'not found')"
  echo ""
  if command -v nvidia-smi &>/dev/null; then
    printf "  %-20s %s\n" "GPU:" "$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null|head -1)"
    printf "  %-20s %s\n" "VRAM:" "$(nvidia-smi --query-gpu=memory.total --format=csv,noheader 2>/dev/null|head -1)"
    printf "  %-20s %s\n" "Compute:" "$CUDA_ARCH"
    if (( CUDA_ARCH >= 61 )); then
      printf "  %-20s ${BGREEN}✓ CUDA supported${R}\n" "Support:"
    else
      printf "  %-20s ${BRED}✗ Legacy GPU (CPU only)${R}\n" "Support:"
    fi
  else
    printf "  %-20s %s\n" "GPU:" "none"
  fi
  echo ""
  printf "  %-20s %s\n" "Active model:"  "${ACTIVE_MODEL:-not set}"
  printf "  %-20s %s\n" "Backend:"       "${ACTIVE_BACKEND:-auto}"
  printf "  %-20s %s\n" "Session:"       "${ACTIVE_SESSION:-default}"
  printf "  %-20s %s\n" "Persona:"       "${ACTIVE_PERSONA:-default}"
  printf "  %-20s %s\n" "Canvas:"        "${CANVAS_ACTIVE:-none}"
  printf "  %-20s %s\n" "GUI theme:"     "${GUI_THEME:-dark}"
  echo ""
  printf "  %-20s %s (v%s)\n" "TTM:"  "$TTM_AUTO_TRAIN" "$TTM_VERSION"
  printf "  %-20s %s (v%s)\n" "MTM:"  "$MTM_AUTO_TRAIN" "$MTM_VERSION"
  printf "  %-20s %s (v%s)\n" "Mtm:"  "$MMTM_AUTO_TRAIN" "$MMTM_VERSION"
  printf "  %-20s %s → %s\n" "HF sync:" "$HF_DATASET_SYNC" "$HF_DATASET_REPO"
  echo ""
  printf "  %-20s %s\n" "Temperature:"  "$TEMPERATURE"
  printf "  %-20s %s\n" "Max tokens:"   "$MAX_TOKENS"
  printf "  %-20s %s\n" "Context:"      "$CONTEXT_SIZE"
  printf "  %-20s %s\n" "GPU layers:"   "$GPU_LAYERS"
  echo ""
  hdr "API Keys"
  for k in OPENAI_API_KEY ANTHROPIC_API_KEY GEMINI_API_KEY HF_TOKEN HF_DATASET_KEY BRAVE_API_KEY; do
    local v; v=$(eval "echo \"\${$k:-}\"")
    if [[ -n "$v" ]]; then printf "  %-22s ${BGREEN}set${R} (%s…%s)\n" "$k:" "${v:0:4}" "${v: -4}"
    else printf "  %-22s ${DIM}not set${R}\n" "$k:"; fi
  done
  echo ""
  printf "  %-20s %s\n" "GGUF models:" "$(find "$MODELS_DIR" -name "*.gguf" 2>/dev/null | wc -l)"
  printf "  %-20s %s\n" "Chat logs:"   "$(ls "$CHAT_LOGS_DIR"/*.jsonl 2>/dev/null | wc -l)"
}

# ════════════════════════════════════════════════════════════════════════════════
#  HELP
# ════════════════════════════════════════════════════════════════════════════════
show_help() {
  echo -e "${B}${BWHITE}AI CLI v${VERSION}${R} — Chat · Vision · Audio · Video · Canvas · TTM · MTM · Mtm"
  echo ""
  echo -e "${B}${BCYAN}ASKING & CHAT${R}"
  echo "  ai ask <prompt>                    — Ask a question"
  echo "  ai chat                            — Interactive chat"
  echo "  ai -C [name|auto] ask <prompt>     — Named chat (saved as JSONL)"
  echo "  ai chat-list / chat-show / chat-delete"
  echo "  ai code <prompt> [--run]           — Generate & optionally run code"
  echo "  ai pipe / review / explain / summarize / translate"
  echo ""
  echo -e "${B}${BCYAN}MEDIA${R}"
  echo "  ai audio transcribe/tts/analyze/convert/extract/ask/play/info"
  echo "  ai video analyze/transcribe/caption/extract/trim/convert/ask/summary"
  echo "  ai vision ask/ocr/caption/compare"
  echo "  ai imagine <prompt>"
  echo ""
  echo -e "${B}${BCYAN}TRAINED MODELS${R}"
  echo "  ${B}TTM${R}  (~179.35M — any GPU/CPU)      ai ttm <cmd>"
  echo "  ${B}MTM${R}  (~0.61B  — GTX 1080 fp16)     ai mtm <cmd>"
  echo "  ${B}Mtm${R}  (~1.075B — RTX 2080+ bf16)    ai Mtm <cmd>   (case sensitive!)"
  echo ""
  echo "  Commands: pretrain [c1] [c2]  enable  disable  train-now"
  echo "            upload [v]  create-repo  status  load [v]"
  echo "            set-custom1 <hf-id>   set-custom2 <hf-id>"
  echo ""
  echo "  Load shortcuts:"
  echo "    ai -TTM / ai -MTM / ai -Mtm"
  echo ""
  echo "  Pretraining datasets (6 standard + 2 custom):"
  echo "    1. TinyStories (6k)   2. CodeAlpaca (4k)   3. OpenOrca (3k)"
  echo "    4. The Stack Smol (3k) 5. FineWeb-Edu (4k) 6. Wikipedia-en (4k)"
  echo "    7. Custom 1 (user-defined HF id or local path)"
  echo "    8. Custom 2 (user-defined HF id or local path)"
  echo ""
  echo -e "${B}${BCYAN}CANVAS${R}"
  echo "  ai canvas new/open/ask/show/run/edit/diff/save/list/close"
  echo ""
  echo -e "${B}${BCYAN}MODELS${R}"
  echo "  ai model <name>  models  download  recommended  search-models  upload  model-info"
  echo "  ai model-create new/train/list/edit/info/delete/presets"
  echo ""
  echo -e "${B}${BCYAN}FINE-TUNING${R}"
  echo "  ai finetune prepare/start/merge/quantize/status"
  echo ""
  echo -e "${B}${BCYAN}SETTINGS${R}"
  echo "  ai config [key value]     ai keys [set KEY val]"
  echo "  ai session list/new/load  ai persona list/set/create"
  echo "  ai history [--search x]   ai status"
  echo "  ai install-deps [--cpu-only]"
  echo "  sudo ai -uninstall"
  echo ""
  echo -e "${B}${BCYAN}GUI${R}"
  echo "  ai -gui / ai gui   — Python curses GUI (click or keyboard)"
  echo "  Themes: dark (default) · light · hacker · matrix"
  echo "  ai config gui_theme <theme>"
  echo ""
  echo -e "${B}${BCYAN}HF DATASET SYNC${R}"
  echo "  ai config hf_dataset_sync 1   — Enable chat sync to $HF_DATASET_REPO"
  echo "  ai config dataset_key <key>   — Set write-only HF key"
}

# ════════════════════════════════════════════════════════════════════════════════
#  MISC HELPERS (personas, sessions, config, history, etc.)
# ════════════════════════════════════════════════════════════════════════════════
_set_key() {
  local var="$1"; local val="$2"; [[ -z "$val" ]] && return 0
  local tmpf; tmpf=$(mktemp)
  grep -v "^${var}=" "$KEYS_FILE" > "$tmpf" 2>/dev/null || true
  echo "${var}=\"${val}\"" >> "$tmpf"
  mv "$tmpf" "$KEYS_FILE"; chmod 600 "$KEYS_FILE"
  eval "${var}=\"${val}\""; ok "Set $var"
}
cmd_keys() {
  if [[ "${1:-}" == "set" ]]; then _set_key "${2:-}" "${3:-}"; return; fi
  hdr "API Key Status"
  for k in OPENAI_API_KEY ANTHROPIC_API_KEY GEMINI_API_KEY HF_TOKEN HF_DATASET_KEY BRAVE_API_KEY; do
    local v; v=$(eval "echo \"\${$k:-}\"")
    if [[ -n "$v" ]]; then printf "  %-22s ${BGREEN}set${R} (%s…%s)\n" "$k:" "${v:0:6}" "${v: -4}"
    else printf "  %-22s ${DIM}not set${R}\n" "$k:"; fi
  done
}
cmd_persona() {
  local sub="${1:-list}"; shift || true
  case "$sub" in
    list)
      hdr "Personas"
      for k in "${!BUILTIN_PERSONAS[@]}"; do
        local a=""; [[ "$k" == "${ACTIVE_PERSONA:-default}" ]] && a=" ${BGREEN}◀${R}"
        printf "  ${B}%-12s${R}%b\n" "$k" "$a"
      done
      for f in "$PERSONAS_DIR"/*; do
        [[ -f "$f" ]] || continue
        local a=""; [[ "$(basename "$f")" == "${ACTIVE_PERSONA:-}" ]] && a=" ${BGREEN}◀${R}"
        printf "  ${B}%-12s${R} (custom)%b\n" "$(basename "$f")" "$a"
      done ;;
    set)   ACTIVE_PERSONA="${1:-default}"; save_config; ok "Persona: $ACTIVE_PERSONA" ;;
    create)
      local n="${1:-}"; [[ -z "$n" ]] && { read -rp "Name: " n; }
      read -rp "System prompt: " p; echo "$p" > "$PERSONAS_DIR/$n"; ok "Created: $n" ;;
    edit)  "${EDITOR:-nano}" "$PERSONAS_DIR/${1:-$ACTIVE_PERSONA}" ;;
    clear) ACTIVE_PERSONA="default"; save_config; ok "Reset to default" ;;
  esac
}
cmd_session() {
  local sub="${1:-list}"; shift || true
  case "$sub" in
    list)
      hdr "Sessions"
      for f in "$SESSIONS_DIR"/*.json; do
        [[ -f "$f" ]] || continue
        local name; name=$(basename "$f" .json)
        local turns; turns=$(python3 -c "import json; print(len(json.load(open('$f')))//2)" 2>/dev/null || echo "?")
        local a=""; [[ "$name" == "$ACTIVE_SESSION" ]] && a=" ${BGREEN}◀ active${R}"
        printf "  ${B}%-25s${R} %s turns%b\n" "$name" "$turns" "$a"
      done ;;
    new)
      local n="${1:-session_$(date +%H%M%S)}"; ACTIVE_SESSION="$n"
      echo "[]" > "$SESSIONS_DIR/${n}.json"; save_config; ok "Session: $n" ;;
    load)  ACTIVE_SESSION="${1:-}"; save_config; ok "Loaded: $ACTIVE_SESSION" ;;
    delete)
      read -rp "Delete session '${1:-}'? [y/N]: " a
      [[ "$a" =~ ^[Yy]$ ]] && rm -f "$SESSIONS_DIR/${1:-}.json" && ok "Deleted" ;;
  esac
}
cmd_config() {
  if [[ $# -eq 0 ]]; then cat "$CONFIG_FILE" 2>/dev/null; return; fi
  local key="${1,,}"; local val="${2:-}"
  case "$key" in
    temperature|temp)        TEMPERATURE="$val"; save_config; ok "Temperature: $val" ;;
    max_tokens|tokens)       MAX_TOKENS="$val";  save_config ;;
    context|context_size)    CONTEXT_SIZE="$val"; save_config ;;
    gpu_layers|gpu)          GPU_LAYERS="$val";  save_config ;;
    stream)                  STREAM="$val";       save_config ;;
    tool_calling|tools)      TOOL_CALLING="$val"; save_config ;;
    web_search|websearch)    WEB_SEARCH_ENABLED="$val"; save_config ;;
    hf_dataset_sync|sync)    HF_DATASET_SYNC="$val"; save_config ;;
    ttm_auto_train)          TTM_AUTO_TRAIN="$val"; save_config ;;
    mtm_auto_train)          MTM_AUTO_TRAIN="$val"; save_config ;;
    mmtm_auto_train)         MMTM_AUTO_TRAIN="$val"; save_config ;;
    gui_theme|theme)         GUI_THEME="$val"; save_config ;;
    hf_dataset_key|dataset_key) HF_DATASET_KEY="$val"; save_config; ok "HF dataset key set" ;;
    *) err "Unknown key: $key" ;;
  esac
}
cmd_history() {
  local n=20 search=""
  while [[ $# -gt 0 ]]; do
    case "$1" in --n) n="$2"; shift 2 ;; --search) search="$2"; shift 2 ;; *) shift ;; esac
  done
  [[ ! -f "$LOG_FILE" ]] && { info "No history"; return; }
  if [[ -n "$search" ]]; then grep -i "$search" "$LOG_FILE" | tail -n "$n"
  else tail -n "$n" "$LOG_FILE"; fi
}
cmd_bench() {
  local prompt="${*:-Hello, how are you?}"; local runs=3; hdr "Benchmark"
  local total=0
  for (( i=1; i<=runs; i++ )); do
    local s; s=$(date +%s%N)
    dispatch_ask "$prompt" &>/dev/null
    local ms=$(( ( $(date +%s%N) - s ) / 1000000 ))
    printf "  Run %d: %dms\n" "$i" "$ms"; total=$(( total + ms ))
  done
  printf "  Average: %dms\n" $(( total / runs ))
}
cmd_serve() {
  local port=8080 host="127.0.0.1"
  while [[ $# -gt 0 ]]; do
    case "$1" in --port) port="$2"; shift 2 ;; --host) host="$2"; shift 2 ;; *) shift ;; esac
  done
  local model="${ACTIVE_MODEL:-}"; [[ -z "$model" ]] && { err "No model set"; return 1; }
  if [[ -n "$LLAMA_BIN" && "$LLAMA_BIN" != "llama_cpp_python" ]]; then
    local srv; srv=$(dirname "$LLAMA_BIN")/llama-server
    [[ -x "$srv" ]] && { info "Starting llama.cpp server on $host:$port"; "$srv" -m "$model" --host "$host" --port "$port" -c "$CONTEXT_SIZE" -ngl "$GPU_LAYERS"; return; }
  fi
  [[ -n "$PYTHON" ]] && "$PYTHON" -m llama_cpp.server --model "$model" --host "$host" --port "$port"
}

# ════════════════════════════════════════════════════════════════════════════════
#  MAIN DISPATCHER
# ════════════════════════════════════════════════════════════════════════════════
main() {
  # Handle -C [name] named chat flag
  if [[ "${1:-}" == "-C" ]]; then
    shift; local chat_name="${1:-auto}"; shift || true
    _chat_start "$chat_name"
  fi

  local cmd="${1:-help}"; shift || true

  case "$cmd" in
    # ── Asking ──────────────────────────────────────────────────────────────
    ask|a)      dispatch_ask "$*" ;;
    chat)       cmd_chat_interactive ;;
    code)
      local lang="" run=0 args=()
      while [[ $# -gt 0 ]]; do
        case "$1" in --lang) lang="$2"; shift 2 ;; --run) run=1; shift ;; *) args+=("$1"); shift ;; esac
      done
      local result; result=$(dispatch_ask "Write ${lang:-code}: ${args[*]}")
      echo "$result"
      if [[ $run -eq 1 ]]; then
        local ext="${lang:-python}"; ext="${ext//python/py}"
        local tmp; tmp=$(mktemp /tmp/ai_code_XXXX."$ext")
        echo "$result" | sed 's/^```[a-z]*$//' | sed 's/^```$//' > "$tmp"
        case "${lang:-python}" in python|py|"") python3 "$tmp" ;; bash|sh) bash "$tmp" ;; esac
        rm -f "$tmp"
      fi ;;
    review)   dispatch_ask "Code review for bugs, security, performance:
$(cat "${1:--}" 2>/dev/null)" ;;
    explain)  dispatch_ask "Explain in plain English:
$(cat "${1:--}" 2>/dev/null)" ;;
    summarize)
      local fmt="${3:-paragraph}"
      dispatch_ask "Summarize (format: $fmt):
$(cat "${1:--}" 2>/dev/null)" ;;
    translate) dispatch_ask "Translate to ${3:-English}: $1" ;;
    pipe)     cat | dispatch_ask "${*:-Summarize:}
$(cat)" ;;

    # ── Named chat management ────────────────────────────────────────────────
    chat-list)   cmd_chat_list ;;
    chat-show)   cmd_chat_show "$@" ;;
    chat-delete) cmd_chat_delete "$@" ;;

    # ── Media ────────────────────────────────────────────────────────────────
    audio)       cmd_audio "$@" ;;
    video)       cmd_video "$@" ;;
    vision)      cmd_vision "$@" ;;
    imagine)     cmd_imagine "$@" ;;
    tts)         _audio_tts "$@" ;;
    transcribe)  _audio_transcribe "$@" ;;

    # ── Canvas ───────────────────────────────────────────────────────────────
    canvas)  cmd_canvas "$@" ;;

    # ── Models ───────────────────────────────────────────────────────────────
    model)
      if [[ $# -gt 0 ]]; then
        ACTIVE_MODEL="$1"; [[ $# -gt 1 ]] && ACTIVE_BACKEND="$2"
        save_config; ok "Model: $ACTIVE_MODEL"
      else echo "Active: ${ACTIVE_MODEL:-not set}"; fi ;;
    models)          cmd_list_models ;;
    download)        cmd_download "$@" ;;
    recommended)     cmd_recommended "$@" ;;
    search-models)   cmd_search_models "$@" ;;
    upload)          cmd_upload "$@" ;;
    model-info)      cmd_model_info "$@" ;;
    model-create|create-model) cmd_model_create "$@" ;;

    # ── TTM / MTM / Mtm — case-sensitive ─────────────────────────────────────
    ttm|TTM)    cmd_ttm "$@" ;;
    mtm|MTM)    cmd_mtm "$@" ;;
    Mtm|MMTM)   cmd_Mtm "$@" ;;
    -TTM|--TTM) _tm_load "TTM" ;;
    -MTM|--MTM) _tm_load "MTM" ;;
    -Mtm|--Mtm) _tm_load "Mtm" ;;

    # ── Fine-tuning ───────────────────────────────────────────────────────────
    finetune|ft) cmd_finetune "$@" ;;

    # ── Sessions / Personas ───────────────────────────────────────────────────
    session)  cmd_session "$@" ;;
    persona)  cmd_persona "$@" ;;

    # ── Tools / Search ────────────────────────────────────────────────────────
    tools)      echo "Builtin tools: ${BUILTIN_TOOLS[*]}" ;;
    tool)       run_tool "${1:-}" "${2:-{}}" ;;
    websearch|search) cmd_websearch "$@" ;;

    # ── Settings ──────────────────────────────────────────────────────────────
    config)        cmd_config "$@" ;;
    keys)          cmd_keys "$@" ;;
    history)       cmd_history "$@" ;;
    clear-history) echo "[]" > "$SESSIONS_DIR/${ACTIVE_SESSION}.json"; ok "Cleared" ;;
    status)        cmd_status ;;
    install-deps)  cmd_install_deps "$@" ;;
    -uninstall|--uninstall|uninstall) cmd_uninstall ;;

    # ── GUI / Bench / Serve ───────────────────────────────────────────────────
    -gui|--gui|gui) cmd_gui ;;
    bench)  cmd_bench "$@" ;;
    serve)  cmd_serve "$@" ;;

    # ── Misc ──────────────────────────────────────────────────────────────────
    version|-v|--version) echo "AI CLI v${VERSION}" ;;
    help|-h|--help|"")    show_help ;;
    *)  dispatch_ask "$cmd $*" ;;
  esac
}

# ─── Background auto-train check (TTM/MTM/Mtm) ───────────────────────────────
_bg_auto_train() {
  local now; now=$(date +%s)
  for id in TTM MTM Mtm; do
    _tm_vars "$id" 2>/dev/null || continue
    local auto; auto=$(_tm_get_var "$TM_AUTO_TRAIN_VAR")
    [[ "$auto" != "1" ]] && continue
    local last_file="$TM_DIR/.last_train"
    local last=0
    [[ -f "$last_file" ]] && last=$(date -r "$last_file" +%s 2>/dev/null || stat -c %Y "$last_file" 2>/dev/null || echo 0)
    if (( now - last > 3600 )); then
      _tm_train_batch "$id" &>/dev/null & disown
    fi
  done
}

if [[ "${1:-}" != "install-deps" ]] && [[ "${1:-}" != "-uninstall" ]]; then
  _bg_auto_train
fi

main "$@"
