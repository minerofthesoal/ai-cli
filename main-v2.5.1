#!/usr/bin/env bash
# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║  AI.SH v2.5.1 — Universal AI CLI · TUI · Fine-tune · Canvas · TTM/MTM/Mtm  ║
# ║  GGUF·PyTorch·Diffusers·OpenAI·Claude·Gemini·HF·Audio·Video·Vision·GUI     ║
# ║  Pacman·XZ·GitHub·RLHF-v2·Multimodal·Papers·Canvas-v2·Dataset·KDE6·FLUX   ║
# ╚══════════════════════════════════════════════════════════════════════════════╝
# Linux/Mac:   chmod +x ai.sh && sudo cp ai.sh /usr/local/bin/ai
# Arch Linux:  pacman -S python python-pip git ffmpeg && ai install-deps
# Windows 10:  Run in Git Bash / WSL; see 'ai install-deps --windows' for setup
set -euo pipefail
VERSION="2.5.1"

# ════════════════════════════════════════════════════════════════════════════════
#  ENVIRONMENT DETECTION
# ════════════════════════════════════════════════════════════════════════════════
# ════════════════════════════════════════════════════════════════════════════════
#  PLATFORM DETECTION (Windows 10/11 CPU-only, Linux, macOS) :D
# ════════════════════════════════════════════════════════════════════════════════
detect_platform() {
  local os; os="$(uname -s 2>/dev/null || echo unknown)"
  case "$os" in
    MINGW*|MSYS*|CYGWIN*) echo "windows" ;;
    Darwin)               echo "macos"   ;;
    Linux)
      # Check if running inside WSL
      if grep -qi microsoft /proc/version 2>/dev/null; then echo "wsl"
      else echo "linux"; fi ;;
    *)                    echo "unknown" ;;
  esac
}
PLATFORM="$(detect_platform)"
IS_WINDOWS=0; IS_WSL=0; IS_MACOS=0
[[ "$PLATFORM" == "windows" ]] && IS_WINDOWS=1
[[ "$PLATFORM" == "wsl"     ]] && IS_WSL=1
[[ "$PLATFORM" == "macos"   ]] && IS_MACOS=1

# Windows-safe config root (falls back to APPDATA if set)
if [[ $IS_WINDOWS -eq 1 && -n "${APPDATA:-}" ]]; then
  _WIN_CONFIG_ROOT="$(cygpath -u "$APPDATA" 2>/dev/null || echo "$HOME")/ai-cli"
else
  _WIN_CONFIG_ROOT="$HOME/.config/ai-cli"
fi

find_python() {
  # Windows: try 'py' launcher first, then standard names
  if [[ $IS_WINDOWS -eq 1 ]]; then
    for c in "py -3.12" "py -3.11" "py -3.10" "py -3" python3.12 python3.11 python3.10 python3 python; do
      local p; p=$(command -v ${c%% *} 2>/dev/null) || continue
      local v; v=$($c -c "import sys; print(sys.version_info.major,sys.version_info.minor)" 2>/dev/null) || continue
      read -r ma mi <<< "$v"
      (( ma==3 && mi>=10 )) && { echo "$p"; return 0; }
    done
    echo ""; return
  fi
  for c in python3.12 python3.11 python3.10 python3 python; do
    local p; p=$(command -v "$c" 2>/dev/null) || continue
    local v; v=$("$p" -c "import sys; print(sys.version_info.major,sys.version_info.minor)" 2>/dev/null) || continue
    read -r ma mi <<< "$v"
    (( ma==3 && mi>=10 )) && { echo "$p"; return 0; }
  done; echo ""
}
find_llama() {
  for b in llama-cli llama llama-run llama-main llama.cpp; do
    command -v "$b" &>/dev/null && { command -v "$b"; return 0; }
  done
  for d in "$HOME/.local/bin" "$HOME/bin" "$HOME/llama.cpp/build/bin" \
            "$HOME/llama.cpp/build" "$HOME/llama.cpp" "/usr/local/bin" \
            "/opt/llama.cpp/bin" "/opt/homebrew/bin"; do
    for b in llama-cli llama llama-run main; do
      [[ -x "$d/$b" ]] && { echo "$d/$b"; return 0; }
    done
  done
  local py; py=$(find_python)
  [[ -n "$py" ]] && "$py" -c "import llama_cpp" 2>/dev/null && { echo "llama_cpp_python"; return 0; }
  echo ""
}
detect_cuda_arch() {
  local py; py=$(find_python)
  if [[ -n "$py" ]] && "$py" -c "import torch; torch.cuda.is_available()" 2>/dev/null; then
    "$py" -c "import torch; cc=torch.cuda.get_device_capability(0); print(cc[0]*10+cc[1])" 2>/dev/null || echo "0"
  elif command -v nvidia-smi &>/dev/null; then
    nvidia-smi --query-gpu=compute_cap --format=csv,noheader 2>/dev/null | head -1 | tr -d '.' || echo "0"
  else
    echo "0"
  fi
}

PYTHON="$(find_python)"
LLAMA_BIN="$(find_llama)"
CUDA_ARCH="$(detect_cuda_arch)"

# ════════════════════════════════════════════════════════════════════════════════
#  COLORS
# ════════════════════════════════════════════════════════════════════════════════
R="\033[0m"; B="\033[1m"; DIM="\033[2m"; IT="\033[3m"; UL="\033[4m"
BL="\033[5m"; INV="\033[7m"
BLACK="\033[30m";RED="\033[31m";GREEN="\033[32m";YELLOW="\033[33m"
BLUE="\033[34m";MAGENTA="\033[35m";CYAN="\033[36m";WHITE="\033[37m";GRAY="\033[90m"
BRED="\033[91m";BGREEN="\033[92m";BYELLOW="\033[93m";BBLUE="\033[94m"
BMAGENTA="\033[95m";BCYAN="\033[96m";BWHITE="\033[97m"
BG_BLACK="\033[40m";BG_RED="\033[41m";BG_GREEN="\033[42m";BG_YELLOW="\033[43m"
BG_BLUE="\033[44m";BG_MAGENTA="\033[45m";BG_CYAN="\033[46m";BG_WHITE="\033[47m"

# ════════════════════════════════════════════════════════════════════════════════
#  PATHS & CONFIG
# ════════════════════════════════════════════════════════════════════════════════
CONFIG_DIR="${AI_CLI_CONFIG:-$HOME/.config/ai-cli}"
CONFIG_FILE="$CONFIG_DIR/config.env"
KEYS_FILE="$CONFIG_DIR/keys.env"
LOG_FILE="$CONFIG_DIR/history.log"
SESSIONS_DIR="$CONFIG_DIR/sessions"
PERSONAS_DIR="$CONFIG_DIR/personas"
MODELS_DIR="${AI_CLI_MODELS:-$HOME/.ai-cli/models}"
AI_OUTPUT_DIR="${AI_OUTPUT_DIR:-$HOME/ai-outputs}"
CANVAS_DIR="$AI_OUTPUT_DIR/canvas"
FINETUNE_DIR="$CONFIG_DIR/finetune"
TOOLS_DIR="$CONFIG_DIR/tools"
PLUGINS_DIR="$CONFIG_DIR/plugins"
SEARCH_CACHE="$CONFIG_DIR/search_cache"
CUSTOM_MODELS_DIR="$CONFIG_DIR/custom_models"
TTM_DIR="$CONFIG_DIR/tiny_model"
MTM_DIR="$CONFIG_DIR/mini_model"
MMTM_DIR="$CONFIG_DIR/medium_model"
CHAT_LOGS_DIR="$CONFIG_DIR/chat_logs"
AUDIO_DIR="$AI_OUTPUT_DIR/audio"
VIDEO_DIR="$AI_OUTPUT_DIR/video"
DATASETS_DIR="$CONFIG_DIR/datasets"       # v2.4:   custom dataset storage
API_PID_FILE="$CONFIG_DIR/api.pid"        # v2.4:   LLM API server PID
API_KEYS_FILE="$CONFIG_DIR/api_keys.json" # v2.4.5: shareable API key store
MULTIAI_DIR="$CONFIG_DIR/multiai"         # v2.4.5: multi-AI conversation logs
RLHF_HF_DATASETS_FILE="$CONFIG_DIR/rlhf_hf_datasets.json" # v2.4.5: imported HF RLHF datasets
GITHUB_DIR="$CONFIG_DIR/github"                            # v2.5:   GitHub integration cache
PAPERS_DIR="$AI_OUTPUT_DIR/papers"                         # v2.5:   downloaded research papers
MULTIMODAL_DIR="$CONFIG_DIR/multimodal"                    # v2.5:   multimodal training
CANVAS_V2_DIR="$AI_OUTPUT_DIR/canvas_v2"                  # v2.5:   Canvas v2 workspaces
BUILD_DIR="$CONFIG_DIR/builds"                             # v2.5:   XZ bundle output

mkdir -p "$CONFIG_DIR" "$MODELS_DIR" "$SESSIONS_DIR" "$PERSONAS_DIR" \
         "$AI_OUTPUT_DIR" "$CANVAS_DIR" "$FINETUNE_DIR" "$TOOLS_DIR" \
         "$PLUGINS_DIR" "$SEARCH_CACHE" "$CUSTOM_MODELS_DIR" \
         "$TTM_DIR" "$MTM_DIR" "$MMTM_DIR" \
         "$CHAT_LOGS_DIR" "$AUDIO_DIR" "$VIDEO_DIR" "$DATASETS_DIR" \
         "$MULTIAI_DIR" "$GITHUB_DIR" "$PAPERS_DIR" \
         "$MULTIMODAL_DIR" "$CANVAS_V2_DIR" "$BUILD_DIR"
touch "$KEYS_FILE" && chmod 600 "$KEYS_FILE"

[[ -f "$CONFIG_FILE" ]] && source "$CONFIG_FILE"
[[ -f "$KEYS_FILE"   ]] && source "$KEYS_FILE"

# Runtime vars
ACTIVE_MODEL="${ACTIVE_MODEL:-}"
ACTIVE_BACKEND="${ACTIVE_BACKEND:-}"
ACTIVE_PERSONA="${ACTIVE_PERSONA:-}"
ACTIVE_SESSION="${ACTIVE_SESSION:-default}"
MAX_TOKENS="${MAX_TOKENS:-2048}"
TEMPERATURE="${TEMPERATURE:-0.7}"
TOP_P="${TOP_P:-0.9}"
REPEAT_PENALTY="${REPEAT_PENALTY:-1.1}"
CONTEXT_SIZE="${CONTEXT_SIZE:-4096}"
GPU_LAYERS="${GPU_LAYERS:--1}"
THREADS="${THREADS:-$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || python3 -c "import os;print(os.cpu_count())" 2>/dev/null || echo 4)}"
STREAM="${STREAM:-1}"
VERBOSE="${VERBOSE:-0}"
TOOL_CALLING="${TOOL_CALLING:-1}"
WEB_SEARCH_ENABLED="${WEB_SEARCH_ENABLED:-1}"
SEARCH_ENGINE="${SEARCH_ENGINE:-ddg}"
CANVAS_ACTIVE="${CANVAS_ACTIVE:-}"
HF_DATASET_SYNC="${HF_DATASET_SYNC:-0}"
HF_DATASET_REPO="${HF_DATASET_REPO:-ray0rf1re/cli}"
HF_DATASET_KEY="${HF_DATASET_KEY:-}"
GUI_THEME="${GUI_THEME:-dark}"

# v2.5: GitHub integration
GITHUB_TOKEN="${GITHUB_TOKEN:-}"
GITHUB_USER="${GITHUB_USER:-}"
GITHUB_DEFAULT_BRANCH="${GITHUB_DEFAULT_BRANCH:-main}"

# v2.5: Research papers
PAPERS_DEFAULT_SOURCES="${PAPERS_DEFAULT_SOURCES:-arxiv,pmc,biorxiv,core,openalex}"
PAPERS_CITATION_FORMAT="${PAPERS_CITATION_FORMAT:-apa}"

# v2.5: Multimodal training
MULTIMODAL_VL_MODEL="${MULTIMODAL_VL_MODEL:-Qwen/Qwen2-VL-2B-Instruct}"
MULTIMODAL_T2I_MODEL="${MULTIMODAL_T2I_MODEL:-stabilityai/stable-diffusion-xl-base-1.0}"

# TTM (Tiny — 179.35M)
TTM_AUTO_TRAIN="${TTM_AUTO_TRAIN:-0}"
TTM_PRETRAINED="${TTM_PRETRAINED:-0}"
TTM_VERSION="${TTM_VERSION:-0}"

# MTM (Mini — 0.61B — GTX 1080 optimized)
MTM_AUTO_TRAIN="${MTM_AUTO_TRAIN:-0}"
MTM_PRETRAINED="${MTM_PRETRAINED:-0}"
MTM_VERSION="${MTM_VERSION:-0}"

# Mtm (Medium — 1.075B — RTX 2080+ optimized)
MMTM_AUTO_TRAIN="${MMTM_AUTO_TRAIN:-0}"
MMTM_PRETRAINED="${MMTM_PRETRAINED:-0}"
MMTM_VERSION="${MMTM_VERSION:-0}"

save_config() {
  cat > "$CONFIG_FILE" <<CONF
ACTIVE_MODEL="${ACTIVE_MODEL}"
ACTIVE_BACKEND="${ACTIVE_BACKEND}"
ACTIVE_PERSONA="${ACTIVE_PERSONA}"
ACTIVE_SESSION="${ACTIVE_SESSION}"
MAX_TOKENS="${MAX_TOKENS}"
TEMPERATURE="${TEMPERATURE}"
TOP_P="${TOP_P}"
REPEAT_PENALTY="${REPEAT_PENALTY}"
CONTEXT_SIZE="${CONTEXT_SIZE}"
GPU_LAYERS="${GPU_LAYERS}"
THREADS="${THREADS}"
STREAM="${STREAM}"
VERBOSE="${VERBOSE}"
TOOL_CALLING="${TOOL_CALLING}"
WEB_SEARCH_ENABLED="${WEB_SEARCH_ENABLED}"
SEARCH_ENGINE="${SEARCH_ENGINE}"
CANVAS_ACTIVE="${CANVAS_ACTIVE}"
HF_DATASET_SYNC="${HF_DATASET_SYNC}"
HF_DATASET_REPO="${HF_DATASET_REPO}"
HF_DATASET_KEY="${HF_DATASET_KEY}"
GUI_THEME="${GUI_THEME}"
TTM_AUTO_TRAIN="${TTM_AUTO_TRAIN}"
TTM_PRETRAINED="${TTM_PRETRAINED}"
TTM_VERSION="${TTM_VERSION}"
MTM_AUTO_TRAIN="${MTM_AUTO_TRAIN}"
MTM_PRETRAINED="${MTM_PRETRAINED}"
MTM_VERSION="${MTM_VERSION}"
MMTM_AUTO_TRAIN="${MMTM_AUTO_TRAIN}"
MMTM_PRETRAINED="${MMTM_PRETRAINED}"
MMTM_VERSION="${MMTM_VERSION}"
RLHF_AUTO="${RLHF_AUTO}"
RLHF_JUDGE="${RLHF_JUDGE}"
RLHF_MANUAL_ENABLED="${RLHF_MANUAL_ENABLED}"
RLHF_REWARD_THRESHOLD="${RLHF_REWARD_THRESHOLD}"
RLHF_ACTIVE_HF_DATASET="${RLHF_ACTIVE_HF_DATASET}"
RCLICK_ENABLED="${RCLICK_ENABLED}"
RCLICK_VL_MODEL="${RCLICK_VL_MODEL}"
RCLICK_CUSTOM_MODEL="${RCLICK_CUSTOM_MODEL}"
RCLICK_KEYBIND="${RCLICK_KEYBIND}"
AUP_REPO="${AUP_REPO}"
AUP_CHECK_INTERVAL="${AUP_CHECK_INTERVAL}"
AUP_LAST_CHECK="${AUP_LAST_CHECK}"
AGENT_MODE="${AGENT_MODE}"
AGENT_MAX_STEPS="${AGENT_MAX_STEPS}"
AGENT_SEARCH_ENGINE="${AGENT_SEARCH_ENGINE}"
PRETRAIN_CUSTOM_1="${PRETRAIN_CUSTOM_1}"
PRETRAIN_CUSTOM_2="${PRETRAIN_CUSTOM_2}"
API_HOST="${API_HOST}"
API_PORT="${API_PORT}"
API_KEY="${API_KEY}"
API_CORS="${API_CORS}"
API_SHARE_ENABLED="${API_SHARE_ENABLED}"
API_SHARE_HOST="${API_SHARE_HOST}"
API_SHARE_PORT="${API_SHARE_PORT}"
API_SHARE_RATE_LIMIT="${API_SHARE_RATE_LIMIT}"
CPU_ONLY_MODE="${CPU_ONLY_MODE}"
MULTIAI_ROUNDS="${MULTIAI_ROUNDS}"
MULTIAI_SAVE_DATASET="${MULTIAI_SAVE_DATASET}"
MULTIAI_RLHF_TRAIN="${MULTIAI_RLHF_TRAIN}"
GITHUB_TOKEN="${GITHUB_TOKEN}"
GITHUB_USER="${GITHUB_USER}"
GITHUB_DEFAULT_BRANCH="${GITHUB_DEFAULT_BRANCH}"
PAPERS_DEFAULT_SOURCES="${PAPERS_DEFAULT_SOURCES}"
PAPERS_CITATION_FORMAT="${PAPERS_CITATION_FORMAT}"
MULTIMODAL_VL_MODEL="${MULTIMODAL_VL_MODEL}"
MULTIMODAL_T2I_MODEL="${MULTIMODAL_T2I_MODEL}"
CONF
}

log_history() { local role="$1"; local msg="$2"; echo "$(date -Iseconds) [$role] $msg" >> "$LOG_FILE"; }
err()  { echo -e "${BRED}✗ $*${R}" >&2; }
ok()   { echo -e "${BGREEN}✓ $*${R}"; }
info() { echo -e "${BCYAN}ℹ $*${R}"; }
warn() { echo -e "${BYELLOW}⚠ $*${R}"; }
hdr()  { echo -e "${B}${BWHITE}$*${R}"; }
dim()  { echo -e "${DIM}$*${R}"; }

# ════════════════════════════════════════════════════════════════════════════════
#  RECOMMENDED MODELS
# ════════════════════════════════════════════════════════════════════════════════
declare -A RECOMMENDED_MODELS
RECOMMENDED_MODELS=(
  # ── Tiny / CPU-friendly LLMs ──────────────────────────────────────────────
  [1]="Amu/supertiny-llama3-0.25B-v0.1|gguf|0.25B|Supertiny Llama3 — runs on ANY CPU"
  [2]="bartowski/Phi-3.1-mini-128k-instruct-GGUF|gguf|3.8B|Phi-3 Mini 128k context"
  [3]="bartowski/SmolLM2-1.7B-Instruct-GGUF|gguf|1.7B|SmolLM2 — ultra-fast on CPU"
  [4]="bartowski/Qwen2.5-1.5B-Instruct-GGUF|gguf|1.5B|Qwen2.5 1.5B — multilingual tiny"
  # ── General-purpose LLMs (7–9B) ───────────────────────────────────────────
  [5]="TheBloke/Mistral-7B-Instruct-v0.2-GGUF|gguf|7B|Mistral 7B — top general-purpose"
  [6]="bartowski/Meta-Llama-3.1-8B-Instruct-GGUF|gguf|8B|Llama 3.1 8B — strong reasoning"
  [7]="Qwen/Qwen2-7B-Instruct-GGUF|gguf|7B|Qwen2 7B — multilingual + coding"
  [8]="bartowski/gemma-2-9b-it-GGUF|gguf|9B|Gemma 2 9B — Google model"
  [9]="bartowski/Qwen2.5-7B-Instruct-GGUF|gguf|7B|Qwen2.5 7B — best 7B 2025"
  [10]="bartowski/Mistral-Nemo-Instruct-2407-GGUF|gguf|12B|Mistral Nemo 12B — long context"
  # ── Coding LLMs ───────────────────────────────────────────────────────────
  [11]="bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF|gguf|16B|DeepSeek Coder V2 — code"
  [12]="bartowski/Codestral-22B-v0.1-GGUF|gguf|22B|Codestral 22B — Mistral code model"
  [13]="bartowski/Qwen2.5-Coder-7B-Instruct-GGUF|gguf|7B|Qwen2.5 Coder 7B — top coder"
  # ── Vision / Multimodal LLMs ──────────────────────────────────────────────
  [14]="Qwen/Qwen2-VL-7B-Instruct|hf|7B|Qwen2-VL 7B — best open vision-language"
  [15]="microsoft/Phi-3.5-vision-instruct|hf|4.2B|Phi-3.5 Vision — small + capable"
  [16]="llava-hf/llava-1.5-7b-hf|hf|7B|LLaVA 1.5 7B — classic vision model"
  # ── Image Generation ──────────────────────────────────────────────────────
  [17]="stabilityai/stable-diffusion-xl-base-1.0|diffusers|SDXL|SDXL 1.0 — high quality"
  [18]="black-forest-labs/FLUX.1-schnell|diffusers|FLUX|FLUX Schnell — fast 4-step"
  [19]="black-forest-labs/FLUX.1-dev|diffusers|FLUX|FLUX Dev — best quality"
  [20]="stabilityai/stable-diffusion-2-1|diffusers|SD2|SD 2.1 — lightweight option"
  # ── Audio / Speech ────────────────────────────────────────────────────────
  [21]="openai/whisper-base|hf|74M|Whisper base — fast speech-to-text"
  [22]="openai/whisper-large-v3|hf|1.5B|Whisper Large v3 — best transcription"
  [23]="suno/bark|hf|TTS|Bark — realistic multi-lingual TTS"
  # ── Cloud API Models ──────────────────────────────────────────────────────
  [24]="gpt-4o|openai|API|GPT-4o with vision"
  [25]="gpt-4o-mini|openai|API|GPT-4o mini — fast and affordable"
  [26]="claude-sonnet-4-5|claude|API|Claude Sonnet 4.5 — top reasoning"
  [27]="claude-haiku-4-5-20251001|claude|API|Claude Haiku 4.5 — ultra-fast"
  [28]="gemini-2.0-flash|gemini|API|Gemini 2.0 Flash — Google best"
)

# ════════════════════════════════════════════════════════════════════════════════
#  BUILTIN PERSONAS
# ════════════════════════════════════════════════════════════════════════════════
declare -A BUILTIN_PERSONAS
BUILTIN_PERSONAS=(
  [default]="You are a helpful, friendly AI assistant."
  [dev]="You are an expert software engineer. Write clean, secure, well-documented code. Prefer idiomatic solutions. Flag potential bugs and security issues."
  [researcher]="You are a rigorous researcher. Cite your reasoning. Acknowledge uncertainty. Be precise and thorough."
  [writer]="You are a skilled writer. Prioritize clarity, flow, and engagement. Adapt tone to context."
  [teacher]="You are a patient teacher. Use analogies and examples. Check for understanding. Scaffold complex concepts."
  [sysadmin]="You are a Linux/DevOps expert. Give precise, tested commands. Prefer minimal dependencies."
  [security]="You are a cybersecurity expert. Think offensively to defend. Reference CVEs where relevant."
  [data]="You are a data scientist. Use pandas, numpy, matplotlib. Apply statistical rigor."
  [creative]="You are a bold, original creative. Push boundaries. Experiment with form and voice."
  [concise]="Be maximally concise. Use the fewest words without losing accuracy."
  [audio]="You are an audio engineering and music production expert. Help with DSP, audio files, mixing, analysis."
  [video]="You are a video production and ffmpeg expert. Help with video editing, transcoding, analysis."
)


# ════════════════════════════════════════════════════════════════════════════════
#  CUSTOM MODEL CONFIGS (prebuilt architecture presets)
# ════════════════════════════════════════════════════════════════════════════════
declare -A MODEL_PRESETS
MODEL_PRESETS=(
  [nano]="hidden_size=256|num_hidden_layers=8|num_attention_heads=8|intermediate_size=512|max_position_embeddings=512|vocab_size=32000|params=~0.125B"
  [micro]="hidden_size=512|num_hidden_layers=8|num_attention_heads=8|intermediate_size=1024|max_position_embeddings=1024|vocab_size=32000|params=~0.25B"
  [tiny]="hidden_size=512|num_hidden_layers=10|num_attention_heads=8|intermediate_size=1024|max_position_embeddings=2048|vocab_size=32000|params=~0.35B"
  [small]="hidden_size=768|num_hidden_layers=12|num_attention_heads=12|intermediate_size=2048|max_position_embeddings=2048|vocab_size=32000|params=~0.5B"
  [medium]="hidden_size=1024|num_hidden_layers=16|num_attention_heads=16|intermediate_size=4096|max_position_embeddings=2048|vocab_size=32000|params=~1B"
  [tinyllama]="hidden_size=2048|num_hidden_layers=22|num_attention_heads=32|intermediate_size=5632|max_position_embeddings=2048|vocab_size=32000|params=~1.1B"
)

# ── Model JSON configs ────────────────────────────────────────────────────────

# TTM: 179.35M — TinyLlama-based
TTM_CONFIG_JSON='{
  "architectures":["LlamaForCausalLM"],"bos_token_id":1,"eos_token_id":2,
  "hidden_act":"silu","hidden_size":1280,"initializer_range":0.02,
  "intermediate_size":3328,"max_position_embeddings":2048,"model_type":"llama",
  "num_attention_heads":16,"num_hidden_layers":14,"num_key_value_heads":4,
  "rms_norm_eps":1e-05,"tie_word_embeddings":false,"torch_dtype":"bfloat16",
  "use_cache":true,"vocab_size":32000,"_comment":"~179.35M params"
}'

# MTM: 0.61B — GTX 1080 optimized (fp16, 8GB VRAM — Pascal arch)
# Uses grouped-query attention (GQA) 4 kv heads, smaller intermediate for VRAM fit
MTM_CONFIG_JSON='{
  "architectures":["LlamaForCausalLM"],"bos_token_id":1,"eos_token_id":2,
  "hidden_act":"silu","hidden_size":2048,"initializer_range":0.02,
  "intermediate_size":5120,"max_position_embeddings":2048,"model_type":"llama",
  "num_attention_heads":16,"num_hidden_layers":18,"num_key_value_heads":4,
  "rms_norm_eps":1e-05,"tie_word_embeddings":false,"torch_dtype":"float16",
  "use_cache":true,"vocab_size":32000,
  "_comment":"~0.61B params — GTX 1080 (8GB fp16) optimized"
}'

# Mtm: 1.075B — RTX 2080+ optimized (bf16, 8GB+ VRAM — Turing/Ampere)
# More layers, wider intermediate, bf16 for Turing+ tensor cores
MMTM_CONFIG_JSON='{
  "architectures":["LlamaForCausalLM"],"bos_token_id":1,"eos_token_id":2,
  "hidden_act":"silu","hidden_size":2048,"initializer_range":0.02,
  "intermediate_size":5632,"max_position_embeddings":4096,"model_type":"llama",
  "num_attention_heads":16,"num_hidden_layers":28,"num_key_value_heads":4,
  "rms_norm_eps":1e-05,"tie_word_embeddings":false,"torch_dtype":"bfloat16",
  "use_cache":true,"vocab_size":32000,
  "_comment":"~1.075B params — RTX 2080+ (bf16) optimized"
}'

# ════════════════════════════════════════════════════════════════════════════════
#  6 PRETRAINING DATASETS (shared by TTM/MTM/Mtm)
#  +2 optional custom ones set by user
# ════════════════════════════════════════════════════════════════════════════════
PRETRAIN_DATASETS=(
  "roneneldan/TinyStories|text|5000|Tiny children's stories — fluent English"
  "sahil2801/CodeAlpaca-20k|instruction+output|3000|Code generation pairs"
  "Open-Orca/OpenOrca|system_prompt+question+response|2000|Instruction following"
  "bigcode/the-stack-smol|content|2000|Small code snippets across languages"
  "HuggingFaceFW/fineweb-edu|text|3000|High-quality educational web text"
  "wikimedia/wikipedia|text|3000|Wikipedia articles (en, 20231101)"
)
# User-settable custom datasets (HF id or 'hf:user/repo' or local path)
PRETRAIN_CUSTOM_1="${PRETRAIN_CUSTOM_1:-}"
PRETRAIN_CUSTOM_2="${PRETRAIN_CUSTOM_2:-}"

# RLHF settings
RLHF_AUTO="${RLHF_AUTO:-0}"
RLHF_JUDGE="${RLHF_JUDGE:-nix26}"           # nix26 | qwen3+luth | qwen3+llama32
RLHF_MANUAL_ENABLED="${RLHF_MANUAL_ENABLED:-1}"
RLHF_REWARD_THRESHOLD="${RLHF_REWARD_THRESHOLD:-0.6}"

# Right-click context menu
RCLICK_ENABLED="${RCLICK_ENABLED:-0}"
RCLICK_VL_MODEL="${RCLICK_VL_MODEL:-qwen3vl}"  # qwen3vl | lfm25vl | lfm25vl_gguf | custom
RCLICK_CUSTOM_MODEL="${RCLICK_CUSTOM_MODEL:-}"

# Auto-updater
AUP_REPO="${AUP_REPO:-minerofthesoal/ai-cli}"
AUP_CHECK_INTERVAL="${AUP_CHECK_INTERVAL:-3600}"
AUP_LAST_CHECK="${AUP_LAST_CHECK:-0}"

# Agent mode
AGENT_MODE="${AGENT_MODE:-0}"
AGENT_MAX_STEPS="${AGENT_MAX_STEPS:-10}"
AGENT_SEARCH_ENGINE="${AGENT_SEARCH_ENGINE:-ddg}"

# v2.4: LLM API server settings
API_HOST="${API_HOST:-127.0.0.1}"
API_PORT="${API_PORT:-8080}"
API_KEY="${API_KEY:-}"            # optional bearer token for API auth
API_CORS="${API_CORS:-1}"         # enable CORS for browser clients

# v2.4.5: API key hosting (shareable keys for others to access your model)
API_SHARE_ENABLED="${API_SHARE_ENABLED:-0}"
API_SHARE_HOST="${API_SHARE_HOST:-0.0.0.0}"
API_SHARE_PORT="${API_SHARE_PORT:-8080}"
API_SHARE_RATE_LIMIT="${API_SHARE_RATE_LIMIT:-60}"  # requests per minute per key

# v2.4.5: Multi-AI settings
MULTIAI_ROUNDS="${MULTIAI_ROUNDS:-6}"               # default conversation rounds
MULTIAI_SAVE_DATASET="${MULTIAI_SAVE_DATASET:-1}"   # save as training dataset
MULTIAI_RLHF_TRAIN="${MULTIAI_RLHF_TRAIN:-0}"       # auto-train on rated exchanges

# v2.4: CPU-only mode (auto-set on Windows or when no GPU found)
CPU_ONLY_MODE="${CPU_ONLY_MODE:-0}"
[[ $IS_WINDOWS -eq 1 ]] && CPU_ONLY_MODE=1
[[ "${CUDA_ARCH:-0}" == "0" ]] && CPU_ONLY_MODE=1

# ════════════════════════════════════════════════════════════════════════════════
#  GENERALIZED TRAINED MODEL ENGINE
#  Handles TTM (tiny/179M), MTM (mini/0.61B), Mtm (medium/1.075B)
# ════════════════════════════════════════════════════════════════════════════════

# _tm_vars MODEL_ID  →  sets TM_DIR TM_HF_REPO TM_CONFIG_JSON TM_LABEL
#                        TM_AUTO_TRAIN_VAR TM_VERSION_VAR TM_PRETRAINED_VAR
_tm_vars() {
  local id="$1"
  case "$id" in
    TTM|ttm)
      TM_DIR="$TTM_DIR"
      TM_HF_REPO="ray0rf1re/tiny"
      TM_CONFIG_JSON="$TTM_CONFIG_JSON"
      TM_LABEL="TTM (Tiny ~179M)"
      TM_AUTO_TRAIN_VAR="TTM_AUTO_TRAIN"
      TM_VERSION_VAR="TTM_VERSION"
      TM_PRETRAINED_VAR="TTM_PRETRAINED"
      TM_DTYPE="bfloat16"
      TM_GPU_OPT="any"
      ;;
    MTM|mtm)
      TM_DIR="$MTM_DIR"
      TM_HF_REPO="ray0rf1re/mini"
      TM_CONFIG_JSON="$MTM_CONFIG_JSON"
      TM_LABEL="MTM (Mini ~0.61B, GTX 1080)"
      TM_AUTO_TRAIN_VAR="MTM_AUTO_TRAIN"
      TM_VERSION_VAR="MTM_VERSION"
      TM_PRETRAINED_VAR="MTM_PRETRAINED"
      TM_DTYPE="float16"
      TM_GPU_OPT="GTX 1080 / Pascal+"
      ;;
    Mtm|mmtm|MMTM)
      TM_DIR="$MMTM_DIR"
      TM_HF_REPO="ray0rf1re/medium"
      TM_CONFIG_JSON="$MMTM_CONFIG_JSON"
      TM_LABEL="Mtm (Medium ~1.075B, RTX 2080+)"
      TM_AUTO_TRAIN_VAR="MMTM_AUTO_TRAIN"
      TM_VERSION_VAR="MMTM_VERSION"
      TM_PRETRAINED_VAR="MMTM_PRETRAINED"
      TM_DTYPE="bfloat16"
      TM_GPU_OPT="RTX 2080+ / Turing+"
      ;;
    *) err "Unknown model ID: $id (use TTM, MTM, or Mtm)"; return 1 ;;
  esac
}

_tm_get_var()  { eval "echo \"\${${1}:-0}\""; }
_tm_set_var()  { eval "${1}=\"${2}\""; }

_tm_init() {
  local id="$1"; _tm_vars "$id"
  mkdir -p "$TM_DIR"
  local cfg="$TM_DIR/config.json"
  if [[ ! -f "$cfg" ]]; then
    echo "$TM_CONFIG_JSON" > "$cfg"
    ok "$TM_LABEL config created: $cfg"
  fi
}

_tm_create_repo() {
  local id="$1"; _tm_vars "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  local hf_key="${HF_TOKEN:-}"; [[ -z "$hf_key" ]] && { err "HF_TOKEN not set"; return 1; }
  info "Creating HuggingFace repo: $TM_HF_REPO ..."
  HF_TOKEN_VAL="$hf_key" REPO_ID="$TM_HF_REPO" MODEL_LABEL="$TM_LABEL" \
  MODEL_CFG="$TM_CONFIG_JSON" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from huggingface_hub import HfApi
except ImportError:
    print("huggingface_hub not installed. Run: ai install-deps"); sys.exit(1)
api = HfApi(token=os.environ['HF_TOKEN_VAL'])
repo = os.environ['REPO_ID']
label = os.environ['MODEL_LABEL']
cfg   = os.environ['MODEL_CFG']
try:
    api.create_repo(repo_id=repo, exist_ok=True, private=False, repo_type='model')
    readme = f"# {label}\n\nAuto-trained model by AI CLI v2.3.\n\n```json\n{cfg}\n```\n"
    api.upload_file(
        path_or_fileobj=readme.encode(),
        path_in_repo="README.md",
        repo_id=repo,
        commit_message="init: create repo",
    )
    print(f"Created: https://huggingface.co/{repo}")
except Exception as e:
    print(f"Error: {e}", file=sys.stderr)
    sys.exit(1)
PYEOF
}

_tm_pretrain() {
  local id="$1"; shift
  local custom1="${1:-$PRETRAIN_CUSTOM_1}"; local custom2="${2:-$PRETRAIN_CUSTOM_2}"
  _tm_vars "$id"; _tm_init "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }

  info "$TM_LABEL — Starting pretraining"
  info "Using 6 standard datasets + ${custom1:+custom1: $custom1 }${custom2:+custom2: $custom2}"
  echo ""

  TM_DIR_VAL="$TM_DIR" TM_DTYPE_VAL="$TM_DTYPE" \
  CUSTOM1="${custom1:-}" CUSTOM2="${custom2:-}" \
  "$PYTHON" - <<'PYEOF'
import os, json, sys
TM_DIR   = os.environ['TM_DIR_VAL']
TM_DTYPE = os.environ.get('TM_DTYPE_VAL','float32')
CUSTOM1  = os.environ.get('CUSTOM1','')
CUSTOM2  = os.environ.get('CUSTOM2','')

try:
    import torch
    from transformers import (AutoTokenizer, LlamaConfig, LlamaForCausalLM,
                               TrainingArguments, Trainer, DataCollatorForLanguageModeling)
    from datasets import load_dataset, Dataset
except ImportError as e:
    print(f"Missing: {e}\nRun: ai install-deps"); sys.exit(1)

cfg_path = f"{TM_DIR}/config.json"
out_dir  = f"{TM_DIR}/pretrained"
os.makedirs(out_dir, exist_ok=True)

with open(cfg_path) as f:
    raw = json.load(f)
cfg = LlamaConfig(**{k:v for k,v in raw.items() if not k.startswith('_') and k!='architectures'})
model = LlamaForCausalLM(cfg)
total = sum(p.numel() for p in model.parameters())
print(f"Parameters: {total:,} ({total/1e6:.2f}M)")

tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
tokenizer.pad_token = tokenizer.eos_token
MAX_LEN = min(cfg.max_position_embeddings, 256)

records = []

# ── Dataset 1: TinyStories ────────────────────────────────────────────────────
print("Dataset 1/8: roneneldan/TinyStories")
try:
    ds = load_dataset("roneneldan/TinyStories", split="train[:6000]")
    for ex in ds: records.append(ex.get('text','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 2: CodeAlpaca ─────────────────────────────────────────────────────
print("Dataset 2/8: sahil2801/CodeAlpaca-20k")
try:
    ds = load_dataset("sahil2801/CodeAlpaca-20k", split="train[:4000]")
    for ex in ds:
        t = (ex.get('instruction','') + '\n' + ex.get('output',''))[:MAX_LEN*4]
        records.append(t)
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 3: OpenOrca ───────────────────────────────────────────────────────
print("Dataset 3/8: Open-Orca/OpenOrca")
try:
    ds = load_dataset("Open-Orca/OpenOrca", split="train[:3000]")
    for ex in ds:
        t = (ex.get('system_prompt','') + ' ' + ex.get('question','') + '\n' + ex.get('response',''))[:MAX_LEN*4]
        records.append(t)
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 4: The Stack Smol ─────────────────────────────────────────────────
print("Dataset 4/8: bigcode/the-stack-smol")
try:
    ds = load_dataset("bigcode/the-stack-smol", data_dir="data/python", split="train[:3000]")
    for ex in ds: records.append(ex.get('content','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 5: FineWeb-Edu ────────────────────────────────────────────────────
print("Dataset 5/8: HuggingFaceFW/fineweb-edu")
try:
    ds = load_dataset("HuggingFaceFW/fineweb-edu", name="sample-10BT", split="train[:4000]",
                      streaming=False)
    for ex in ds: records.append(ex.get('text','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Dataset 6: Wikipedia ──────────────────────────────────────────────────────
print("Dataset 6/8: wikimedia/wikipedia (en)")
try:
    ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train[:4000]")
    for ex in ds: records.append(ex.get('text','')[:MAX_LEN*4])
    print(f"  +{len(ds)} records (total {len(records)})")
except Exception as e: print(f"  skipped: {e}")

# ── Custom dataset 1 ──────────────────────────────────────────────────────────
if CUSTOM1:
    print(f"Dataset 7/8: {CUSTOM1} (custom)")
    try:
        if CUSTOM1.startswith('/') or CUSTOM1.endswith('.jsonl') or CUSTOM1.endswith('.txt'):
            with open(CUSTOM1) as f:
                for line in f:
                    line = line.strip()
                    if not line: continue
                    try:
                        obj = json.loads(line)
                        t = obj.get('text','') or obj.get('content','') or str(obj)
                    except:
                        t = line
                    records.append(t[:MAX_LEN*4])
        else:
            ds_id = CUSTOM1.replace('hf:','')
            ds = load_dataset(ds_id, split="train[:2000]")
            tcols = [c for c in ds.column_names if c in ['text','content','input','instruction']]
            col = tcols[0] if tcols else ds.column_names[0]
            for ex in ds: records.append(str(ex.get(col,''))[:MAX_LEN*4])
        print(f"  added custom1 (total {len(records)})")
    except Exception as e: print(f"  skipped: {e}")

# ── Custom dataset 2 ──────────────────────────────────────────────────────────
if CUSTOM2:
    print(f"Dataset 8/8: {CUSTOM2} (custom)")
    try:
        if CUSTOM2.startswith('/') or CUSTOM2.endswith('.jsonl') or CUSTOM2.endswith('.txt'):
            with open(CUSTOM2) as f:
                for line in f:
                    line = line.strip()
                    if not line: continue
                    try:
                        obj = json.loads(line)
                        t = obj.get('text','') or obj.get('content','') or str(obj)
                    except:
                        t = line
                    records.append(t[:MAX_LEN*4])
        else:
            ds_id = CUSTOM2.replace('hf:','')
            ds = load_dataset(ds_id, split="train[:2000]")
            tcols = [c for c in ds.column_names if c in ['text','content','input','instruction']]
            col = tcols[0] if tcols else ds.column_names[0]
            for ex in ds: records.append(str(ex.get(col,''))[:MAX_LEN*4])
        print(f"  added custom2 (total {len(records)})")
    except Exception as e: print(f"  skipped: {e}")

# ── Filter and tokenize ───────────────────────────────────────────────────────
records = [r for r in records if r and len(r.strip()) > 20]
print(f"\nTotal records: {len(records)}")
if not records:
    print("No data loaded"); sys.exit(1)

ds_all = Dataset.from_list([{'text': r} for r in records])
def tokenize(ex):
    return tokenizer(ex['text'], truncation=True, max_length=MAX_LEN, padding='max_length')
ds_all = ds_all.map(tokenize, batched=True, remove_columns=['text'])

device = 'cuda' if torch.cuda.is_available() else 'cpu'
dtype_map = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}
torch_dtype = dtype_map.get(TM_DTYPE, torch.float32)
model = model.to(device)
if device == 'cuda':
    model = model.to(torch_dtype)

print(f"Training on: {device} | dtype: {TM_DTYPE} | records: {len(ds_all)}")

args = TrainingArguments(
    output_dir=out_dir,
    num_train_epochs=1.075,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-4,
    lr_scheduler_type='cosine',
    warmup_ratio=0.05,
    fp16=(device=='cuda' and TM_DTYPE=='float16'),
    bf16=(device=='cuda' and TM_DTYPE=='bfloat16'),
    logging_steps=50,
    save_steps=500,
    save_total_limit=1,
    report_to='none',
    dataloader_num_workers=0,
)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds_all,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"\nPretrained model saved: {out_dir}")
PYEOF

  if [[ -d "$TM_DIR/pretrained" ]]; then
    _tm_set_var "$TM_PRETRAINED_VAR" "1"
    save_config
    ok "$TM_LABEL pretraining complete"
  fi
}

_tm_train_batch() {
  local id="$1"; _tm_vars "$id"
  local auto_var; auto_var=$(_tm_get_var "$TM_AUTO_TRAIN_VAR")
  [[ "$auto_var" != "1" ]] && return 0
  [[ -z "$PYTHON" ]] && return 0

  # Build batch data from chat logs + history
  local batch_file; batch_file=$(mktemp /tmp/tm_batch_XXXX.jsonl)
  find "$CHAT_LOGS_DIR" -name "*.jsonl" -newer "$TM_DIR/.last_train" 2>/dev/null | head -5 | while read -r f; do
    cat "$f"
  done > "$batch_file" 2>/dev/null || true
  if [[ -f "$LOG_FILE" ]]; then
    tail -20 "$LOG_FILE" | while IFS= read -r line; do
      local msg; msg=$(echo "$line" | sed 's/^[0-9T:+-]* \[[a-z]*\] //')
      [[ -n "$msg" ]] && echo "{\"text\":\"$msg\"}" >> "$batch_file"
    done
  fi

  local count; count=$(wc -l < "$batch_file" 2>/dev/null || echo 0)
  if (( count < 3 )); then rm -f "$batch_file"; return 0; fi

  local base_model="$TM_DIR/pretrained"
  local latest_ft; latest_ft=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "")
  [[ -n "$latest_ft" ]] && base_model="$latest_ft"
  [[ ! -d "$base_model" ]] && { rm -f "$batch_file"; return 0; }

  local cur_ver; cur_ver=$(_tm_get_var "$TM_VERSION_VAR")
  local new_ver=$(( cur_ver + 1 ))
  local out_dir="$TM_DIR/ft_v${new_ver}"
  mkdir -p "$out_dir"

  BATCH_FILE="$batch_file" BASE_MODEL="$base_model" OUT_DIR="$out_dir" \
  TM_DTYPE_VAL="$TM_DTYPE" "$PYTHON" - <<'PYEOF' &>/dev/null &
import os, json, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, \
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, TaskType
except ImportError: sys.exit(0)

batch_file = os.environ['BATCH_FILE']
base_model = os.environ['BASE_MODEL']
out_dir    = os.environ['OUT_DIR']
TM_DTYPE   = os.environ.get('TM_DTYPE_VAL','float32')

records = []
with open(batch_file) as f:
    for line in f:
        line = line.strip()
        if not line: continue
        try:
            obj = json.loads(line)
            txt = obj.get('text','') or obj.get('output','') + ' ' + obj.get('instruction','')
            if txt.strip(): records.append({'text': txt[:256]})
        except: pass
if len(records) < 3: sys.exit(0)

tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(base_model)
lora = LoraConfig(task_type=TaskType.CAUSAL_LM, r=4, lora_alpha=8,
                  lora_dropout=0.05, target_modules=["q_proj","v_proj"])
model = get_peft_model(model, lora)
ds = Dataset.from_list(records)
def tok(ex): return tokenizer(ex['text'], truncation=True, max_length=128, padding='max_length')
ds = ds.map(tok, batched=True, remove_columns=['text'])
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
dtype_map = {'float16': torch.float16, 'bfloat16': torch.bfloat16}
if device == 'cuda' and TM_DTYPE in dtype_map:
    model = model.to(dtype_map[TM_DTYPE])
args = TrainingArguments(
    output_dir=out_dir, num_train_epochs=1, per_device_train_batch_size=1,
    gradient_accumulation_steps=1, max_steps=1, learning_rate=2e-4,
    fp16=(device=='cuda' and TM_DTYPE=='float16'),
    bf16=(device=='cuda' and TM_DTYPE=='bfloat16'),
    logging_steps=1, save_steps=1, report_to='none',
)
Trainer(model=model, args=args, train_dataset=ds,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)).train()
merged = model.merge_and_unload()
merged.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
PYEOF

  rm -f "$batch_file"
  touch "$TM_DIR/.last_train"
  _tm_set_var "$TM_VERSION_VAR" "$new_ver"
  save_config
}

_tm_upload() {
  local id="$1"; local version="${2:-latest}"; _tm_vars "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  local hf_key="${HF_TOKEN:-}"; [[ -z "$hf_key" ]] && { err "HF_TOKEN not set"; return 1; }

  local model_dir
  if [[ "$version" == "latest" ]]; then
    model_dir=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "$TM_DIR/pretrained")
  else
    model_dir="$TM_DIR/ft_v${version}"
  fi
  [[ ! -d "$model_dir" ]] && { err "No $id model at $model_dir. Run: ai $id pretrain"; return 1; }

  local folder_name; folder_name=$(basename "$model_dir")
  info "Uploading $id $folder_name → $TM_HF_REPO/$folder_name"

  HF_TOKEN_VAL="$hf_key" MODEL_DIR="$model_dir" FOLDER_NAME="$folder_name" \
  TM_HF_REPO="$TM_HF_REPO" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from huggingface_hub import HfApi
except ImportError:
    print("huggingface_hub not installed. Run: ai install-deps"); sys.exit(1)
api   = HfApi(token=os.environ['HF_TOKEN_VAL'])
repo  = os.environ['TM_HF_REPO']
mdir  = os.environ['MODEL_DIR']
fname = os.environ['FOLDER_NAME']
try:
    api.create_repo(repo_id=repo, exist_ok=True, private=False)
except: pass
api.upload_folder(
    folder_path=mdir, repo_id=repo, path_in_repo=fname,
    commit_message=f"auto-upload: {fname}",
)
print(f"Uploaded: https://huggingface.co/{repo}/tree/main/{fname}")
PYEOF
}

_tm_load() {
  local id="$1"; local version="${2:-latest}"; _tm_vars "$id"
  local mdir
  if [[ "$version" == "latest" ]]; then
    mdir=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "$TM_DIR/pretrained")
  else
    mdir="$TM_DIR/ft_v${version}"
  fi
  [[ ! -d "$mdir" ]] && { err "$id not trained yet. Run: ai $id pretrain"; return 1; }
  ACTIVE_MODEL="$mdir"; ACTIVE_BACKEND="pytorch"; save_config
  ok "Loaded $TM_LABEL from $mdir"
}

_tm_status() {
  local id="$1"; _tm_vars "$id"
  local auto_var;    auto_var=$(_tm_get_var "$TM_AUTO_TRAIN_VAR")
  local pretrain_var; pretrain_var=$(_tm_get_var "$TM_PRETRAINED_VAR")
  local ver_var;     ver_var=$(_tm_get_var "$TM_VERSION_VAR")

  hdr "$TM_LABEL Status"
  echo "  HF Repo:    $TM_HF_REPO"
  echo "  GPU target: $TM_GPU_OPT"
  echo "  Data type:  $TM_DTYPE"
  echo "  Config:     $TM_DIR/config.json"
  echo "  Auto-train: $auto_var"
  echo "  Pretrained: $pretrain_var"
  echo "  Version:    $ver_var"
  echo ""
  [[ -d "$TM_DIR/pretrained" ]] && ok "Pretrained model present" || warn "Not pretrained yet"
  local latest; latest=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "")
  [[ -n "$latest" ]] && ok "Latest finetune: $(basename "$latest")"
  [[ -n "$PRETRAIN_CUSTOM_1" ]] && echo "  Custom DS 1: $PRETRAIN_CUSTOM_1"
  [[ -n "$PRETRAIN_CUSTOM_2" ]] && echo "  Custom DS 2: $PRETRAIN_CUSTOM_2"
}

# ── Generic cmd dispatcher for TTM/MTM/Mtm ────────────────────────────────────
_tm_cmd() {
  local id="$1"; shift
  local sub="${1:-help}"; shift || true
  _tm_init "$id"
  case "$sub" in
    pretrain)
      local c1="${1:-$PRETRAIN_CUSTOM_1}"; local c2="${2:-$PRETRAIN_CUSTOM_2}"
      _tm_pretrain "$id" "$c1" "$c2"
      ;;
    status)    _tm_status "$id" ;;
    load)      _tm_load "$id" "${1:-latest}" ;;
    train-now) _tm_train_batch "$id" ;;
    upload)    _tm_upload "$id" "${1:-latest}" ;;
    create-repo) _tm_create_repo "$id" ;;
    enable)
      _tm_vars "$id"
      _tm_set_var "$TM_AUTO_TRAIN_VAR" "1"; save_config
      ok "$TM_LABEL auto-training enabled"
      ;;
    disable)
      _tm_vars "$id"
      _tm_set_var "$TM_AUTO_TRAIN_VAR" "0"; save_config
      ok "$TM_LABEL auto-training disabled"
      ;;
    set-custom1)
      PRETRAIN_CUSTOM_1="${1:-}"; save_config
      ok "Custom dataset 1: $PRETRAIN_CUSTOM_1"
      ;;
    set-custom2)
      PRETRAIN_CUSTOM_2="${1:-}"; save_config
      ok "Custom dataset 2: $PRETRAIN_CUSTOM_2"
      ;;
    finetune|fine-tune|ft)
      local dataset="${1:-}"; local epochs="${2:-3}"; local lr="${3:-2e-4}"
      _tm_finetune "$id" "$dataset" "$epochs" "$lr"
      ;;
    *)
      _tm_vars "$id"
      echo -e "${B}${BCYAN}$TM_LABEL${R}"
      echo "  GPU target: $TM_GPU_OPT | Dtype: $TM_DTYPE | Repo: $TM_HF_REPO"
      echo ""
      echo "  ${B}ai $id pretrain [custom1] [custom2]${R} — Pretrain (6 standard + 2 optional)"
      echo "  ${B}ai $id finetune <dataset> [epochs] [lr]${R} — Fine-tune on custom dataset (v2.4)"
      echo "  ${B}ai $id enable / disable${R}              — Toggle auto-training"
      echo "  ${B}ai $id train-now${R}                     — Force one batch"
      echo "  ${B}ai $id upload [version]${R}              — Upload to $TM_HF_REPO"
      echo "  ${B}ai $id create-repo${R}                   — Create HF repo"
      echo "  ${B}ai $id status${R}                        — Show status"
      echo "  ${B}ai $id load [version]${R}                — Set as active model"
      echo "  ${B}ai $id set-custom1 <hf-id-or-path>${R}   — Set custom dataset 1"
      echo "  ${B}ai $id set-custom2 <hf-id-or-path>${R}   — Set custom dataset 2"
      echo ""
      echo "  ${B}ai -TTM${R} / ${B}ai -MTM${R} / ${B}ai -Mtm${R}            — Load respective model"
      ;;
  esac
}

cmd_ttm() { _tm_cmd "TTM" "$@"; }
cmd_mtm() { _tm_cmd "MTM" "$@"; }
cmd_Mtm() { _tm_cmd "Mtm" "$@"; }

# ════════════════════════════════════════════════════════════════════════════════
#  TTM / MTM / Mtm FINE-TUNING  (v2.4)
#  Fine-tune any trained model on a custom dataset using LoRA/QLoRA
#  ai ttm finetune <dataset-name-or-path> [epochs=3] [lr=2e-4]
#  ai mtm finetune <dataset-name-or-path> [epochs=3] [lr=2e-4]
#  ai Mtm finetune <dataset-name-or-path> [epochs=3] [lr=2e-4]
# ════════════════════════════════════════════════════════════════════════════════
_tm_finetune() {
  local id="$1"; local dataset="${2:-}"; local epochs="${3:-3}"; local lr="${4:-2e-4}"
  _tm_vars "$id"; _tm_init "$id"
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }

  # Resolve dataset path
  local ds_path=""
  if [[ -z "$dataset" ]]; then
    err "Usage: ai $id finetune <dataset-name-or-path> [epochs] [lr]"
    echo "  Available datasets: $(ls "$DATASETS_DIR" 2>/dev/null | tr '\n' ' ')"
    return 1
  fi
  if [[ -f "$DATASETS_DIR/$dataset/data.jsonl" ]]; then
    ds_path="$DATASETS_DIR/$dataset/data.jsonl"
  elif [[ -f "$dataset" ]]; then
    ds_path="$dataset"
  else
    err "Dataset '$dataset' not found"
    echo "  Create one: ai dataset create $dataset"
    echo "  Or provide a path to a JSONL file"
    return 1
  fi

  local n_pairs; n_pairs=$(wc -l < "$ds_path" 2>/dev/null || echo 0)
  if (( n_pairs < 10 )); then
    err "Dataset too small: $n_pairs pairs (need at least 10)"
    echo "  Add more pairs: ai dataset add $dataset \"<prompt>\" \"<response>\""
    return 1
  fi

  local base_model_dir="$TM_DIR/pretrained"
  if [[ ! -d "$base_model_dir" ]]; then
    warn "No pretrained model found at $base_model_dir"
    warn "Run 'ai $id pretrain' first, or fine-tuning from config..."
    base_model_dir="$TM_DIR"
  fi

  local ft_out="$TM_DIR/finetuned_$(date +%Y%m%d_%H%M%S)"
  mkdir -p "$ft_out"

  hdr "$TM_LABEL Fine-tuning"
  echo "  Dataset:  $ds_path ($n_pairs pairs)"
  echo "  Base:     $base_model_dir"
  echo "  Output:   $ft_out"
  echo "  Epochs:   $epochs | LR: $lr | Dtype: $TM_DTYPE"
  echo ""

  TM_DIR_VAL="$TM_DIR" TM_DTYPE_VAL="$TM_DTYPE" TM_LABEL_VAL="$TM_LABEL" \
  DS_PATH="$ds_path" EPOCHS="$epochs" LR="$lr" FT_OUT="$ft_out" \
  BASE_MODEL="$base_model_dir" \
  "$PYTHON" - <<'PYEOF'
import os, sys, json

TM_DIR   = os.environ['TM_DIR_VAL']
TM_DTYPE = os.environ.get('TM_DTYPE_VAL', 'float32')
TM_LABEL = os.environ.get('TM_LABEL_VAL', 'Model')
DS_PATH  = os.environ['DS_PATH']
EPOCHS   = int(os.environ.get('EPOCHS', '3'))
LR       = float(os.environ.get('LR', '2e-4'))
FT_OUT   = os.environ['FT_OUT']
BASE     = os.environ['BASE_MODEL']

try:
    import torch
    from transformers import (AutoTokenizer, AutoModelForCausalLM,
                               TrainingArguments, LlamaForCausalLM, LlamaConfig)
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
except ImportError as e:
    print(f"Missing: {e}\nRun: ai install-deps"); sys.exit(1)

# CPU-only mode: use float32 and minimal batch
device = "cpu"
dtype = torch.float32
if torch.cuda.is_available():
    device = "cuda"
    dtype = torch.float16 if TM_DTYPE == 'float16' else torch.bfloat16
    if TM_DTYPE == 'bfloat16' and not torch.cuda.is_bf16_supported():
        dtype = torch.float16
        print("  Note: BF16 not supported, falling back to FP16")

print(f"  Device: {device} | Dtype: {dtype}")

# Load tokenizer — try from base dir, fallback to TinyLlama tokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained(BASE)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
tokenizer.pad_token = tokenizer.eos_token

# Load model — try from pretrained dir, else from config
try:
    if TM_DTYPE == 'bfloat16' and device == 'cuda':
        model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=dtype).to(device)
    else:
        model = AutoModelForCausalLM.from_pretrained(BASE, torch_dtype=torch.float32).to(device)
    print(f"  Loaded pretrained model from {BASE}")
except Exception as e:
    print(f"  Loading from config (no pretrained weights): {e}")
    cfg_path = f"{TM_DIR}/config.json"
    with open(cfg_path) as f:
        raw = json.load(f)
    cfg = LlamaConfig(**{k: v for k, v in raw.items() if not k.startswith('_') and k != 'architectures'})
    model = LlamaForCausalLM(cfg).to(device)

total_params = sum(p.numel() for p in model.parameters())
print(f"  Parameters: {total_params:,} ({total_params/1e6:.2f}M)")

# Apply LoRA for efficient fine-tuning (CPU-friendly: small rank)
cpu_mode = (device == "cpu")
lora_r = 4 if cpu_mode else 8
lora_alpha = 8 if cpu_mode else 16

lora_cfg = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=lora_r,
    lora_alpha=lora_alpha,
    lora_dropout=0.05,
    bias="none",
    target_modules=["q_proj", "v_proj"]
)
model = get_peft_model(model, lora_cfg)
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  LoRA rank={lora_r} | Trainable params: {trainable:,} ({trainable/total_params*100:.2f}%)")

# Load dataset
records = []
with open(DS_PATH) as f:
    for line in f:
        try:
            r = json.loads(line)
            prompt = r.get('prompt', r.get('instruction', ''))
            response = r.get('response', r.get('output', r.get('answer', '')))
            if prompt and response:
                records.append({"text": f"User: {prompt}\nAssistant: {response}{tokenizer.eos_token}"})
        except: pass
print(f"  Dataset: {len(records)} training pairs")

if not records:
    print("ERROR: No valid pairs found in dataset"); sys.exit(1)

MAX_LEN = 256 if cpu_mode else 512

def tokenize(batch):
    out = tokenizer(batch['text'], truncation=True, padding='max_length',
                    max_length=MAX_LEN, return_tensors=None)
    out['labels'] = out['input_ids'].copy()
    return out

ds = Dataset.from_list(records).map(tokenize, batched=True, remove_columns=['text'])

# Training arguments — CPU-optimized when no GPU
batch_size = 1 if cpu_mode else 4
grad_accum = 8 if cpu_mode else 4

args = TrainingArguments(
    output_dir=FT_OUT,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=grad_accum,
    learning_rate=LR,
    warmup_ratio=0.05,
    lr_scheduler_type="cosine",
    logging_steps=10,
    save_strategy="epoch",
    fp16=(dtype == torch.float16 and device == 'cuda'),
    bf16=(dtype == torch.bfloat16 and device == 'cuda'),
    dataloader_num_workers=0,
    no_cuda=(device == 'cpu'),
    report_to="none",
    save_total_limit=2,
    load_best_model_at_end=False,
    optim="adamw_torch",
)

from transformers import Trainer, DataCollatorForLanguageModeling
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
trainer = Trainer(model=model, args=args, train_dataset=ds, data_collator=collator)

print(f"\n  Starting fine-tuning ({EPOCHS} epochs)...")
trainer.train()

# Save merged model
try:
    merged = model.merge_and_unload()
    merged.save_pretrained(FT_OUT)
    print(f"  Merged model saved: {FT_OUT}")
except Exception as e:
    # Save LoRA adapter only
    model.save_pretrained(FT_OUT)
    print(f"  LoRA adapter saved: {FT_OUT}")

tokenizer.save_pretrained(FT_OUT)
print(f"\n  Fine-tuning complete → {FT_OUT}")
print(f"  Load: ai {TM_LABEL.split()[0].lower()} load")
PYEOF

  if [[ $? -eq 0 ]]; then
    ok "Fine-tuning complete: $ft_out"
    echo "  Load model: ai $(echo "$id" | tr '[:upper:]' '[:lower:]') load"
    echo "  Upload:     ai $(echo "$id" | tr '[:upper:]' '[:lower:]') upload"
  else
    err "Fine-tuning failed. Check logs above."
    return 1
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
#  RLHF — Reinforcement Learning from Human Feedback
#  Auto-RLHF: judge model rates responses → DPO training
#  Manual RLHF: thumbs up/down + star ratings → stored preferences
# ════════════════════════════════════════════════════════════════════════════════

# Judge model configs
declare -A RLHF_JUDGES
RLHF_JUDGES=(
  [nix26]="mradermacher/Nix2.6-GGUF|Nix2.6-Q4_K_M.gguf|Single judge — Nix 2.6 (general alignment)"
  [qwen3+luth]="Qwen/Qwen3-1.7B-GGUF|qwen3-1.7b-q4_k_m.gguf+kurakurai/Luth-LFM2-350M-GGUF|Luth-LFM2-350M.Q4_K_M.gguf|Dual judge — Qwen3-1.7B + Luth 350M (fast+quality)"
  [qwen3+llama32]="Qwen/Qwen3-1.7B-GGUF|qwen3-1.7b-q4_k_m.gguf+bartowski/Llama-3.2-3B-Instruct-GGUF|Llama-3.2-3B-Instruct-Q4_K_M.gguf|Dual judge — Qwen3-1.7B + Llama 3.2-3B (balanced)"
)

RLHF_PREF_FILE="$CONFIG_DIR/rlhf_preferences.jsonl"
RLHF_PAIRS_FILE="$CONFIG_DIR/rlhf_pairs.jsonl"
RLHF_RATINGS_FILE="$CONFIG_DIR/rlhf_ratings.jsonl"
# Ensure RLHF data files exist
touch "$RLHF_PAIRS_FILE" "$RLHF_RATINGS_FILE" 2>/dev/null || true

# Active HF RLHF dataset for training (set via 'ai rlhf use-dataset')
RLHF_ACTIVE_HF_DATASET="${RLHF_ACTIVE_HF_DATASET:-}"

# ── Download judge models ─────────────────────────────────────────────────────
_rlhf_download_judge() {
  local judge="${RLHF_JUDGE:-nix26}"
  local entry="${RLHF_JUDGES[$judge]:-${RLHF_JUDGES[nix26]}}"
  local judge_dir="$MODELS_DIR/rlhf_judges"
  mkdir -p "$judge_dir"

  info "Downloading RLHF judge(s): $judge"

  # Parse single or dual judge
  IFS='+' read -ra parts <<< "$entry"
  local i=0
  for part in "${parts[@]}"; do
    IFS='|' read -r repo filename desc <<< "$part"
    [[ -z "$repo" ]] && continue
    local dest="$judge_dir/${filename}"
    if [[ ! -f "$dest" ]]; then
      info "  Downloading $filename from $repo..."
      curl -L --progress-bar \
        --retry 5 --retry-delay 2 \
        ${HF_TOKEN:+-H "Authorization: Bearer $HF_TOKEN"} \
        "https://huggingface.co/${repo}/resolve/main/${filename}" \
        -o "$dest" 2>/dev/null || \
      curl -L --progress-bar \
        --retry 5 --retry-delay 2 \
        ${HF_TOKEN:+-H "Authorization: Bearer $HF_TOKEN"} \
        "https://huggingface.co/${repo}/resolve/main/$(echo "$filename" | tr '[:upper:]' '[:lower:]')" \
        -o "$dest" 2>/dev/null || { warn "  Could not download $filename"; continue; }
      ok "  Downloaded: $dest"
    else
      ok "  Already present: $filename"
    fi
    (( i++ ))
  done
}

# ── Score a response using judge model(s) ────────────────────────────────────
_rlhf_score_response() {
  local prompt="$1" response="$2" judge="${RLHF_JUDGE:-nix26}"
  local entry="${RLHF_JUDGES[$judge]:-${RLHF_JUDGES[nix26]}}"
  local judge_dir="$MODELS_DIR/rlhf_judges"
  [[ -z "$LLAMA_BIN" && -z "$PYTHON" ]] && { echo "0.5"; return; }

  # Build judge prompt
  local judge_prompt="[INST] Rate this AI response on a scale of 0.0-1.0.
Consider: factual accuracy, helpfulness, coherence, no hallucinations.
Respond with ONLY a decimal number between 0.0 and 1.0.

User prompt: ${prompt:0:200}
AI response: ${response:0:400}
[/INST] Score:"

  local score="0.5"

  # Try with llama.cpp
  if [[ -n "$LLAMA_BIN" && "$LLAMA_BIN" != "llama_cpp_python" ]]; then
    IFS='+' read -ra parts <<< "$entry"
    local scores=()
    for part in "${parts[@]}"; do
      IFS='|' read -r repo filename _ <<< "$part"
      local model="$judge_dir/$filename"
      [[ ! -f "$model" ]] && continue
      local s
      s=$("$LLAMA_BIN" -m "$model" -p "$judge_prompt" \
          -n 8 --temp 0 --top-k 1 --repeat-penalty 1.0 \
          --no-display-prompt 2>/dev/null | \
          grep -oP '[0-9]\.[0-9]+' | head -1)
      [[ -n "$s" ]] && scores+=("$s")
    done
    if [[ ${#scores[@]} -gt 0 ]]; then
      # Average scores from dual judges
      score=$(python3 -c "scores=[${scores[*]}]; print(round(sum(scores)/len(scores),3))" 2>/dev/null || echo "0.5")
    fi
  elif [[ -n "$PYTHON" ]]; then
    # Use transformers for scoring
    IFS='+' read -ra parts <<< "$entry"
    local part="${parts[0]}"
    IFS='|' read -r repo filename _ <<< "$part"
    local model="$judge_dir/$filename"
    [[ -f "$model" ]] && score=$(JUDGE_MODEL="$model" JUDGE_PROMPT="$judge_prompt" \
      "$PYTHON" - <<'PYEOF' 2>/dev/null
import os,sys
try:
    from llama_cpp import Llama
    llm=Llama(model_path=os.environ['JUDGE_MODEL'],n_ctx=512,verbose=False)
    out=llm(os.environ['JUDGE_PROMPT'],max_tokens=8,temperature=0,stop=['\n'])
    txt=out['choices'][0]['text'].strip()
    import re; m=re.search(r'[0-9]\.[0-9]+',txt)
    print(m.group(0) if m else '0.5')
except: print('0.5')
PYEOF
    )
  fi
  echo "$score"
}

# ── Auto-RLHF: collect (prompt, response, score) pairs → DPO training ────────
_rlhf_auto_collect() {
  local prompt="$1" response="$2"
  [[ "$RLHF_AUTO" != "1" ]] && return
  [[ -z "$prompt" || -z "$response" ]] && return

  local score
  score=$(_rlhf_score_response "$prompt" "$response")
  local ts; ts=$(date -Iseconds)

  # Store pair
  printf '{"ts":"%s","prompt":%s,"response":%s,"score":%s,"judge":"%s"}\n' \
    "$ts" \
    "$(python3 -c "import json,sys; print(json.dumps(sys.argv[1]))" "$prompt" 2>/dev/null || echo "\"$prompt\"")" \
    "$(python3 -c "import json,sys; print(json.dumps(sys.argv[1]))" "$response" 2>/dev/null || echo "\"$response\"")" \
    "$score" "$RLHF_JUDGE" \
    >> "$RLHF_PAIRS_FILE"

  # Trigger DPO if enough pairs accumulated and score is low
  local count; count=$(wc -l < "$RLHF_PAIRS_FILE" 2>/dev/null || echo 0)
  if (( count > 0 && count % 20 == 0 )); then
    _rlhf_dpo_train &
  fi
}

# ── DPO training on collected pairs ──────────────────────────────────────────
_rlhf_dpo_train() {
  local model_dir="${1:-$ACTIVE_MODEL}"
  # Resolve TTM/MTM/Mtm shortcuts
  case "$model_dir" in
    TTM|ttm) model_dir="$TTM_DIR" ;;
    MTM|mtm) model_dir="$MTM_DIR" ;;
    Mtm|mmtm|MMTM) model_dir="$MMTM_DIR" ;;
  esac
  if [[ -z "$model_dir" || ! -d "$model_dir" ]]; then
    warn "RLHF: model directory not found: ${model_dir:-<not set>}"
    warn "  Run 'ai ttm pretrain' first, or specify model: ai rlhf train TTM"
    return 1
  fi
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }

  # Merge any active HF dataset pairs in first
  local pairs_source="$RLHF_PAIRS_FILE"
  if [[ -n "$RLHF_ACTIVE_HF_DATASET" && -f "$RLHF_ACTIVE_HF_DATASET" ]]; then
    local merged="/tmp/ai_rlhf_merged_$$.jsonl"
    cat "$RLHF_PAIRS_FILE" "$RLHF_ACTIVE_HF_DATASET" > "$merged" 2>/dev/null || true
    pairs_source="$merged"
  fi

  [[ ! -f "$pairs_source" ]] && { warn "No RLHF pairs collected yet."; return 1; }
  local count; count=$(wc -l < "$pairs_source" 2>/dev/null || echo 0)
  if (( count < 10 )); then
    warn "RLHF: Need at least 10 pairs (have $count). Rate more responses or add HF dataset."
    return 1
  fi

  info "RLHF: Running DPO training on $count pairs → $model_dir ..."
  PAIRS_FILE="$pairs_source" MODEL_DIR="$model_dir" \
  THRESHOLD="${RLHF_REWARD_THRESHOLD:-0.6}" CPU_ONLY="${CPU_ONLY_MODE:-0}" \
  "$PYTHON" - <<'PYEOF' &
import os, json, sys, random, pathlib, shutil
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, TaskType
except ImportError as e:
    print(f"RLHF: Missing dependency: {e}")
    print("  Install: pip install trl peft transformers datasets torch")
    sys.exit(1)

pairs_file = os.environ['PAIRS_FILE']
model_dir  = os.environ['MODEL_DIR']
threshold  = float(os.environ.get('THRESHOLD', '0.6'))
cpu_only   = os.environ.get('CPU_ONLY', '0') == '1'
out_dir    = model_dir + "_dpo"

# ── Load & deduplicate pairs ─────────────────────────────────────────────────
pairs = []; seen = set()
with open(pairs_file) as f:
    for line in f:
        line = line.strip()
        if not line: continue
        try:
            p = json.loads(line)
            # Normalise score field — could be 'score', 'rating', 'reward'
            if 'rating' in p and 'score' not in p:
                p['score'] = float(p['rating']) / 5.0  # 1-5 stars → 0-1
            key = str(p.get('prompt', ''))[:120]
            if key in seen: continue
            seen.add(key); pairs.append(p)
        except: pass

if not pairs:
    print("RLHF: No valid pairs found in pairs file"); sys.exit(1)

# ── Build DPO format ─────────────────────────────────────────────────────────
dpo_data = []
if all('chosen' in p and 'rejected' in p for p in pairs):
    dpo_data = [{'prompt': p.get('prompt', ''),
                 'chosen': str(p['chosen']),
                 'rejected': str(p['rejected'])}
                for p in pairs if p.get('prompt') and p.get('chosen') and p.get('rejected')]
else:
    chosen   = [p for p in pairs if float(p.get('score', 0)) >= threshold]
    rejected = [p for p in pairs if float(p.get('score', 1)) <  threshold]
    if len(chosen) < 2 or len(rejected) < 2:
        print(f"RLHF: Not enough contrast pairs (chosen={len(chosen)}, rejected={len(rejected)})")
        print(f"  threshold={threshold}. Try: ai rlhf threshold 0.4")
        print("  Or import pairs: ai rlhf add-dataset hh-rlhf")
        sys.exit(1)
    random.shuffle(chosen); random.shuffle(rejected)
    for c, r in zip(chosen, rejected):
        dpo_data.append({
            'prompt':   str(c.get('prompt', '')),
            'chosen':   str(c.get('response', c.get('chosen', ''))),
            'rejected': str(r.get('response', r.get('rejected', ''))),
        })

dpo_data = [d for d in dpo_data if d['prompt'] and d['chosen'] and d['rejected']]
if not dpo_data:
    print("RLHF: No usable DPO pairs after filtering"); sys.exit(1)

print(f"RLHF: {len(dpo_data)} DPO pairs — training on {'CPU' if cpu_only else 'GPU/CPU'}...")

# ── Model setup ──────────────────────────────────────────────────────────────
try:
    from trl import DPOTrainer, DPOConfig
    import inspect

    device   = 'cpu' if cpu_only else ('cuda' if torch.cuda.is_available() else 'cpu')
    use_cuda = device == 'cuda'
    dtype    = torch.float32 if (cpu_only or not use_cuda) else \
               (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)

    load_kw = {'torch_dtype': dtype, 'low_cpu_mem_usage': True}
    if use_cuda:
        load_kw['device_map'] = 'auto'

    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model     = AutoModelForCausalLM.from_pretrained(model_dir, **load_kw)
    ref_model = AutoModelForCausalLM.from_pretrained(model_dir, **load_kw)

    lora = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16,
                      lora_dropout=0.05,
                      target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                      bias="none")
    model = get_peft_model(model, lora)
    model.print_trainable_parameters()

    if not use_cuda:
        model = model.to(device)
        ref_model = ref_model.to(device)

    ds = Dataset.from_list(dpo_data)

    # ── DPOConfig — compatible with trl 0.7 through 0.12+ ────────────────────
    dpo_cfg_params = inspect.signature(DPOConfig.__init__).parameters
    cfg_kwargs = dict(
        output_dir=out_dir,
        num_train_epochs=1,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        max_steps=min(len(dpo_data), 200),
        learning_rate=5e-6,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        logging_steps=10,
        save_steps=500,
        report_to='none',
        remove_unused_columns=False,
        bf16=(use_cuda and dtype == torch.bfloat16),
        fp16=(use_cuda and dtype == torch.float16),
    )
    # max_length / max_prompt_length removed in trl ≥ 0.9
    if 'max_length' in dpo_cfg_params:
        cfg_kwargs['max_length'] = 512
    if 'max_prompt_length' in dpo_cfg_params:
        cfg_kwargs['max_prompt_length'] = 256

    cfg = DPOConfig(**cfg_kwargs)

    # ── DPOTrainer — 'tokenizer' renamed to 'processing_class' in trl ≥ 0.12 ─
    trainer_params = inspect.signature(DPOTrainer.__init__).parameters
    trainer_kwargs = dict(model=model, ref_model=ref_model, args=cfg, train_dataset=ds)
    if 'processing_class' in trainer_params:
        trainer_kwargs['processing_class'] = tokenizer
    else:
        trainer_kwargs['tokenizer'] = tokenizer

    trainer = DPOTrainer(**trainer_kwargs)
    trainer.train()

    # Merge LoRA weights back into base model and save
    merged = model.merge_and_unload()
    merged.save_pretrained(out_dir)
    tokenizer.save_pretrained(out_dir)

    # Copy config.json if missing from output
    src_cfg = pathlib.Path(model_dir) / 'config.json'
    dst_cfg = pathlib.Path(out_dir) / 'config.json'
    if src_cfg.exists() and not dst_cfg.exists():
        shutil.copy(src_cfg, dst_cfg)

    print(f"RLHF DPO complete → {out_dir}")
    print(f"  Load with: ai model {out_dir}")

except ImportError as ie:
    print(f"RLHF: Missing dependency: {ie}")
    print("  Install: pip install 'trl>=0.7' peft transformers datasets")
    sys.exit(1)
except Exception as e:
    import traceback
    print(f"RLHF DPO error: {e}")
    traceback.print_exc()
    sys.exit(1)
PYEOF
  # Clean up temp merged file if we created one
  [[ -f "/tmp/ai_rlhf_merged_$$.jsonl" ]] && rm -f "/tmp/ai_rlhf_merged_$$.jsonl" 2>/dev/null || true
}

# ── Mandatory realignment (anti-hallucination) using Qwen3 ───────────────────
_tm_align() {
  local id="$1"; _tm_vars "$id"
  local base_model="$TM_DIR/pretrained"
  local latest_ft; latest_ft=$(ls -td "$TM_DIR"/ft_v*/ 2>/dev/null | head -1 || echo "")
  [[ -n "$latest_ft" ]] && base_model="$latest_ft"
  [[ ! -d "$base_model" ]] && { warn "No model to align"; return 1; }
  [[ -z "$PYTHON" ]] && { warn "Python not found for alignment"; return 1; }

  # Download Qwen3-1.7B if not present
  local qwen_dir="$MODELS_DIR/rlhf_judges"
  local qwen_gguf="$qwen_dir/qwen3-1.7b-q4_k_m.gguf"
  mkdir -p "$qwen_dir"
  if [[ ! -f "$qwen_gguf" ]]; then
    info "Downloading Qwen3-1.7B for alignment..."
    curl -L --retry 5 --progress-bar \
      ${HF_TOKEN:+-H "Authorization: Bearer $HF_TOKEN"} \
      "https://huggingface.co/Qwen/Qwen3-1.7B-GGUF/resolve/main/qwen3-1.7b-q4_k_m.gguf" \
      -o "$qwen_gguf" 2>/dev/null || { warn "Could not download Qwen3; skipping alignment"; return 1; }
  fi

  local out_dir="$TM_DIR/aligned_v$(_tm_get_var "$TM_VERSION_VAR")"
  mkdir -p "$out_dir"
  info "Alignment: generating anti-hallucination training pairs with Qwen3..."

  BASE_MODEL="$base_model" QWEN_GGUF="$qwen_gguf" OUT_DIR="$out_dir" \
  LLAMA_BIN_VAL="${LLAMA_BIN:-}" "$PYTHON" - <<'PYEOF'
import os, json, subprocess, sys, random
base_model = os.environ['BASE_MODEL']
qwen_gguf  = os.environ['QWEN_GGUF']
out_dir    = os.environ['OUT_DIR']
llama_bin  = os.environ.get('LLAMA_BIN_VAL','')

# Generate factual Q&A pairs using Qwen3 as teacher
alignment_prompts = [
    "What is 2+2? Answer only with the number.",
    "Name the capital of France. Answer in one word.",
    "Is the Earth round or flat? Answer in one sentence.",
    "What is Python? Answer in 1-2 sentences without fabricating details.",
    "What does CPU stand for? Answer in one sentence.",
    "Name three primary colors. List only the colors.",
    "What year did World War 2 end? Answer with just the year.",
    "What language is used to style web pages? One word answer.",
    "Is the sun a star? Yes or no, then one sentence explanation.",
    "What is machine learning? Define it in 1-2 sentences without making things up.",
    "Who wrote Hamlet? One sentence answer.",
    "What does HTTP stand for? Full expansion only.",
    "Name the planet closest to the Sun. One word.",
    "What is the boiling point of water at sea level? Number and unit.",
    "How many continents are there? Number only.",
    "What is photosynthesis? One factual sentence.",
    "What programming language is named after a snake? One word.",
    "What is RAM used for? One sentence.",
    "Name the largest ocean. One word.",
    "What does AI stand for? Two words.",
]

pairs = []
if llama_bin and llama_bin != 'llama_cpp_python':
    for prompt in alignment_prompts:
        try:
            result = subprocess.run(
                [llama_bin, '-m', qwen_gguf, '-p',
                 f'<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n',
                 '-n', '64', '--temp', '0.1', '--top-k', '10',
                 '--no-display-prompt', '--repeat-penalty', '1.1'],
                capture_output=True, text=True, timeout=30
            )
            response = result.stdout.strip()
            if response and len(response) > 2:
                pairs.append({'instruction': prompt, 'response': response,
                              'source': 'qwen3_alignment'})
        except Exception as e:
            pass

if not pairs:
    # Fallback: static high-quality alignment pairs
    pairs = [
        {"instruction": "What is 2+2?", "response": "4"},
        {"instruction": "Name the capital of France.", "response": "Paris"},
        {"instruction": "Is the Earth round or flat?", "response": "The Earth is round (an oblate spheroid)."},
        {"instruction": "What does CPU stand for?", "response": "Central Processing Unit."},
        {"instruction": "What year did World War 2 end?", "response": "1945"},
        {"instruction": "What is Python?", "response": "Python is a high-level, interpreted programming language known for its readable syntax."},
        {"instruction": "What does HTTP stand for?", "response": "HyperText Transfer Protocol."},
        {"instruction": "What is machine learning?", "response": "Machine learning is a subset of AI where systems learn patterns from data to make predictions."},
        {"instruction": "Name the planet closest to the Sun.", "response": "Mercury."},
        {"instruction": "What is the boiling point of water at sea level?", "response": "100°C (212°F)."},
        {"instruction": "How many continents are there?", "response": "7"},
        {"instruction": "What does RAM stand for?", "response": "Random Access Memory."},
        {"instruction": "Who wrote Hamlet?", "response": "William Shakespeare."},
        {"instruction": "Name the largest ocean.", "response": "The Pacific Ocean."},
        {"instruction": "What does AI stand for?", "response": "Artificial Intelligence."},
        {"instruction": "What language styles web pages?", "response": "CSS (Cascading Style Sheets)."},
        {"instruction": "What programming language is named after a snake?", "response": "Python."},
        {"instruction": "Name three primary colors.", "response": "Red, blue, and yellow."},
        {"instruction": "Is the sun a star?", "response": "Yes. The Sun is a G-type main-sequence star at the center of our solar system."},
        {"instruction": "What is photosynthesis?", "response": "Photosynthesis is the process by which plants use sunlight, water, and CO2 to produce glucose and oxygen."},
    ]

# Save alignment dataset
align_file = f"{out_dir}/alignment_data.jsonl"
with open(align_file, 'w') as f:
    for p in pairs: f.write(json.dumps(p) + '\n')
print(f"Generated {len(pairs)} alignment pairs → {align_file}")

# Fine-tune the model on alignment data
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, \
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, TaskType

    tokenizer = AutoTokenizer.from_pretrained(base_model)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(base_model)
    lora = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16,
                      lora_dropout=0.05, target_modules=["q_proj","v_proj","k_proj","o_proj"])
    model = get_peft_model(model, lora)

    texts = [f"### Instruction:\n{p['instruction']}\n\n### Response:\n{p['response']}" for p in pairs]
    ds = Dataset.from_list([{'text': t} for t in texts])
    def tok(ex):
        return tokenizer(ex['text'], truncation=True, max_length=256, padding='max_length')
    ds = ds.map(tok, batched=True, remove_columns=['text'])

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)

    args = TrainingArguments(
        output_dir=out_dir, num_train_epochs=3,
        per_device_train_batch_size=1, gradient_accumulation_steps=4,
        learning_rate=2e-4, logging_steps=5, save_steps=100,
        report_to='none', warmup_ratio=0.1,
        fp16=(device=='cuda'), dataloader_num_workers=0,
    )
    Trainer(model=model, args=args, train_dataset=ds,
            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)).train()
    merged = model.merge_and_unload()
    merged.save_pretrained(out_dir)
    tokenizer.save_pretrained(out_dir)
    print(f"Alignment fine-tune saved → {out_dir}")
except Exception as e:
    print(f"Alignment training error (data saved): {e}")
PYEOF

  if [[ -d "$out_dir" ]]; then
    ok "Alignment complete: $out_dir"
    # Set as latest model
    local ver; ver=$(_tm_get_var "$TM_VERSION_VAR")
    local aligned_link="$TM_DIR/ft_v${ver}_aligned"
    [[ -L "$aligned_link" ]] && rm -f "$aligned_link"
    ln -sfn "$out_dir" "$aligned_link" 2>/dev/null || true
  else
    warn "Alignment output not found"
  fi
}

# ── Manual RLHF: rating system ────────────────────────────────────────────────
_rlhf_rate() {
  local prompt="$1" response="$2" rating="${3:-}"

  if [[ -z "$rating" ]]; then
    echo -e "\n${B}Rate this response:${R}"
    echo -e "  ${BGREEN}5${R} ★★★★★ Excellent"
    echo -e "  ${BBLUE}4${R} ★★★★☆ Good"
    echo -e "  ${BYELLOW}3${R} ★★★☆☆ OK"
    echo -e "  ${BRED}2${R} ★★☆☆☆ Poor"
    echo -e "  ${RED}1${R} ★☆☆☆☆ Wrong/Harmful"
    echo -e "  ${DIM}s${R} Skip"
    read -rp "Rating [1-5/s]: " rating
    [[ "$rating" == "s" || -z "$rating" ]] && return
  fi

  local ts; ts=$(date -Iseconds)
  local score
  case "$rating" in
    5) score="1.0" ;;  4) score="0.8" ;;  3) score="0.6" ;;
    2) score="0.3" ;;  1) score="0.0" ;;  *) return ;;
  esac

  printf '{"ts":"%s","prompt":%s,"response":%s,"rating":%s,"score":%s,"source":"manual"}\n' \
    "$ts" \
    "$(python3 -c "import json,sys; print(json.dumps(sys.argv[1]))" "$prompt" 2>/dev/null || echo "\"$prompt\"")" \
    "$(python3 -c "import json,sys; print(json.dumps(sys.argv[1]))" "$response" 2>/dev/null || echo "\"$response\"")" \
    "$rating" "$score" \
    >> "$RLHF_RATINGS_FILE"

  case "$rating" in
    5) echo -e "${BGREEN}✓ Saved — Excellent${R}" ;;
    4) echo -e "${BBLUE}✓ Saved — Good${R}" ;;
    3) echo -e "${BYELLOW}✓ Saved${R}" ;;
    2|1) echo -e "${BRED}✓ Saved — Will use for improvement${R}" ;;
  esac
  ok "Rating saved. Total: $(wc -l < "$RLHF_RATINGS_FILE" 2>/dev/null)  |  ai rlhf train-on-ratings"
}

cmd_rlhf() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    status)
      hdr "RLHF Status"
      printf "  %-25s %s\n" "Auto-RLHF:" "$RLHF_AUTO"
      printf "  %-25s %s\n" "Judge model:" "$RLHF_JUDGE"
      printf "  %-25s %s\n" "Reward threshold:" "$RLHF_REWARD_THRESHOLD"
      printf "  %-25s %s\n" "Manual ratings:" "$(wc -l < "$RLHF_RATINGS_FILE" 2>/dev/null || echo 0)"
      printf "  %-25s %s\n" "Auto pairs collected:" "$(wc -l < "$RLHF_PAIRS_FILE" 2>/dev/null || echo 0)"
      ;;
    enable)
      RLHF_AUTO="1"; save_config
      ok "Auto-RLHF enabled (judge: $RLHF_JUDGE)"
      warn "Run 'ai rlhf download-judges' to get judge models"
      ;;
    disable) RLHF_AUTO="0"; save_config; ok "Auto-RLHF disabled" ;;
    judge)
      local j="${1:-}"; [[ -z "$j" ]] && {
        hdr "Available RLHF Judges"
        for k in "${!RLHF_JUDGES[@]}"; do
          IFS='|' read -r _ _ desc <<< "${RLHF_JUDGES[$k]}"
          printf "  ${B}%-18s${R} %s\n" "$k" "$desc"
        done
        echo ""; read -rp "Choose judge [nix26/qwen3+luth/qwen3+llama32]: " j
      }
      [[ -z "${RLHF_JUDGES[$j]:-}" ]] && { err "Unknown judge: $j"; return 1; }
      RLHF_JUDGE="$j"; save_config; ok "Judge: $j"
      ;;
    download-judges) _rlhf_download_judge ;;
    train)
      local model="${1:-$ACTIVE_MODEL}"
      [[ -z "$model" ]] && { err "No model specified"; return 1; }
      info "Running DPO training on auto-collected pairs..."
      _rlhf_dpo_train "$model"
      ;;
    train-on-ratings)
      local count; count=$(wc -l < "$RLHF_RATINGS_FILE" 2>/dev/null || echo 0)
      (( count < 5 )) && { warn "Need at least 5 ratings (have $count)"; return 1; }
      # Merge ratings into pairs file then train
      cat "$RLHF_RATINGS_FILE" >> "$RLHF_PAIRS_FILE"
      _rlhf_dpo_train "$ACTIVE_MODEL"
      ;;
    rate)
      local prompt="${1:-}"; local response="${2:-}"; local rating="${3:-}"
      if [[ -z "$prompt" ]]; then
        read -rp "Prompt: " prompt; read -rp "Response: " response
      fi
      _rlhf_rate "$prompt" "$response" "$rating"
      ;;
    align)
      local id="${1:-TTM}"
      _tm_vars "$id" 2>/dev/null && _tm_align "$id"
      ;;
    clear-pairs)
      read -rp "Clear all collected RLHF pairs? [y/N]: " c
      [[ "$c" =~ ^[Yy]$ ]] && { > "$RLHF_PAIRS_FILE"; > "$RLHF_RATINGS_FILE"; ok "Cleared"; }
      ;;
    threshold)
      RLHF_REWARD_THRESHOLD="${1:-0.6}"; save_config
      ok "Reward threshold: $RLHF_REWARD_THRESHOLD"
      ;;

    # ── v2.4.5: HuggingFace RLHF datasets ─────────────────────────────────────
    datasets|list-datasets)
      _rlhf_hf_list_presets
      ;;
    add-dataset)
      local ds="${1:?Usage: ai rlhf add-dataset <hf-id-or-preset-name>}"
      local split="${2:-train[:3000]}"
      _rlhf_hf_import "$ds" "$split"
      ;;
    use-dataset)
      local ds="${1:?Usage: ai rlhf use-dataset <hf-id-or-preset-name>}"
      _rlhf_hf_set_active "$ds"
      ;;
    my-datasets)
      _rlhf_hf_list_imported
      ;;

    # v2.5: RLHF v2 — reward model, PPO, GRPO
    reward-model|train-reward)
      _rlhf_train_reward_model "${1:-TTM}" ;;
    ppo|train-ppo)
      _rlhf_train_ppo "${1:-TTM}" ;;
    grpo|train-grpo)
      _rlhf_train_grpo "${1:-TTM}" ;;

    *)
      hdr "RLHF v2 — Reinforcement Learning from Human Feedback"
      echo ""
      echo "  ${B}Auto-RLHF${R}  (judge models score responses, DPO trains on pairs)"
      echo "  ai rlhf enable / disable"
      echo "  ai rlhf judge [nix26|qwen3+luth|qwen3+llama32]"
      echo "  ai rlhf download-judges      — Download selected judge model(s)"
      echo "  ai rlhf train [model-path]   — Run DPO on collected pairs"
      echo "  ai rlhf threshold <0.0-1.0>  — Set reward cutoff (default 0.6)"
      echo ""
      echo "  ${B}v2.5: RLHF v2 additions${R}"
      echo "  ai rlhf reward-model [model] — Train reward model on pairs"
      echo "  ai rlhf ppo [model]          — PPO fine-tuning with reward model"
      echo "  ai rlhf grpo [model]         — GRPO training (DeepSeek-R1 style)"
      echo ""
      echo "  ${B}Manual RLHF${R}  (rate responses 1-5 stars)"
      echo "  ai rlhf rate                 — Rate a response interactively"
      echo "  ai rlhf train-on-ratings     — Fine-tune on your ratings"
      echo ""
      echo "  ${B}HF RLHF Datasets (v2.4.5)${R}  — curated preference datasets"
      echo "  ai rlhf datasets             — List available HF preset datasets"
      echo "  ai rlhf add-dataset <id>     — Import a HF dataset into RLHF pairs"
      echo "  ai rlhf use-dataset <id>     — Set as active RLHF training source"
      echo "  ai rlhf my-datasets          — Show imported datasets + counts"
      echo ""
      echo "  ${B}Alignment${R}  (Qwen3-powered anti-hallucination pass)"
      echo "  ai rlhf align TTM|MTM|Mtm    — Run alignment on trained model"
      echo ""
      echo "  ai rlhf status               — Show RLHF stats"
      echo "  ai rlhf clear-pairs          — Clear collected data"
      echo ""
      echo -e "  ${DIM}Judge options:${R}"
      for k in "${!RLHF_JUDGES[@]}"; do
        IFS='|' read -r _ _ desc <<< "${RLHF_JUDGES[$k]}"
        printf "    ${B}%-18s${R} %s\n" "$k" "$desc"
      done
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  RLHF HF DATASETS  (v2.4.5)
#  Curated list of HuggingFace preference/RLHF datasets
#  Import → convert to {prompt, chosen, rejected} format → merge into RLHF pairs
# ════════════════════════════════════════════════════════════════════════════════

# Curated RLHF / preference datasets on HuggingFace
# Format: "hf-id|field-map|description"
declare -A RLHF_HF_PRESETS
RLHF_HF_PRESETS=(
  [hh-rlhf]="Anthropic/hh-rlhf|chosen+rejected|Anthropic HH-RLHF (human pref, 160k pairs)"
  [summarize]="openai/summarize_from_feedback|info.post+summary|OpenAI Summary Feedback (93k)"
  [pku-safe]="PKU-Alignment/PKU-SafeRLHF|prompt+response_0+response_1+better_response_id|PKU-SafeRLHF (330k safety pairs)"
  [ultrafeedback]="HuggingFaceH4/ultrafeedback_binarized|prompt+chosen+rejected|UltraFeedback binarized (61k)"
  [helpsteer2]="nvidia/HelpSteer2|prompt+response+helpfulness|NVIDIA HelpSteer2 (21k rated)"
  [orca-dpo]="Intel/orca_dpo_pairs|system+question+chosen+rejected|Orca DPO pairs (12.9k)"
  [capybara]="argilla/distilabel-capybara-dpo-7k-binarized|instruction+chosen+rejected|Capybara DPO 7k"
  [math-pref]="argilla/distilabel-math-preference-dpo|instruction+chosen+rejected|Math preference DPO"
  [openhermes-pref]="argilla/openhermes2.5-dpo-binarized-alpha|prompt+chosen+rejected|OpenHermes 2.5 DPO"
  [skywork-reward]="Skywork/Skywork-Reward-Preference-80K-v0.2|prompt+chosen+rejected|Skywork Reward 80k"
)

_rlhf_hf_list_presets() {
  hdr "HuggingFace RLHF Datasets (v2.4.5)"
  echo ""
  printf "  %-18s  %-45s  %s\n" "PRESET NAME" "HF REPO" "DESCRIPTION"
  printf "  %s\n" "$(printf '%0.s-' {1..90})"
  for key in "${!RLHF_HF_PRESETS[@]}"; do
    IFS='|' read -r hf_id _ desc <<< "${RLHF_HF_PRESETS[$key]}"
    printf "  %-18s  %-45s  %s\n" "$key" "$hf_id" "$desc"
  done | sort
  echo ""
  echo "  Add any preset: ai rlhf add-dataset <name>"
  echo "  Or any HF id:   ai rlhf add-dataset Anthropic/hh-rlhf"
  echo "  Limit sample:   ai rlhf add-dataset hh-rlhf 'train[:2000]'"
}

_rlhf_hf_list_imported() {
  [[ ! -f "$RLHF_HF_DATASETS_FILE" ]] && { info "No HF datasets imported yet"; return; }
  hdr "Imported RLHF Datasets"
  python3 -c "
import json, sys
try:
    ds = json.load(open('$RLHF_HF_DATASETS_FILE'))
except:
    ds = []
if not ds:
    print('  None.')
    sys.exit()
for d in ds:
    active = ' [active]' if d.get('active') else ''
    print(f\"  {d.get('name','?'):20}  {d.get('source','?'):40}  {d.get('pairs',0):>6} pairs{active}\")
"
}

_rlhf_hf_set_active() {
  local name="$1"
  [[ ! -f "$RLHF_HF_DATASETS_FILE" ]] && { err "No HF datasets. Import one first."; return 1; }
  python3 - <<PYEOF
import json
dss = json.load(open('$RLHF_HF_DATASETS_FILE'))
found = False
for d in dss:
    if d.get('name') == '$name' or d.get('source', '').endswith('$name'):
        d['active'] = True; found = True
    else:
        d['active'] = False
json.dump(dss, open('$RLHF_HF_DATASETS_FILE', 'w'), indent=2)
# Print the jsonl file path for the active dataset so bash can capture it
active = next((d for d in dss if d.get('active')), None)
if active:
    import pathlib
    # Derive the per-dataset jsonl path from pairs file convention
    ds_name = active.get('name', '')
    print(f'ACTIVE_PATH:$CONFIG_DIR/rlhf_hf_{ds_name}.jsonl')
print('Active RLHF dataset set to: $name' if found else 'Dataset not found: $name')
PYEOF
  local out; out=$?
  if [[ $out -eq 0 ]]; then
    # Extract and save the active file path if printed
    local active_path
    active_path=$(python3 -c "
import json,sys
dss=json.load(open('$RLHF_HF_DATASETS_FILE'))
a=next((d for d in dss if d.get('active')),None)
print('$CONFIG_DIR/rlhf_hf_'+a['name']+'.jsonl' if a else '') " 2>/dev/null || true)
    [[ -n "$active_path" ]] && RLHF_ACTIVE_HF_DATASET="$active_path"
    save_config
    ok "Active RLHF dataset: $name"
  fi
}

_rlhf_hf_import() {
  local input="$1"; local split="${2:-train[:3000]}"
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }

  # Resolve preset name to HF id + field map
  local hf_id="$input" field_map=""
  if [[ -n "${RLHF_HF_PRESETS[$input]:-}" ]]; then
    IFS='|' read -r hf_id field_map _ <<< "${RLHF_HF_PRESETS[$input]}"
  fi

  local ds_name; ds_name=$(echo "$hf_id" | tr '/' '_' | tr -d '.')
  info "Importing RLHF dataset: $hf_id (split: $split)"
  info "Output: $RLHF_PAIRS_FILE"

  HF_ID="$hf_id" SPLIT="$split" FIELD_MAP="$field_map" \
  PAIRS_FILE="$RLHF_PAIRS_FILE" DS_NAME="$ds_name" \
  HF_DATASETS_FILE="$RLHF_HF_DATASETS_FILE" \
  HF_TOKEN_VAL="${HF_TOKEN:-}" \
  "$PYTHON" - <<'PYEOF'
import os, sys, json
hf_id      = os.environ['HF_ID']
split      = os.environ.get('SPLIT', 'train[:3000]')
field_map  = os.environ.get('FIELD_MAP', '')
pairs_file = os.environ['PAIRS_FILE']
ds_name    = os.environ['DS_NAME']
hf_ds_file = os.environ['HF_DATASETS_FILE']
hf_token   = os.environ.get('HF_TOKEN_VAL', '') or None

try:
    from datasets import load_dataset
except ImportError:
    print("Missing: datasets\nRun: pip install datasets"); sys.exit(1)

print(f"  Loading {hf_id} [{split}] ...")
try:
    ds = load_dataset(hf_id, split=split, token=hf_token, trust_remote_code=True)
except Exception as e:
    # Try without split parameter
    try:
        ds = load_dataset(hf_id, token=hf_token, trust_remote_code=True)
        # Get first available split
        if hasattr(ds, 'keys'):
            first_split = list(ds.keys())[0]
            ds = ds[first_split]
            if '[' in split:
                n = int(split.split('[')[1].rstrip(']').replace(':', ''))
                ds = ds.select(range(min(n, len(ds))))
    except Exception as e2:
        print(f"  Failed to load: {e2}"); sys.exit(1)

print(f"  Loaded {len(ds)} examples. Columns: {ds.column_names}")

# Smart field detection
cols = ds.column_names
pairs = []
for row in ds:
    chosen = None; rejected = None; prompt = None
    # Try explicit field_map first
    if field_map:
        fields = field_map.split('+')
        if len(fields) >= 2:
            if 'chosen' in fields and 'rejected' in fields:
                chosen   = str(row.get('chosen', '') or '')
                rejected = str(row.get('rejected', '') or '')
                prompt   = str(row.get('prompt', row.get('instruction', row.get('system', ''))) or '')
            elif 'response_0' in fields:
                # PKU-SafeRLHF style
                bid = int(row.get('better_response_id', 0))
                r0 = str(row.get('response_0', '') or '')
                r1 = str(row.get('response_1', '') or '')
                prompt = str(row.get('prompt', '') or '')
                chosen   = r0 if bid == 0 else r1
                rejected = r1 if bid == 0 else r0
            else:
                # Generic: treat first as prompt, second as chosen
                vals = [str(row.get(f, '') or '') for f in fields if f in row]
                if len(vals) >= 2:
                    prompt = vals[0]; chosen = vals[1]
    # Auto-detect if still not set
    if chosen is None:
        if 'chosen' in cols and 'rejected' in cols:
            chosen   = str(row.get('chosen', '') or '')
            rejected = str(row.get('rejected', '') or '')
            prompt   = str(row.get('prompt', row.get('instruction', '')) or '')
        elif 'response' in cols and 'helpfulness' in cols:
            # HelpSteer2 style — score > 3 = chosen
            score = float(row.get('helpfulness', 3))
            if score >= 3.5:
                chosen = str(row.get('response', '') or '')
                prompt = str(row.get('prompt', '') or '')
        elif 'output' in cols:
            prompt = str(row.get('input', row.get('instruction', '')) or '')
            chosen = str(row.get('output', '') or '')
    if not chosen or not prompt:
        continue
    entry = {'prompt': prompt[:512], 'chosen': chosen[:1024]}
    if rejected:
        entry['rejected'] = rejected[:1024]
    entry['source'] = hf_id
    pairs.append(entry)

print(f"  Extracted {len(pairs)} preference pairs")
if not pairs:
    print("  No pairs could be extracted — check column names above"); sys.exit(1)

# Append to RLHF pairs file
with open(pairs_file, 'a') as f:
    for p in pairs:
        f.write(json.dumps(p) + '\n')
print(f"  Appended to: {pairs_file}")

# Track in HF datasets registry
try:
    reg = json.load(open(hf_ds_file)) if os.path.exists(hf_ds_file) else []
except:
    reg = []
# Update or add entry
found = False
for d in reg:
    if d.get('source') == hf_id:
        d['pairs'] += len(pairs); found = True
if not found:
    reg.append({'name': ds_name, 'source': hf_id, 'pairs': len(pairs),
                'split': split, 'active': True})
json.dump(reg, open(hf_ds_file, 'w'), indent=2)
print(f"  Registered as: {ds_name}")
print(f"  Total RLHF pairs now: {sum(1 for _ in open(pairs_file))}")
PYEOF

  if [[ $? -eq 0 ]]; then
    ok "HF RLHF dataset imported: $hf_id"
    local total; total=$(wc -l < "$RLHF_PAIRS_FILE" 2>/dev/null || echo 0)
    echo "  Total RLHF pairs: $total"
    echo "  Train now: ai rlhf train"
  else
    err "Import failed"
    return 1
  fi
}


# ════════════════════════════════════════════════════════════════════════════════
#  RIGHT-CLICK CONTEXT MENU — Linux system-wide "Ask AI" integration  (v2.4.6)
#  Works on: GNOME, KDE Plasma 5/6, XFCE, LXDE, MATE, Cinnamon, Openbox,
#            i3, sway, Hyprland, river, dwm, any WM/DE
#  Grabs selected text (X11 primary / Wayland / clipboard), sends to AI,
#  shows result in best available display method.
#  Custom keybind support: ai rclick keybind <key-combo>
# ════════════════════════════════════════════════════════════════════════════════

# Vision-Language model options for right-click
declare -A RCLICK_VL_MODELS
RCLICK_VL_MODELS=(
  [qwen3vl]="Qwen/Qwen3-VL-2B-Thinking-GGUF|Qwen3-VL-2B-Thinking-Q4_K_M.gguf|Qwen3 VL 2B Thinking — best reasoning"
  [lfm25vl]="LiquidAI/LFM2.5-VL-1.6B|model.safetensors|LFM2.5 VL 1.6B (PyTorch)"
  [lfm25vl_gguf]="LiquidAI/LFM2.5-VL-1.6B-GGUF|lfm2.5-vl-1.6b-q4_k_m.gguf|LFM2.5 VL 1.6B GGUF — fast"
  [custom]="custom||Custom model (set RCLICK_CUSTOM_MODEL)"
)

# Default keybind — user can change via: ai rclick keybind <combo>
RCLICK_KEYBIND="${RCLICK_KEYBIND:-Super+Shift+a}"

_rclick_install_deps() {
  info "Installing right-click context menu dependencies..."
  local pkgs=()
  # Detect display server: Wayland or X11
  local is_wayland=0
  [[ -n "${WAYLAND_DISPLAY:-}" ]] && is_wayland=1
  [[ -n "${SWAYSOCK:-}" || -n "${HYPRLAND_INSTANCE_SIGNATURE:-}" ]] && is_wayland=1

  if command -v apt-get &>/dev/null; then
    pkgs=(libnotify-bin xdg-utils python3-tk)
    if [[ $is_wayland -eq 1 ]]; then
      pkgs+=(wl-clipboard)
      # Try to get ydotool for Wayland input simulation
      command -v ydotool &>/dev/null || pkgs+=(ydotool) 2>/dev/null || true
    else
      pkgs+=(xdotool xclip xsel zenity)
      command -v python3 &>/dev/null && pkgs+=(python3-tkinter python3-gi)
    fi
    sudo apt-get install -y -q "${pkgs[@]}" 2>/dev/null || true
  elif command -v dnf &>/dev/null; then
    if [[ $is_wayland -eq 1 ]]; then
      sudo dnf install -y libnotify wl-clipboard python3-tkinter 2>/dev/null || true
    else
      sudo dnf install -y xdotool xclip xsel libnotify zenity python3-tkinter 2>/dev/null || true
    fi
  elif command -v pacman &>/dev/null; then
    if [[ $is_wayland -eq 1 ]]; then
      sudo pacman -S --noconfirm libnotify wl-clipboard tk 2>/dev/null || true
    else
      sudo pacman -S --noconfirm xdotool xclip xsel libnotify zenity tk 2>/dev/null || true
    fi
  elif command -v zypper &>/dev/null; then
    sudo zypper install -y xdotool xclip libnotify-tools zenity python3-tk 2>/dev/null || true
  fi
  ok "Dependencies installed"
}

_rclick_get_selection() {
  # Robust text retrieval: Wayland primary → X11 primary → clipboard
  local text=""
  # Wayland
  if [[ -n "${WAYLAND_DISPLAY:-}" || -n "${SWAYSOCK:-}" || -n "${HYPRLAND_INSTANCE_SIGNATURE:-}" ]]; then
    command -v wl-paste &>/dev/null && text=$(wl-paste --primary --no-newline 2>/dev/null || true)
    [[ -z "$text" ]] && command -v wl-paste &>/dev/null && text=$(wl-paste --no-newline 2>/dev/null || true)
  fi
  # X11 primary (highlighted text)
  if [[ -z "$text" ]]; then
    command -v xclip &>/dev/null && text=$(xclip -selection primary -o 2>/dev/null || true)
    [[ -z "$text" ]] && command -v xsel &>/dev/null && text=$(xsel --primary --output 2>/dev/null || true)
  fi
  # Clipboard fallback
  if [[ -z "$text" ]]; then
    command -v xclip  &>/dev/null && text=$(xclip -selection clipboard -o 2>/dev/null || true)
    [[ -z "$text" ]] && command -v xsel     &>/dev/null && text=$(xsel --clipboard --output 2>/dev/null || true)
    [[ -z "$text" ]] && command -v wl-paste &>/dev/null && text=$(wl-paste --no-newline 2>/dev/null || true)
  fi
  echo "${text:0:4000}"
}

# Convert user-friendly key combo to gsettings format
# e.g. "Super+Shift+a" → "<Super><Shift>a"
_rclick_key_to_gsettings() {
  local key="$1"
  local out=""
  IFS='+' read -ra parts <<< "$key"
  local last="${parts[-1]}"
  for (( i=0; i<${#parts[@]}-1; i++ )); do
    local mod="${parts[$i]}"
    case "${mod,,}" in
      super|win|meta) out+="<Super>" ;;
      ctrl|control)   out+="<Ctrl>"  ;;
      alt)            out+="<Alt>"   ;;
      shift)          out+="<Shift>" ;;
      *)              out+="<${mod}>" ;;
    esac
  done
  out+="${last,,}"
  echo "$out"
}

# Convert user-friendly key combo to sway/i3 bindsym format
# e.g. "Super+Shift+a" → "$mod+Shift+a"  (or "Ctrl+Shift+a" stays as-is)
_rclick_key_to_sway() {
  local key="$1"
  # Replace Super/Win with $mod
  echo "$key" | sed 's/Super/$mod/Ig;s/Win/$mod/Ig'
}

# Write the ai-rclick script to /usr/local/bin/ai-rclick  (v2.4.6 rewrite)
_rclick_write_script() {
  local script_path="/usr/local/bin/ai-rclick"
  local cli_bin; cli_bin=$(command -v ai 2>/dev/null || echo "ai")
  local vl_model="${RCLICK_VL_MODEL:-qwen3vl}"
  local vl_dir="$MODELS_DIR/rclick_vl"

  cat > /tmp/ai_rclick_v246.sh << RCLICK_SCRIPT
#!/usr/bin/env bash
# AI Right-Click Handler — installed by ai-cli v2.4.6
# Supports: X11, Wayland, all major DEs/WMs
CLI_BIN="${cli_bin}"
VL_MODEL_DIR="${vl_dir}"
VL_TYPE="${vl_model}"

# ── Get selected text (X11 primary / Wayland / clipboard) ─────────────────────
get_text() {
  local t=""
  # Wayland primary selection
  if [[ -n "\${WAYLAND_DISPLAY:-}" || -n "\${SWAYSOCK:-}" || -n "\${HYPRLAND_INSTANCE_SIGNATURE:-}" ]]; then
    command -v wl-paste &>/dev/null && t=\$(wl-paste --primary --no-newline 2>/dev/null || true)
  fi
  # X11 primary (highlighted text — most reliable for "select then trigger")
  if [[ -z "\$t" ]]; then
    command -v xclip &>/dev/null && t=\$(xclip -selection primary -o 2>/dev/null || true)
    [[ -z "\$t" ]] && command -v xsel &>/dev/null && t=\$(xsel --primary --output 2>/dev/null || true)
  fi
  # Clipboard fallback
  if [[ -z "\$t" ]]; then
    command -v xclip  &>/dev/null && t=\$(xclip -selection clipboard -o 2>/dev/null || true)
    [[ -z "\$t" ]] && command -v xsel     &>/dev/null && t=\$(xsel --clipboard --output 2>/dev/null || true)
    [[ -z "\$t" ]] && command -v wl-paste &>/dev/null && t=\$(wl-paste --no-newline 2>/dev/null || true)
  fi
  echo "\${t:0:4000}"
}

# ── Show result in best available UI ──────────────────────────────────────────
show_result() {
  local title="\$1" body="\$2" tmp
  tmp=\$(mktemp /tmp/ai_result_XXXXX.txt)
  echo "\$body" > "\$tmp"
  # Prefer GUI dialogs, fall back gracefully
  if command -v zenity &>/dev/null; then
    zenity --text-info --title="\$title" --filename="\$tmp" \
           --width=720 --height=500 --font="Monospace 10" 2>/dev/null &
  elif command -v kdialog &>/dev/null; then
    kdialog --title "\$title" --textbox "\$tmp" 720 500 2>/dev/null &
  elif command -v yad &>/dev/null; then
    yad --text-info --filename="\$tmp" --title="\$title" \
        --width=720 --height=500 --wrap --button=Close:0 2>/dev/null &
  elif command -v xmessage &>/dev/null; then
    xmessage -file "\$tmp" -title "\$title" -buttons OK 2>/dev/null &
  elif command -v python3 &>/dev/null; then
    python3 - "\$title" "\$tmp" &
cat << 'PYEOF'
import sys, pathlib, tkinter as tk
from tkinter import scrolledtext, font as tkfont
root = tk.Tk()
root.title(sys.argv[1])
root.geometry("800x550")
try:
    mf = tkfont.Font(family="Monospace", size=10)
except:
    mf = None
txt = scrolledtext.ScrolledText(root, wrap=tk.WORD, font=mf, padx=8, pady=8)
txt.pack(fill='both', expand=True)
txt.insert('1.0', pathlib.Path(sys.argv[2]).read_text())
txt.config(state='disabled')
btn = tk.Frame(root); btn.pack(fill='x', pady=4)
tk.Button(btn, text='Copy', command=lambda: (root.clipboard_clear(), root.clipboard_append(txt.get('1.0','end')))).pack(side='left', padx=8)
tk.Button(btn, text='Close', command=root.destroy).pack(side='right', padx=8)
root.mainloop()
PYEOF
  elif command -v foot &>/dev/null; then
    foot -e bash -c "cat \$tmp; echo; read -p 'Press Enter to close...'" &
  elif command -v alacritty &>/dev/null; then
    alacritty -e bash -c "cat \$tmp; echo; read -p 'Press Enter to close...'" &
  elif command -v xterm &>/dev/null; then
    xterm -title "\$title" -e "cat \$tmp; read -p 'Press Enter...'" &
  else
    # Last resort: notification + save
    notify-send "\$title" "\${body:0:400}..." -t 20000 2>/dev/null || true
    cp "\$tmp" /tmp/ai_rclick_last_result.txt
    echo "Full result: /tmp/ai_rclick_last_result.txt"
  fi
  # Cleanup after 60 s
  (sleep 60 && rm -f "\$tmp") &
}

# ── Input dialog for question ─────────────────────────────────────────────────
ask_question() {
  local ctx="\${1:0:100}"
  local q=""
  if command -v zenity &>/dev/null; then
    q=\$(zenity --entry --title="Ask AI" \
      --text="Selected: \${ctx}...\n\nYour question:" \
      --entry-text="Explain this" --width=500 2>/dev/null) || return 1
  elif command -v kdialog &>/dev/null; then
    q=\$(kdialog --title "Ask AI" \
      --inputbox "Selected: \${ctx}...\nYour question:" "Explain this" 2>/dev/null) || return 1
  elif command -v yad &>/dev/null; then
    q=\$(yad --entry --title="Ask AI" \
      --text="Selected: \${ctx}...\n\nYour question:" \
      --entry-text="Explain this" --width=500 2>/dev/null) || return 1
  elif command -v python3 &>/dev/null; then
    q=\$(python3 - "\$ctx" << 'PYEOF'
import sys, tkinter as tk
from tkinter import simpledialog
root = tk.Tk(); root.withdraw()
q = simpledialog.askstring("Ask AI",
    f"Context: {sys.argv[1]}...\n\nYour question:",
    initialvalue="Explain this")
print(q or "")
PYEOF
) || return 1
  else
    q="Explain this"
  fi
  [[ -z "\$q" ]] && return 1
  echo "\$q"
}

# ── Main ─────────────────────────────────────────────────────────────────────
main() {
  local selected; selected=\$(get_text)
  if [[ -z "\$selected" ]]; then
    notify-send "Ask AI" "No text selected. Highlight text first, then press the shortcut." \
      -t 5000 -i dialog-information 2>/dev/null || true
    exit 0
  fi

  local question
  question=\$(ask_question "\$selected") || exit 0
  [[ -z "\$question" ]] && exit 0

  notify-send "Ask AI" "Thinking..." -t 3000 -i dialog-information 2>/dev/null || true

  local full_prompt="\$question

Context:
\$selected"

  local result
  result=\$("\$CLI_BIN" ask "\$full_prompt" 2>&1)
  if [[ -z "\$result" || "\$result" =~ ^(error|Error) ]]; then
    result="[No response — is 'ai' installed and a model configured? Run: ai status]"
  fi

  show_result "Ask AI — \${question:0:60}" "\$result"
}

main "\$@"
RCLICK_SCRIPT

  sudo cp /tmp/ai_rclick_v246.sh "$script_path"
  sudo chmod +x "$script_path"
  rm -f /tmp/ai_rclick_v246.sh
  ok "Installed: $script_path"
}

_rclick_download_vl() {
  local vl="${RCLICK_VL_MODEL:-qwen3vl}"
  local vl_dir="$MODELS_DIR/rclick_vl"
  mkdir -p "$vl_dir"

  local entry="${RCLICK_VL_MODELS[$vl]:-${RCLICK_VL_MODELS[qwen3vl]}}"
  IFS='|' read -r repo filename desc <<< "$entry"
  [[ "$repo" == "custom" ]] && { err "Set RCLICK_CUSTOM_MODEL first"; return 1; }
  [[ -z "$filename" ]] && { err "No filename for $vl"; return 1; }

  if [[ "$filename" == *.gguf ]]; then
    local dest="$vl_dir/$filename"
    [[ -f "$dest" ]] && { ok "Already present: $filename"; return; }
    info "Downloading $desc..."
    curl -L --retry 5 --progress-bar \
      ${HF_TOKEN:+-H "Authorization: Bearer $HF_TOKEN"} \
      "https://huggingface.co/${repo}/resolve/main/${filename}" \
      -o "$dest"
    ok "Downloaded: $dest"
  else
    [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
    info "Downloading PyTorch model: $desc..."
    HF_REPO="$repo" DEST="$vl_dir" HF_TOKEN_VAL="${HF_TOKEN:-}" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from huggingface_hub import snapshot_download
except ImportError:
    print("huggingface_hub not installed. Run: pip install huggingface_hub"); sys.exit(1)
snapshot_download(repo_id=os.environ['HF_REPO'],
                  local_dir=os.environ['DEST'],
                  token=os.environ.get('HF_TOKEN_VAL') or None)
print(f"Downloaded to {os.environ['DEST']}")
PYEOF
  fi
}

# ── DE-specific integrations ───────────────────────────────────────────────────

_rclick_install_gnome() {
  local keybind_gs; keybind_gs=$(_rclick_key_to_gsettings "$RCLICK_KEYBIND")
  info "Installing GNOME Nautilus right-click action..."
  mkdir -p "$HOME/.local/share/nautilus/scripts"
  printf '#!/usr/bin/env bash\nai-rclick "$@"\n' > "$HOME/.local/share/nautilus/scripts/Ask AI"
  chmod +x "$HOME/.local/share/nautilus/scripts/Ask AI"
  ok "GNOME Nautilus: Scripts > 'Ask AI'"

  # Also register with Files/Nemo (Cinnamon)
  if [[ -d "$HOME/.local/share/nemo/scripts" ]]; then
    cp "$HOME/.local/share/nautilus/scripts/Ask AI" "$HOME/.local/share/nemo/scripts/Ask AI" 2>/dev/null || true
    chmod +x "$HOME/.local/share/nemo/scripts/Ask AI" 2>/dev/null || true
    ok "Cinnamon Nemo: Scripts > 'Ask AI'"
  fi

  info "Installing GNOME Shell keyboard shortcut ($RCLICK_KEYBIND)..."
  local base="/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/ai-rclick"
  # Append to existing custom-keybindings list (don't overwrite others)
  local cur_list
  cur_list=$(gsettings get org.gnome.settings-daemon.plugins.media-keys custom-keybindings 2>/dev/null || echo "@as []")
  if [[ "$cur_list" != *"ai-rclick"* ]]; then
    local new_list="${cur_list%']'},'${base}/']"
    new_list="${new_list/\[@as \[/[}"
    gsettings set org.gnome.settings-daemon.plugins.media-keys custom-keybindings \
      "$new_list" 2>/dev/null || \
    gsettings set org.gnome.settings-daemon.plugins.media-keys custom-keybindings \
      "['${base}/']" 2>/dev/null || true
  fi
  gsettings set "org.gnome.settings-daemon.plugins.media-keys.custom-keybinding:${base}/" \
    name "Ask AI" 2>/dev/null || true
  gsettings set "org.gnome.settings-daemon.plugins.media-keys.custom-keybinding:${base}/" \
    command "ai-rclick" 2>/dev/null || true
  gsettings set "org.gnome.settings-daemon.plugins.media-keys.custom-keybinding:${base}/" \
    binding "$keybind_gs" 2>/dev/null || true
  ok "GNOME shortcut: $RCLICK_KEYBIND → ai-rclick"
}

_rclick_install_kde() {
  # Detect Plasma version
  local plasma_ver=5
  command -v kwriteconfig6 &>/dev/null && plasma_ver=6
  plasmashell --version 2>/dev/null | grep -q "^plasmashell 6" && plasma_ver=6

  info "Installing KDE Dolphin right-click service menu (Plasma $plasma_ver)..."

  # Plasma 5 service menu path
  local p5="$HOME/.local/share/kservices5/ServiceMenus"
  # Plasma 6 service menu path (KIO servicemenus)
  local p6="$HOME/.local/share/kio/servicemenus"
  mkdir -p "$p5" "$p6"

  # Write .desktop for Plasma 5 (KonqPopupMenu/Plugin style)
  cat > "$p5/ai-rclick.desktop" <<'DESK5'
[Desktop Entry]
Type=Service
ServiceTypes=KonqPopupMenu/Plugin
MimeType=all/all;
Actions=ask_ai
X-KDE-StartupNotify=false
X-KDE-Priority=TopLevel

[Desktop Action ask_ai]
Name=Ask AI
Name[en]=Ask AI
Icon=applications-science
Exec=ai-rclick %F
DESK5

  # Write .desktop for Plasma 6 (KIO servicemenu style — Actions= format)
  cat > "$p6/ai-rclick.desktop" <<'DESK6'
[Desktop Entry]
Type=Service
MimeType=all/all;
Actions=ask_ai
X-KDE-Submenu=AI Tools
X-KDE-StartupNotify=false

[Desktop Action ask_ai]
Name=Ask AI
Name[en]=Ask AI
Icon=applications-science
Exec=ai-rclick %F
DESK6

  ok "KDE Dolphin (Plasma $plasma_ver): right-click → Ask AI"

  # ── KDE Plasma 6: register global shortcut via D-Bus + kglobalaccel6 ────────
  local keybind_kde
  keybind_kde=$(echo "$RCLICK_KEYBIND" | sed 's/Super/Meta/Ig; s/Ctrl/Ctrl/g')

  if [[ $plasma_ver -eq 6 ]] && command -v kwriteconfig6 &>/dev/null; then
    # Write to kglobalshortcutsrc
    kwriteconfig6 --file kglobalshortcutsrc --group "ai-rclick.desktop" \
      --key "_k_friendly_name" "Ask AI" 2>/dev/null || true
    kwriteconfig6 --file kglobalshortcutsrc --group "ai-rclick.desktop" \
      --key "Ask AI" "${keybind_kde},none,Ask AI" 2>/dev/null || true

    # Register via kglobalaccel6 D-Bus if available
    if command -v qdbus6 &>/dev/null || command -v qdbus &>/dev/null; then
      local QDBUS
      QDBUS=$(command -v qdbus6 2>/dev/null || command -v qdbus)
      # Reload kglobalaccel to pick up new shortcut
      $QDBUS org.kde.kglobalaccel /kglobalaccel \
        org.kde.kglobalaccel.Component.reloadActionIdentifiers \
        2>/dev/null || true
    fi

    # Also register as custom shortcut via khotkeys (Plasma 6 fallback)
    local khotkeys="$HOME/.config/khotkeysrc"
    if [[ ! -f "$khotkeys" ]] || ! grep -q "ai-rclick" "$khotkeys" 2>/dev/null; then
      cat >> "$khotkeys" <<KHOTKEYS

[Data_ai_rclick]
Comment=Ask AI (ai-cli)
DataCount=1
Enabled=true
Name=Ask AI
SystemGroup=0
Type=SIMPLE_ACTION_DATA

[Data_ai_rclick_Actions]
ActionsCount=1

[Data_ai_rclick_Actions0]
CommandURL=ai-rclick
Type=COMMAND_URL

[Data_ai_rclick_Triggers]
TriggersCount=1

[Data_ai_rclick_Triggers0]
Key=${keybind_kde}
Type=SHORTCUT
Uuid={ai-rclick-uuid}
KHOTKEYS
    fi
    ok "KDE Plasma 6 shortcut: $RCLICK_KEYBIND → ai-rclick"
    info "  Reload shortcuts: qdbus6 org.kde.kglobalaccel /kglobalaccel reloadActionIdentifiers"

  elif command -v kwriteconfig5 &>/dev/null; then
    kwriteconfig5 --file kglobalshortcutsrc --group "ai-rclick.desktop" \
      --key "_k_friendly_name" "Ask AI" 2>/dev/null || true
    kwriteconfig5 --file kglobalshortcutsrc --group "ai-rclick.desktop" \
      --key "Ask AI" "${keybind_kde},none,Ask AI" 2>/dev/null || true
    ok "KDE Plasma 5 shortcut: $RCLICK_KEYBIND → ai-rclick"
  else
    warn "KDE: Manually add shortcut in System Settings > Shortcuts > Custom Shortcuts"
    info "  Command: ai-rclick   Shortcut: $RCLICK_KEYBIND"
  fi

  # Plasma 6: install xdg-open handler so Nautilus scripts work too
  if [[ $plasma_ver -eq 6 ]]; then
    local update_cmd
    update_cmd=$(command -v kbuildsycoca6 2>/dev/null || command -v kbuildsycoca5 2>/dev/null || true)
    [[ -n "$update_cmd" ]] && "$update_cmd" --noincremental 2>/dev/null &
  fi
}

_rclick_install_xfce() {
  info "Installing XFCE Thunar right-click action..."
  local uca="$HOME/.config/Thunar/uca.xml"
  mkdir -p "$(dirname "$uca")"
  [[ ! -f "$uca" ]] && echo '<actions></actions>' > "$uca"
  if ! grep -q "ai-rclick" "$uca" 2>/dev/null; then
    python3 - <<PYEOF
import xml.etree.ElementTree as ET
ET.register_namespace('', '')
try:
    tree = ET.parse('$uca')
    root = tree.getroot()
except:
    import xml.etree.ElementTree as ET2
    root = ET2.Element('actions')
    tree = ET2.ElementTree(root)
action = ET.SubElement(root, 'action')
for tag, text in [('icon','dialog-question'),('name','Ask AI'),
                  ('command','ai-rclick'),
                  ('description','Ask AI about selected text/file'),
                  ('patterns','*'),('directories','1'),('text-files','1'),
                  ('other-files','1')]:
    ET.SubElement(action, tag).text = text
tree.write('$uca', xml_declaration=True, encoding='utf-8')
print("XFCE Thunar action installed")
PYEOF
  fi
  ok "XFCE Thunar: right-click → Ask AI"

  # XFCE keyboard shortcut via xfconf
  if command -v xfconf-query &>/dev/null; then
    local keybind_xfce
    keybind_xfce=$(_rclick_key_to_gsettings "$RCLICK_KEYBIND")
    xfconf-query -c xfce4-keyboard-shortcuts -p \
      "/commands/custom/${keybind_xfce}" \
      --create -t string -s "ai-rclick" 2>/dev/null || true
    ok "XFCE shortcut: $RCLICK_KEYBIND → ai-rclick"
  fi
}

_rclick_install_mate() {
  info "Installing MATE Caja right-click action..."
  mkdir -p "$HOME/.config/caja/scripts"
  printf '#!/usr/bin/env bash\nai-rclick "$@"\n' > "$HOME/.config/caja/scripts/Ask AI"
  chmod +x "$HOME/.config/caja/scripts/Ask AI"
  ok "MATE Caja: right-click → Scripts > Ask AI"

  if command -v dconf &>/dev/null; then
    local keybind_gs; keybind_gs=$(_rclick_key_to_gsettings "$RCLICK_KEYBIND")
    dconf write /org/mate/desktop/keybindings/ask-ai/action "'ai-rclick'" 2>/dev/null || true
    dconf write /org/mate/desktop/keybindings/ask-ai/binding "'${keybind_gs}'" 2>/dev/null || true
    dconf write /org/mate/desktop/keybindings/ask-ai/name "'Ask AI'" 2>/dev/null || true
    ok "MATE shortcut: $RCLICK_KEYBIND → ai-rclick"
  fi
}

_rclick_install_lxde() {
  info "Installing LXDE/LXQt right-click (openbox menu)..."
  local ob_menu="$HOME/.config/openbox/menu.xml"
  if [[ -f "$ob_menu" ]] && ! grep -q "ai-rclick" "$ob_menu"; then
    sed -i 's|</openbox_menu>|  <menu id="ask-ai-menu" label="Ask AI" execute="ai-rclick"/>\n</openbox_menu>|' \
      "$ob_menu" 2>/dev/null || true
    ok "Openbox menu updated: Ask AI entry added"
  fi
  # LXDE keyboard shortcut
  local lxde_kb="$HOME/.config/openbox/lxde-rc.xml"
  if [[ -f "$lxde_kb" ]] && ! grep -q "ai-rclick" "$lxde_kb"; then
    local keybind_ob; keybind_ob=$(echo "$RCLICK_KEYBIND" | sed 's/Super/W/Ig;s/\+/-/g')
    sed -i "s|</keyboard>|  <keybind key=\"${keybind_ob}\">\n      <action name=\"Execute\"><command>ai-rclick</command></action>\n    </keybind>\n  </keyboard>|" \
      "$lxde_kb" 2>/dev/null || true
    ok "LXDE shortcut: $RCLICK_KEYBIND → ai-rclick"
  fi
}

_rclick_install_sway_i3() {
  local wm=""
  command -v sway &>/dev/null && wm="sway"
  command -v i3   &>/dev/null && [[ -z "$wm" ]] && wm="i3"
  [[ -n "${SWAYSOCK:-}" ]] && wm="sway"

  local keybind_sym; keybind_sym=$(_rclick_key_to_sway "$RCLICK_KEYBIND")
  local cfg_file=""
  case "$wm" in
    sway) cfg_file="${XDG_CONFIG_HOME:-$HOME/.config}/sway/config" ;;
    i3)   cfg_file="${XDG_CONFIG_HOME:-$HOME/.config}/i3/config"   ;;
  esac

  if [[ -n "$cfg_file" && -f "$cfg_file" ]]; then
    if ! grep -q "ai-rclick" "$cfg_file"; then
      printf '\n# Ask AI shortcut (ai-cli v2.4.6)\nbindsym %s exec ai-rclick\n' \
        "$keybind_sym" >> "$cfg_file"
      ok "${wm^}: Added '$keybind_sym → ai-rclick' in $cfg_file"
    else
      ok "${wm^}: ai-rclick keybind already in $cfg_file"
    fi
    info "Reload config: ${wm} reload  (or restart ${wm})"
  elif [[ -n "$wm" ]]; then
    # Config file not found — create snippet
    local cfg_dir="${XDG_CONFIG_HOME:-$HOME/.config}/$wm"
    mkdir -p "$cfg_dir"
    printf '# Ask AI shortcut (ai-cli v2.4.6)\nbindsym %s exec ai-rclick\n' \
      "$keybind_sym" >> "$cfg_dir/config"
    ok "${wm^}: Created $cfg_dir/config with keybind"
  else
    echo "  Add to your sway/i3 config:  bindsym $keybind_sym exec ai-rclick"
  fi
}

_rclick_install_hyprland() {
  info "Installing Hyprland keybinding..."
  local hcfg="${XDG_CONFIG_HOME:-$HOME/.config}/hypr/hyprland.conf"
  local keybind_hypr
  # Hyprland format: SUPER SHIFT, A, exec, ai-rclick
  keybind_hypr=$(echo "$RCLICK_KEYBIND" | \
    sed 's/Super/SUPER/Ig;s/Ctrl/CTRL/Ig;s/Alt/ALT/Ig;s/Shift/SHIFT/Ig' | \
    awk -F'+' 'BEGIN{OFS=""} {mods=""; for(i=1;i<NF;i++) mods=mods" "$i; key=$NF; print mods", "key", exec, ai-rclick"}' | \
    sed 's/^ //')
  mkdir -p "$(dirname "$hcfg")"
  if [[ ! -f "$hcfg" ]] || ! grep -q "ai-rclick" "$hcfg"; then
    printf '\n# Ask AI shortcut (ai-cli v2.4.6)\nbind = %s\n' "$keybind_hypr" >> "$hcfg"
    ok "Hyprland: Added 'bind = $keybind_hypr' in $hcfg"
  else
    ok "Hyprland: ai-rclick keybind already in $hcfg"
  fi
  info "Reload: hyprctl reload"
}

_rclick_install_openbox() {
  info "Installing Openbox keybinding..."
  local rc="${XDG_CONFIG_HOME:-$HOME/.config}/openbox/rc.xml"
  if [[ -f "$rc" ]] && ! grep -q "ai-rclick" "$rc"; then
    local keybind_ob; keybind_ob=$(echo "$RCLICK_KEYBIND" | sed 's/Super/W/Ig;s/\+/-/g')
    sed -i "s|</keyboard>|  <keybind key=\"${keybind_ob}\">\n      <action name=\"Execute\"><command>ai-rclick</command></action>\n    </keybind>\n  </keyboard>|" \
      "$rc" 2>/dev/null || true
    ok "Openbox: Added keybind $RCLICK_KEYBIND → ai-rclick"
    info "Reload: openbox --reconfigure"
  else
    echo "  Add to $rc <keyboard> section:"
    printf '    <keybind key="%s">\n      <action name="Execute"><command>ai-rclick</command></action>\n    </keybind>\n' \
      "$(echo "$RCLICK_KEYBIND" | sed 's/Super/W/Ig;s/\+/-/g')"
  fi
}

_rclick_install_xbindkeys() {
  # Universal X11 fallback using xbindkeys
  if ! command -v xbindkeys &>/dev/null; then
    info "xbindkeys not installed. For universal X11 keybind support:"
    info "  apt install xbindkeys   OR   pacman -S xbindkeys"
    return
  fi
  local cfg="$HOME/.xbindkeysrc"
  local keybind_xbk
  keybind_xbk=$(echo "$RCLICK_KEYBIND" | \
    sed 's/Super/Mod4/Ig;s/Ctrl/Control/Ig' | \
    awk -F'+' 'BEGIN{OFS="+"} {key=$NF; printf "\"%s\"\n  ", $0}')
  if [[ ! -f "$cfg" ]] || ! grep -q "ai-rclick" "$cfg"; then
    {
      echo ""
      echo "# Ask AI (ai-cli v2.4.6)"
      echo '"ai-rclick"'
      echo "  $(echo "$RCLICK_KEYBIND" | sed 's/Super/Mod4/Ig;s/Ctrl/Control/Ig;s/+/ + /g')"
    } >> "$cfg"
    ok "xbindkeys: Added $RCLICK_KEYBIND → ai-rclick in $cfg"
    info "Reload: pkill xbindkeys; xbindkeys &"
  fi
}

cmd_rclick() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    install)
      _rclick_install_deps
      _rclick_write_script

      # Auto-detect ALL present DEs/WMs and install for each
      local de="${XDG_CURRENT_DESKTOP:-}"
      local installed=()

      # GNOME / Cinnamon / Unity / Budgie (all use gsettings)
      if [[ -n "${GNOME_DESKTOP_SESSION_ID:-}" ]] \
         || [[ "$de" =~ (GNOME|Unity|Budgie|Cinnamon) ]]; then
        _rclick_install_gnome && installed+=("GNOME/Cinnamon")
      fi
      # KDE Plasma 5 / 6
      if [[ "$de" =~ (KDE) ]] || command -v plasmashell &>/dev/null; then
        _rclick_install_kde && installed+=("KDE")
      fi
      # XFCE
      if [[ "$de" =~ (XFCE|Xfce) ]] || command -v xfce4-session &>/dev/null; then
        _rclick_install_xfce && installed+=("XFCE")
      fi
      # MATE
      if [[ "$de" =~ (MATE) ]] || command -v mate-session &>/dev/null; then
        _rclick_install_mate && installed+=("MATE")
      fi
      # LXDE / LXQt
      if [[ "$de" =~ (LXDE|LXQt) ]] || command -v lxsession &>/dev/null; then
        _rclick_install_lxde && installed+=("LXDE")
      fi
      # Hyprland
      if [[ -n "${HYPRLAND_INSTANCE_SIGNATURE:-}" ]] || command -v hyprctl &>/dev/null; then
        _rclick_install_hyprland && installed+=("Hyprland")
      fi
      # sway
      if [[ -n "${SWAYSOCK:-}" ]] || command -v sway &>/dev/null; then
        _rclick_install_sway_i3 && installed+=("sway")
      fi
      # i3
      if [[ "$de" =~ (i3) ]] || command -v i3 &>/dev/null && [[ ! " ${installed[*]} " =~ "sway" ]]; then
        _rclick_install_sway_i3 && installed+=("i3")
      fi
      # Openbox (standalone)
      if command -v openbox &>/dev/null && [[ ! " ${installed[*]} " =~ "LXDE" ]]; then
        _rclick_install_openbox && installed+=("Openbox")
      fi

      # If nothing matched or as extra fallback, install xbindkeys
      if [[ ${#installed[@]} -eq 0 ]]; then
        warn "Could not detect DE/WM — installing all compatible integrations..."
        _rclick_install_gnome  2>/dev/null || true
        _rclick_install_kde    2>/dev/null || true
        _rclick_install_xfce   2>/dev/null || true
        _rclick_install_openbox 2>/dev/null || true
        _rclick_install_xbindkeys 2>/dev/null || true
      fi
      # Always offer xbindkeys as a universal fallback
      [[ ! " ${installed[*]} " =~ "xbindkeys" ]] && \
        command -v xbindkeys &>/dev/null && _rclick_install_xbindkeys 2>/dev/null || true

      RCLICK_ENABLED="1"; save_config
      echo ""
      ok "Right-click AI installed!"
      [[ ${#installed[@]} -gt 0 ]] && info "  DEs configured: ${installed[*]}"
      info "  Shortcut: $RCLICK_KEYBIND   (select text first, then press)"
      info "  Or right-click in file manager → Ask AI"
      info "  Change shortcut: ai rclick keybind <combo>  e.g. Ctrl+Shift+a"
      ;;

    keybind)
      local kb="${1:-}"
      if [[ -z "$kb" ]]; then
        echo "  Current keybind: ${RCLICK_KEYBIND}"
        echo ""
        echo "  Usage: ai rclick keybind <combo>"
        echo "  Examples:"
        echo "    ai rclick keybind Super+Shift+a    (default)"
        echo "    ai rclick keybind Ctrl+Shift+a"
        echo "    ai rclick keybind Super+Alt+a"
        echo "    ai rclick keybind F12"
        echo ""
        echo "  After changing: run 'ai rclick install' to apply"
        return
      fi
      RCLICK_KEYBIND="$kb"; save_config
      ok "Keybind set: $kb"
      info "Run 'ai rclick install' to apply to your DE/WM"
      ;;

    uninstall)
      sudo rm -f /usr/local/bin/ai-rclick
      rm -f "$HOME/.local/share/nautilus/scripts/Ask AI"
      rm -f "$HOME/.local/share/nemo/scripts/Ask AI"
      rm -f "$HOME/.local/share/kservices5/ServiceMenus/ai-rclick.desktop"
      rm -f "$HOME/.local/share/kio/servicemenus/ai-rclick.desktop"
      rm -f "$HOME/.config/caja/scripts/Ask AI"
      # Remove GNOME shortcut
      gsettings set org.gnome.settings-daemon.plugins.media-keys custom-keybindings \
        "$(gsettings get org.gnome.settings-daemon.plugins.media-keys custom-keybindings 2>/dev/null | \
           sed "s|, *'/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/ai-rclick/'||g" | \
           sed "s|'/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/ai-rclick/', *||g"
        )" 2>/dev/null || true
      RCLICK_ENABLED="0"; save_config
      ok "Right-click AI uninstalled"
      ;;

    model)
      local m="${1:-}"
      if [[ -z "$m" ]]; then
        hdr "VL Model Options for Right-Click"
        for k in "${!RCLICK_VL_MODELS[@]}"; do
          IFS='|' read -r repo _ desc <<< "${RCLICK_VL_MODELS[$k]}"
          printf "  ${B}%-16s${R} %s\n" "$k:" "$desc"
        done
        read -rp "Choose [qwen3vl/lfm25vl/lfm25vl_gguf/custom]: " m
      fi
      [[ -z "${RCLICK_VL_MODELS[$m]:-}" ]] && { err "Unknown: $m"; return 1; }
      RCLICK_VL_MODEL="$m"; save_config
      ok "VL model set: $m"
      info "Run 'ai rclick install' to reinstall script with new model"
      ;;

    download-model) _rclick_download_vl ;;

    test)
      info "Testing right-click AI..."
      local text="Right-click AI test from ai-cli v2.4.6. If you see this, it works!"
      command -v wl-copy  &>/dev/null && echo "$text" | wl-copy 2>/dev/null || true
      command -v xclip    &>/dev/null && echo "$text" | xclip -selection clipboard 2>/dev/null || true
      command -v xsel     &>/dev/null && echo "$text" | xsel --clipboard --input 2>/dev/null || true
      if command -v ai-rclick &>/dev/null; then
        AI_RCLICK_SKIP_QUESTION=1 ai-rclick 2>/dev/null || bash /usr/local/bin/ai-rclick
      else
        err "ai-rclick not installed. Run: ai rclick install"
        return 1
      fi
      ;;

    status)
      hdr "Right-Click AI Status (v2.4.6)"
      local script_loc; script_loc=$(command -v ai-rclick 2>/dev/null || echo 'NOT INSTALLED')
      local disp="X11"
      [[ -n "${WAYLAND_DISPLAY:-}${SWAYSOCK:-}${HYPRLAND_INSTANCE_SIGNATURE:-}" ]] && disp="Wayland"
      printf "  %-22s %s\n" "Enabled:"       "$RCLICK_ENABLED"
      printf "  %-22s %s\n" "VL model:"      "${RCLICK_VL_MODEL:-not set}"
      printf "  %-22s %s\n" "Keybind:"       "$RCLICK_KEYBIND"
      printf "  %-22s %s\n" "Script:"        "$script_loc"
      printf "  %-22s %s\n" "DE/WM:"         "${XDG_CURRENT_DESKTOP:-unknown}"
      printf "  %-22s %s\n" "Display server:" "$disp"
      printf "  %-22s %s\n" "Clipboard tools:" \
        "$(for t in xclip xsel wl-paste wl-copy; do command -v $t &>/dev/null && echo -n "$t "; done; echo)"
      ;;

    *)
      hdr "Right-Click AI Context Menu (v2.4.6)"
      echo ""
      echo "  ${B}ai rclick install${R}             — Install for all detected DEs/WMs"
      echo "  ${B}ai rclick keybind <combo>${R}      — Change keyboard shortcut"
      echo "  ${B}ai rclick uninstall${R}           — Remove all integrations"
      echo "  ${B}ai rclick model <name>${R}         — Set VL model"
      echo "  ${B}ai rclick download-model${R}       — Download VL model"
      echo "  ${B}ai rclick test${R}                 — Test with clipboard content"
      echo "  ${B}ai rclick status${R}               — Show full status"
      echo ""
      echo "  ${B}Supported DEs/WMs (all auto-detected):${R}"
      echo "    GNOME · KDE Plasma 5+6 · XFCE · MATE · Cinnamon"
      echo "    Openbox · LXDE/LXQt · i3 · sway · Hyprland"
      echo "    X11 universal: xbindkeys fallback"
      echo ""
      echo "  ${B}Default keybind:${R} $RCLICK_KEYBIND  (select text first)"
      echo "  Change:  ai rclick keybind Ctrl+Shift+a"
      echo ""
      echo "  ${B}VL models:${R}"
      for k in "${!RCLICK_VL_MODELS[@]}"; do
        IFS='|' read -r _ _ desc <<< "${RCLICK_VL_MODELS[$k]}"
        printf "    ${B}%-16s${R} %s\n" "$k" "$desc"
      done
      ;;
  esac
}


# ════════════════════════════════════════════════════════════════════════════════
#  AUTO-UPDATER — checks github.com/minerofthesoal/ai-cli for new releases
# ════════════════════════════════════════════════════════════════════════════════

_aup_get_latest() {
  # Returns "TAG|URL" of latest release from GitHub
  local api_url="https://api.github.com/repos/${AUP_REPO}/releases/latest"
  local info
  info=$(curl -sS --max-time 10 --retry 3 \
    -H "Accept: application/vnd.github.v3+json" \
    "$api_url" 2>/dev/null) || { echo ""; return 1; }
  local tag download_url
  tag=$(echo "$info" | python3 -c "import json,sys; d=json.load(sys.stdin); print(d.get('tag_name',''))" 2>/dev/null)
  download_url=$(echo "$info" | python3 -c "
import json,sys
d=json.load(sys.stdin)
assets=d.get('assets',[])
for a in assets:
    n=a.get('name','')
    if n.endswith('.sh') or n=='ai.sh' or n=='ai':
        print(a.get('browser_download_url','')); break
else:
    # Fallback: try raw download from tag
    tag=d.get('tag_name','')
    repo='${AUP_REPO}'
    print(f'https://raw.githubusercontent.com/{repo}/{tag}/ai.sh')
" 2>/dev/null)
  echo "${tag}|${download_url}"
}

_aup_compare_versions() {
  # Returns 1 if $1 > $2 (remote > current)
  local remote="$1" current="$2"
  python3 - <<PYEOF 2>/dev/null
import sys
def parse(v):
    v = v.lstrip('v')
    parts = v.split('.')
    try: return [int(x) for x in parts]
    except: return [0,0,0]
remote  = parse('$remote')
current = parse('$current')
sys.exit(0 if remote > current else 1)
PYEOF
}

_aup_do_update() {
  local tag="$1" url="$2"
  local tmp; tmp=$(mktemp /tmp/ai_update_XXXX.sh)
  info "Downloading update $tag from $url ..."
  curl -sS -L --retry 5 --progress-bar "$url" -o "$tmp" 2>/dev/null || {
    rm -f "$tmp"; err "Download failed"; return 1
  }
  # Validate
  bash -n "$tmp" 2>/dev/null || { rm -f "$tmp"; err "Downloaded script has syntax errors"; return 1; }
  local target
  target=$(command -v ai 2>/dev/null || echo "/usr/local/bin/ai")
  if [[ -w "$target" ]]; then
    cp "$tmp" "$target" && chmod +x "$target"
  else
    sudo cp "$tmp" "$target" && sudo chmod +x "$target"
  fi
  rm -f "$tmp"
  AUP_LAST_CHECK=$(date +%s); save_config
  ok "Updated to $tag! Restart ai to use new version."
  info "Run 'ai help' to see what's new"
}

# Model persistence: save/restore active model across updates
_model_save_state() {
  local state_file="$CONFIG_DIR/.model_state"
  cat > "$state_file" <<MSTATE
SAVED_MODEL="${ACTIVE_MODEL}"
SAVED_BACKEND="${ACTIVE_BACKEND}"
SAVED_SESSION="${ACTIVE_SESSION}"
MSTATE
}

_model_restore_state() {
  local state_file="$CONFIG_DIR/.model_state"
  [[ ! -f "$state_file" ]] && return
  source "$state_file" 2>/dev/null || return
  if [[ -n "${SAVED_MODEL:-}" && ( -f "$SAVED_MODEL" || -d "$SAVED_MODEL" ) ]]; then
    ACTIVE_MODEL="$SAVED_MODEL"
    ACTIVE_BACKEND="${SAVED_BACKEND:-}"
    ACTIVE_SESSION="${SAVED_SESSION:-default}"
    save_config
    ok "Model restored: $ACTIVE_MODEL"
  fi
  rm -f "$state_file"
}

cmd_autoupdate() {
  local force=0 check_only=0
  for a in "$@"; do
    [[ "$a" == "--force"      ]] && force=1
    [[ "$a" == "--check-only" ]] && check_only=1
  done

  # Check interval (default 1 hour)
  local now; now=$(date +%s)
  local last="${AUP_LAST_CHECK:-0}"
  if (( force == 0 && now - last < AUP_CHECK_INTERVAL )); then
    dim "Update check: next in $(( (AUP_CHECK_INTERVAL - (now - last)) / 60 )) min"
    return 0
  fi

  info "Checking for updates (${AUP_REPO})..."
  local info_str; info_str=$(_aup_get_latest)
  [[ -z "$info_str" ]] && { warn "Could not reach GitHub (offline?)"; return 1; }

  IFS='|' read -r remote_tag download_url <<< "$info_str"
  [[ -z "$remote_tag" ]] && { warn "Could not parse release info"; return 1; }

  printf "  Current:  %s\n  Latest:   %s\n" "v$VERSION" "$remote_tag"

  if _aup_compare_versions "$remote_tag" "$VERSION"; then
    ok "New release: $remote_tag"
    [[ $check_only -eq 1 ]] && return 0
    read -rp "Update now? [Y/n]: " ans
    [[ "${ans,,}" == "n" ]] && return 0
    _model_save_state
    _aup_do_update "$remote_tag" "$download_url"
    _model_restore_state
  else
    ok "Already up to date ($VERSION)"
    AUP_LAST_CHECK=$now; save_config
  fi
}

# Background silent update check (runs at startup when -aup flag set)
_aup_bg_check() {
  local now; now=$(date +%s)
  local last="${AUP_LAST_CHECK:-0}"
  (( now - last < AUP_CHECK_INTERVAL )) && return

  {
    local info_str; info_str=$(_aup_get_latest 2>/dev/null)
    [[ -z "$info_str" ]] && exit 0
    IFS='|' read -r remote_tag _ <<< "$info_str"
    if _aup_compare_versions "$remote_tag" "$VERSION" 2>/dev/null; then
      echo ""
      echo -e "${BYELLOW}⬆  Update available: ${B}$remote_tag${R}${BYELLOW} (current: $VERSION)${R}"
      echo -e "   Run ${B}ai -aup${R} to update"
    fi
    AUP_LAST_CHECK=$now; save_config
  } &>/dev/null &
  disown
}

# ════════════════════════════════════════════════════════════════════════════════
#  AGENT MODE — multi-step agentic task execution with web search
# ════════════════════════════════════════════════════════════════════════════════

declare -A AGENT_TOOLS_REGISTRY
AGENT_TOOLS_REGISTRY=(
  [web_search]="Search the web for current information. Args: {query: string}"
  [read_url]="Read the content of a URL. Args: {url: string}"
  [write_file]="Write content to a file. Args: {path: string, content: string}"
  [read_file]="Read a file's content. Args: {path: string}"
  [run_code]="Execute Python code. Args: {code: string}"
  [run_bash]="Execute a bash command. Args: {command: string}"
  [ask_user]="Ask the user a clarifying question. Args: {question: string}"
  [calculate]="Evaluate a math expression. Args: {expression: string}"
)

_agent_web_search() {
  local query="$1"
  # Try multiple search backends (no rate limiting)
  local results=""

  # DDG
  if [[ "${AGENT_SEARCH_ENGINE:-ddg}" == "ddg" ]] || true; then
    results=$(curl -sS --max-time 10 --retry 3 \
      -H "User-Agent: Mozilla/5.0 (compatible; ai-cli)" \
      "https://api.duckduckgo.com/?q=$(python3 -c "import urllib.parse; print(urllib.parse.quote('$query'))" 2>/dev/null || echo "${query// /+}")&format=json&no_redirect=1" \
      2>/dev/null | python3 -c "
import json,sys
try:
    d=json.load(sys.stdin)
    items=[]
    if d.get('AbstractText'): items.append({'title':'Summary','snippet':d['AbstractText'],'url':d.get('AbstractURL','')})
    for r in d.get('RelatedTopics',[])[:6]:
        if isinstance(r,dict) and r.get('Text'):
            items.append({'title':r.get('Text','')[:60],'snippet':r.get('Text',''),'url':r.get('FirstURL','')})
    print(json.dumps(items[:5]))
except: print('[]')
" 2>/dev/null)
  fi

  # Brave Search API if key available
  if [[ -n "${BRAVE_API_KEY:-}" ]] && [[ -z "$results" || "$results" == "[]" ]]; then
    results=$(curl -sS --max-time 10 --retry 3 \
      -H "Accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "X-Subscription-Token: $BRAVE_API_KEY" \
      "https://api.search.brave.com/res/v1/web/search?q=$(python3 -c "import urllib.parse; print(urllib.parse.quote('$query'))")" \
      2>/dev/null | python3 -c "
import json,sys
try:
    d=json.load(sys.stdin)
    items=[{'title':r.get('title',''),'snippet':r.get('description',''),'url':r.get('url','')}
           for r in d.get('web',{}).get('results',[])]
    print(json.dumps(items[:6]))
except: print('[]')
" 2>/dev/null)
  fi

  echo "${results:-[]}"
}

_agent_read_url() {
  local url="$1"
  curl -sS --max-time 15 --retry 2 \
    -H "User-Agent: Mozilla/5.0" \
    -L "$url" 2>/dev/null | python3 -c "
import sys,re
html=sys.stdin.read()
# Strip HTML tags
text=re.sub(r'<script[^>]*>.*?</script>','',html,flags=re.DOTALL|re.IGNORECASE)
text=re.sub(r'<style[^>]*>.*?</style>','',text,flags=re.DOTALL|re.IGNORECASE)
text=re.sub(r'<[^>]+>','',text)
text=re.sub(r'\s+',' ',text).strip()
print(text[:3000])
" 2>/dev/null
}

_agent_run_code() {
  local code="$1"
  echo "$code" | python3 2>&1 | head -50
}

_agent_execute_step() {
  local tool="$1" args="$2"
  case "$tool" in
    web_search)
      local q; q=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('query',''))" 2>/dev/null)
      _agent_web_search "$q"
      ;;
    read_url)
      local url; url=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('url',''))" 2>/dev/null)
      _agent_read_url "$url"
      ;;
    write_file)
      local path content
      path=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('path',''))" 2>/dev/null)
      content=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('content',''))" 2>/dev/null)
      echo "$content" > "$path" && echo "Written: $path"
      ;;
    read_file)
      local p; p=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('path',''))" 2>/dev/null)
      cat "$p" 2>/dev/null || echo "File not found: $p"
      ;;
    run_code)
      local code; code=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('code',''))" 2>/dev/null)
      _agent_run_code "$code"
      ;;
    run_bash)
      local cmd; cmd=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('command',''))" 2>/dev/null)
      eval "$cmd" 2>&1 | head -50
      ;;
    ask_user)
      local q; q=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('question',''))" 2>/dev/null)
      read -rp "$q: " ans; echo "$ans"
      ;;
    calculate)
      local expr; expr=$(echo "$args" | python3 -c "import json,sys; print(json.load(sys.stdin).get('expression',''))" 2>/dev/null)
      python3 -c "print(eval('$expr'))" 2>/dev/null
      ;;
    *)
      echo "Unknown tool: $tool"
      ;;
  esac
}

cmd_agent() {
  local task="$*"
  [[ -z "$task" ]] && { read -rp "Task: " task; }
  [[ -z "$task" ]] && return

  AGENT_MODE=1
  local max_steps="${AGENT_MAX_STEPS:-10}"
  local step=0
  local history=()
  local done=0

  hdr "🤖 Agent Mode — Task: $task"
  echo ""

  # Build tools list for the model
  local tools_desc
  tools_desc=$(for t in "${!AGENT_TOOLS_REGISTRY[@]}"; do
    echo "  - $t: ${AGENT_TOOLS_REGISTRY[$t]}"; done)

  local system_prompt="You are an autonomous AI agent. Break complex tasks into steps using available tools.
Available tools:
$tools_desc

Respond in this exact JSON format when using a tool:
{\"thought\": \"reasoning\", \"tool\": \"tool_name\", \"args\": {\"key\": \"value\"}}

Or when done:
{\"thought\": \"reasoning\", \"done\": true, \"answer\": \"final answer\"}

Be systematic. Use web_search for current information. Never make up facts."

  local context="Task: $task"

  while (( step < max_steps && done == 0 )); do
    (( step++ ))
    printf "\n${B}${BCYAN}Step %d/%d${R}\n" "$step" "$max_steps"

    # Get next action from AI
    local response
    response=$(AI_SYSTEM_OVERRIDE="$system_prompt" dispatch_ask "$context" 2>/dev/null)
    echo -e "${DIM}$response${R}"

    # Parse JSON response
    local thought tool args final_answer is_done
    thought=$(echo "$response" | python3 -c "import json,sys; d=json.loads(sys.stdin.read().strip()); print(d.get('thought',''))" 2>/dev/null)
    tool=$(echo "$response" | python3 -c "import json,sys; d=json.loads(sys.stdin.read().strip()); print(d.get('tool',''))" 2>/dev/null)
    args=$(echo "$response" | python3 -c "import json,sys; d=json.loads(sys.stdin.read().strip()); print(json.dumps(d.get('args',{})))" 2>/dev/null || echo "{}")
    is_done=$(echo "$response" | python3 -c "import json,sys; d=json.loads(sys.stdin.read().strip()); print(d.get('done','false'))" 2>/dev/null)
    final_answer=$(echo "$response" | python3 -c "import json,sys; d=json.loads(sys.stdin.read().strip()); print(d.get('answer',''))" 2>/dev/null)

    [[ -n "$thought" ]] && echo -e "${DIM}  💭 $thought${R}"

    if [[ "$is_done" == "True" || "$is_done" == "true" || -n "$final_answer" ]]; then
      done=1
      echo ""
      hdr "✅ Agent Complete"
      echo -e "${BWHITE}$final_answer${R}"
      break
    fi

    if [[ -n "$tool" && "$tool" != "None" && "$tool" != "null" ]]; then
      echo -e "  ${BBLUE}🔧 Tool: $tool${R}"
      local tool_result
      tool_result=$(_agent_execute_step "$tool" "$args")
      echo -e "  ${DIM}Result: ${tool_result:0:500}${R}"
      context="${context}\n\nStep $step — Used: $tool\nResult: ${tool_result:0:1000}"
    else
      # Model gave a plain response, not JSON — treat as final answer
      done=1
      hdr "✅ Agent Response"
      echo "$response"
    fi
  done

  if (( done == 0 )); then
    warn "Max steps ($max_steps) reached. Use 'ai config agent_max_steps N' to increase."
  fi
  AGENT_MODE=0
}

# ════════════════════════════════════════════════════════════════════════════════
#  ENHANCED WEB SEARCH (no rate limiting, multiple backends)
# ════════════════════════════════════════════════════════════════════════════════
cmd_websearch() {
  local query="$*"; local backend="${SEARCH_ENGINE:-ddg}"
  [[ -z "$query" ]] && { read -rp "Search: " query; }
  [[ -z "$query" ]] && return

  hdr "🌐 Web Search: $query"
  echo ""

  local results_json
  results_json=$(_agent_web_search "$query")

  # Display results
  echo "$results_json" | python3 - "$query" <<'PYEOF'
import json, sys
try:
    results = json.loads(sys.stdin.read())
    if not results:
        print("  No results found")
        sys.exit(0)
    for i, r in enumerate(results, 1):
        print(f"  {i}. {r.get('title','')[:70]}")
        snippet = r.get('snippet','')
        if snippet and snippet != r.get('title',''):
            print(f"     {snippet[:120]}")
        url = r.get('url','')
        if url: print(f"     {url[:80]}")
        print()
except Exception as e:
    print(f"Parse error: {e}")
PYEOF

  echo ""
  # Use AI to summarize if model available
  if [[ -n "$ACTIVE_MODEL" || -n "${OPENAI_API_KEY:-}" || -n "${ANTHROPIC_API_KEY:-}" || -n "${GEMINI_API_KEY:-}" ]]; then
    local context
    context=$(echo "$results_json" | python3 -c "
import json,sys
results=json.load(sys.stdin)
lines=[]
for r in results:
    lines.append(f\"Title: {r.get('title','')}\nSnippet: {r.get('snippet','')}\nURL: {r.get('url','')}\")
print('\n'.join(lines))")
    hdr "AI Summary"
    dispatch_ask "Based on these search results, answer: $query

$context

Be factual, cite the sources." 2>/dev/null
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
#  MODEL LOADING PERSISTENCE (save/restore between install/update)
# ════════════════════════════════════════════════════════════════════════════════
cmd_model_save_restore() {
  local sub="${1:-status}"
  case "$sub" in
    save)
      _model_save_state
      ok "Model state saved: $ACTIVE_MODEL ($ACTIVE_BACKEND)"
      ;;
    restore)
      _model_restore_state
      ;;
    status)
      local state_file="$CONFIG_DIR/.model_state"
      if [[ -f "$state_file" ]]; then
        info "Saved state:"
        cat "$state_file"
      else
        info "No saved state (current: $ACTIVE_MODEL)"
      fi
      ;;
  esac
}

cmd_model_create() {
  local subcmd="${1:-help}"; shift || true
  case "$subcmd" in
    presets) _model_list_presets ;;
    new)     _model_new "$@" ;;
    edit)    _model_edit "$@" ;;
    list)    _model_list_custom ;;
    train)   _model_train_custom "$@" ;;
    info)    _model_info_custom "$@" ;;
    delete)  _model_delete_custom "$@" ;;
    *)
      echo -e "${B}${BCYAN}Custom Model Creator${R}"
      echo ""
      echo "  ${B}ai model-create presets${R}         — List built-in architecture presets"
      echo "  ${B}ai model-create new <name> [preset|custom]${R} — Create new model config"
      echo "  ${B}ai model-create edit <name>${R}     — Edit model config JSON"
      echo "  ${B}ai model-create list${R}            — List custom models"
      echo "  ${B}ai model-create train <name> [data.jsonl]${R} — Train from scratch"
      echo "  ${B}ai model-create info <name>${R}     — Show model info"
      echo "  ${B}ai model-create delete <name>${R}   — Delete custom model"
      echo ""
      echo -e "  ${DIM}Minimum: 0.125B params (nano preset)${R}"
      ;;
  esac
}

_model_list_presets() {
  hdr "Built-in Model Presets"
  echo ""
  for key in nano micro tiny small medium tinyllama; do
    local val="${MODEL_PRESETS[$key]}"
    local params; params=$(echo "$val" | grep -o 'params=[^|]*' | cut -d= -f2)
    printf "  ${B}%-12s${R} %s\n" "$key" "$params"
  done
  echo ""
  echo "  Use: ${B}ai model-create new mymodel nano${R}"
  echo "  Or:  ${B}ai model-create new mymodel custom${R} (opens editor)"
}

_model_new() {
  local name="${1:-}"; local preset="${2:-tiny}"
  [[ -z "$name" ]] && { read -rp "Model name: " name; }
  [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ -d "$mdir" ]] && { err "Model '$name' already exists"; return 1; }
  mkdir -p "$mdir"

  local config
  if [[ "$preset" == "custom" ]]; then
    config="$TTM_CONFIG_JSON"
    echo "$config" > "$mdir/config.json"
    "${EDITOR:-nano}" "$mdir/config.json"
  elif [[ -n "${MODEL_PRESETS[$preset]:-}" ]]; then
    local p="${MODEL_PRESETS[$preset]}"
    local hs; hs=$(echo "$p" | grep -o 'hidden_size=[0-9]*' | cut -d= -f2)
    local nhl; nhl=$(echo "$p" | grep -o 'num_hidden_layers=[0-9]*' | cut -d= -f2)
    local nah; nah=$(echo "$p" | grep -o 'num_attention_heads=[0-9]*' | cut -d= -f2)
    local is; is=$(echo "$p" | grep -o 'intermediate_size=[0-9]*' | cut -d= -f2)
    local mpe; mpe=$(echo "$p" | grep -o 'max_position_embeddings=[0-9]*' | cut -d= -f2)
    local vs; vs=$(echo "$p" | grep -o 'vocab_size=[0-9]*' | cut -d= -f2)
    cat > "$mdir/config.json" <<JSON
{
  "architectures": ["LlamaForCausalLM"],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": $hs,
  "initializer_range": 0.02,
  "intermediate_size": $is,
  "max_position_embeddings": $mpe,
  "model_type": "llama",
  "num_attention_heads": $nah,
  "num_hidden_layers": $nhl,
  "num_key_value_heads": $nah,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "use_cache": true,
  "vocab_size": $vs
}
JSON
  elif [[ -f "$preset" ]]; then
    cp "$preset" "$mdir/config.json"
  else
    err "Unknown preset: $preset. Use: nano micro tiny small medium tinyllama custom or a path to JSON"
    rm -rf "$mdir"; return 1
  fi

  cat > "$mdir/meta.json" <<META
{
  "name": "$name",
  "preset": "$preset",
  "created": "$(date -Iseconds)",
  "trained": false,
  "train_steps": 0,
  "version": 1
}
META
  ok "Created custom model '$name' in $mdir"
  echo "  Config: $mdir/config.json"
  echo "  Train:  ai model-create train $name <data.jsonl>"
}

_model_edit() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found"; return 1; }
  "${EDITOR:-nano}" "$mdir/config.json"
  ok "Saved '$name' config"
}

_model_list_custom() {
  hdr "Custom Models"
  local found=0
  for d in "$CUSTOM_MODELS_DIR"/*/; do
    [[ -f "$d/meta.json" ]] || continue
    found=1
    local name; name=$(basename "$d")
    local trained; trained=$(python3 -c "import json,sys; d=json.load(open('$d/meta.json')); print('yes' if d.get('trained') else 'no')" 2>/dev/null || echo "?")
    local steps; steps=$(python3 -c "import json,sys; d=json.load(open('$d/meta.json')); print(d.get('train_steps',0))" 2>/dev/null || echo "0")
    printf "  ${B}%-20s${R} trained=%-4s steps=%s\n" "$name" "$trained" "$steps"
  done
  [[ $found -eq 0 ]] && dim "  No custom models. Create one: ai model-create new mymodel tiny"
}

_model_info_custom() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found"; return 1; }
  hdr "Model: $name"
  [[ -f "$mdir/config.json" ]] && { echo ""; echo "Config:"; cat "$mdir/config.json"; }
  [[ -f "$mdir/meta.json"   ]] && { echo ""; echo "Meta:";   cat "$mdir/meta.json";   }
}

_model_delete_custom() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found"; return 1; }
  read -rp "Delete '$name'? This cannot be undone. [y/N]: " ans
  [[ "$ans" =~ ^[Yy]$ ]] || { info "Cancelled"; return 0; }
  rm -rf "$mdir"; ok "Deleted '$name'"
}

_model_train_custom() {
  local name="${1:-}"; local data="${2:-}"
  [[ -z "$name" ]] && { err "Name required"; return 1; }
  local mdir="$CUSTOM_MODELS_DIR/$name"
  [[ ! -d "$mdir" ]] && { err "Model '$name' not found. Create it first: ai model-create new $name"; return 1; }
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }

  # Use provided dataset or look for default
  if [[ -z "$data" ]]; then
    [[ -f "$FINETUNE_DIR/dataset.jsonl" ]] && data="$FINETUNE_DIR/dataset.jsonl"
    [[ -z "$data" ]] && { err "No dataset. Provide path or run: ai finetune prepare <data>"; return 1; }
  fi
  [[ ! -f "$data" ]] && { err "Dataset not found: $data"; return 1; }

  local out_dir="$mdir/trained_$(date +%Y%m%d_%H%M%S)"
  mkdir -p "$out_dir"
  info "Training custom model '$name' from scratch..."
  info "Config: $mdir/config.json"
  info "Data:   $data"
  info "Output: $out_dir"
  echo ""

  "$PYTHON" - <<PYEOF
import json, os, sys
try:
    import torch
    from transformers import (AutoTokenizer, LlamaConfig, LlamaForCausalLM,
                               TrainingArguments, Trainer, DataCollatorForLanguageModeling)
    from datasets import Dataset
except ImportError as e:
    print(f"Missing dependency: {e}")
    print("Run: ai install-deps")
    sys.exit(1)

config_path = "$mdir/config.json"
data_path   = "$data"
out_dir     = "$out_dir"

with open(config_path) as f:
    cfg_dict = json.load(f)

cfg = LlamaConfig(**{k: v for k, v in cfg_dict.items()
                     if not k.startswith('_') and k != 'architectures'})
model = LlamaForCausalLM(cfg)
total_params = sum(p.numel() for p in model.parameters())
print(f"Model parameters: {total_params:,} ({total_params/1e6:.2f}M)")

if total_params < 0.125e9:
    print(f"WARNING: Model has {total_params/1e6:.2f}M params, minimum is 125M (0.125B)")
    sys.exit(1)

# Tokenizer — use TinyLlama's tokenizer as base
tokenizer_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
try:
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b", use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

# Load dataset
records = []
with open(data_path) as f:
    for line in f:
        line = line.strip()
        if not line: continue
        try:
            obj = json.loads(line)
            txt = obj.get('text') or (obj.get('instruction','') + ' ' + obj.get('output',''))
            if txt.strip(): records.append({'text': txt})
        except: pass

if not records:
    print("No valid records in dataset"); sys.exit(1)
print(f"Dataset records: {len(records)}")

ds = Dataset.from_list(records)
def tokenize(ex):
    return tokenizer(ex['text'], truncation=True, max_length=cfg.max_position_embeddings,
                     padding='max_length')
ds = ds.map(tokenize, batched=True, remove_columns=['text'])

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
print(f"Training on: {device}")

args = TrainingArguments(
    output_dir=out_dir,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=3e-4,
    lr_scheduler_type='cosine',
    warmup_ratio=0.05,
    fp16=(device=='cuda'),
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    report_to='none',
)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"Saved to {out_dir}")
PYEOF

  if [[ $? -eq 0 ]]; then
    # Update meta
    "$PYTHON" -c "
import json
m = json.load(open('$mdir/meta.json'))
m['trained'] = True
m['train_steps'] = m.get('train_steps',0) + 1
m['last_trained'] = '$(date -Iseconds)'
m['last_output'] = '$out_dir'
json.dump(m, open('$mdir/meta.json','w'), indent=2)
"
    ok "Training complete! Model saved to $out_dir"
  else
    err "Training failed"
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
# ════════════════════════════════════════════════════════════════════════════════
#  NAMED CHAT (-C) WITH JSONL SAVE + HF DATASET SYNC
# ════════════════════════════════════════════════════════════════════════════════
CURRENT_CHAT_NAME=""
CURRENT_CHAT_FILE=""

_chat_start() {
  local name="$1"
  # auto: generate a name based on timestamp
  if [[ "$name" == "auto" ]]; then
    name="chat_$(date +%Y%m%d_%H%M%S)"
  fi
  # Sanitize name
  name="${name//[^a-zA-Z0-9_-]/_}"
  CURRENT_CHAT_NAME="$name"
  CURRENT_CHAT_FILE="$CHAT_LOGS_DIR/${name}.jsonl"
  ok "Chat started: $name"
  echo "  Saving to: $CURRENT_CHAT_FILE"
  [[ "$HF_DATASET_SYNC" == "1" ]] && echo "  HF sync:   enabled → $HF_DATASET_REPO"
}

_chat_append() {
  local role="$1"; local content="$2"
  [[ -z "$CURRENT_CHAT_FILE" ]] && return 0
  local ts; ts=$(date -Iseconds)
  local record; record=$(printf '{"timestamp":"%s","session":"%s","role":"%s","content":%s}' \
    "$ts" "$CURRENT_CHAT_NAME" "$role" "$(echo "$content" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read()))' 2>/dev/null || echo '""')")
  echo "$record" >> "$CURRENT_CHAT_FILE"

  # Background sync to HF if enabled
  if [[ "$HF_DATASET_SYNC" == "1" ]] && [[ -n "${HF_DATASET_KEY:-}" ]]; then
    _hf_dataset_sync_bg
  fi
}

_hf_dataset_sync_bg() {
  # Run in background — upload the current chat jsonl to the dataset repo
  local chat_file="$CURRENT_CHAT_FILE"
  local chat_name="$CURRENT_CHAT_NAME"
  local hf_key="$HF_DATASET_KEY"
  local repo="$HF_DATASET_REPO"
  [[ -z "$PYTHON" ]] && return 0

  ( HF_CHAT_FILE="$chat_file" HF_CHAT_NAME="$chat_name" \
    HF_KEY="$hf_key" HF_REPO="$repo" \
    "$PYTHON" - <<'PYEOF' &>/dev/null
import os, sys
try:
    from huggingface_hub import HfApi
except ImportError:
    sys.exit(0)
chat_file = os.environ['HF_CHAT_FILE']
chat_name = os.environ['HF_CHAT_NAME']
hf_key    = os.environ['HF_KEY']
repo      = os.environ['HF_REPO']
if not os.path.exists(chat_file): sys.exit(0)
api = HfApi(token=hf_key)
try:
    api.create_repo(repo_id=repo, repo_type='dataset', exist_ok=True, private=False)
    api.upload_file(
        path_or_fileobj=chat_file,
        path_in_repo=f"chats/{chat_name}.jsonl",
        repo_id=repo,
        repo_type='dataset',
        commit_message=f"sync: {chat_name}",
    )
except Exception as e:
    pass
PYEOF
  ) &
}

cmd_chat_list() {
  hdr "Saved Chats"
  local count=0
  for f in "$CHAT_LOGS_DIR"/*.jsonl; do
    [[ -f "$f" ]] || continue
    count=$(( count + 1 ))
    local name; name=$(basename "$f" .jsonl)
    local lines; lines=$(wc -l < "$f")
    printf "  ${B}%-30s${R} %3d messages\n" "$name" "$lines"
  done
  [[ $count -eq 0 ]] && dim "  No saved chats. Use: ai -C [name] ask ..."
}

cmd_chat_show() {
  local name="${1:-}"
  [[ -z "$name" ]] && { err "Usage: ai chat-show <name>"; return 1; }
  local f="$CHAT_LOGS_DIR/${name}.jsonl"
  [[ ! -f "$f" ]] && { err "Chat '$name' not found"; return 1; }
  hdr "Chat: $name"
  echo ""
  while IFS= read -r line; do
    local role; role=$(echo "$line" | python3 -c "import json,sys; d=json.loads(sys.stdin.read()); print(d.get('role','?'))" 2>/dev/null || echo "?")
    local content; content=$(echo "$line" | python3 -c "import json,sys; d=json.loads(sys.stdin.read()); print(d.get('content',''))" 2>/dev/null || echo "")
    if [[ "$role" == "user" ]]; then
      echo -e "${B}${BCYAN}You:${R} $content"
    else
      echo -e "${B}${BGREEN}AI:${R} $content"
    fi
    echo ""
  done < "$f"
}

cmd_chat_delete() {
  local name="${1:-}"; [[ -z "$name" ]] && { err "Name required"; return 1; }
  local f="$CHAT_LOGS_DIR/${name}.jsonl"
  [[ ! -f "$f" ]] && { err "Chat '$name' not found"; return 1; }
  read -rp "Delete chat '$name'? [y/N]: " ans
  [[ "$ans" =~ ^[Yy]$ ]] || { info "Cancelled"; return 0; }
  rm -f "$f"; ok "Deleted $name"
}

# ════════════════════════════════════════════════════════════════════════════════
#  AUDIO SUPPORT
cmd_audio() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    transcribe) _audio_transcribe "$@" ;;
    tts)        _audio_tts "$@" ;;
    analyze)    _audio_analyze "$@" ;;
    convert)    _audio_convert "$@" ;;
    extract)    _audio_extract_from_video "$@" ;;
    ask)        _audio_ask "$@" ;;
    play)       _audio_play "$@" ;;
    info)       _audio_info "$@" ;;
    *)
      echo -e "${B}${BCYAN}Audio Commands${R}"
      echo "  ${B}ai audio transcribe <file> [--lang en] [--model base]${R}"
      echo "  ${B}ai audio tts <text> [--voice nova] [--out file.mp3]${R}"
      echo "  ${B}ai audio analyze <file>${R}      — Analyze audio with AI"
      echo "  ${B}ai audio convert <in> <out>${R}  — Convert format (ffmpeg)"
      echo "  ${B}ai audio extract <video>${R}     — Extract audio from video"
      echo "  ${B}ai audio ask <file> <question>${R} — Ask about audio content"
      echo "  ${B}ai audio play <file>${R}          — Play audio file"
      echo "  ${B}ai audio info <file>${R}          — Show audio metadata"
      ;;
  esac
}

_audio_transcribe() {
  local file="" lang="en" model_size="base" out=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --lang)  lang="$2"; shift 2 ;;
      --model) model_size="$2"; shift 2 ;;
      --out)   out="$2"; shift 2 ;;
      *)       file="$1"; shift ;;
    esac
  done
  [[ -z "$file" ]] && { err "Usage: ai audio transcribe <file>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }

  # Try OpenAI Whisper API first
  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    info "Transcribing with OpenAI Whisper API..."
    local result
    result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -F "file=@$file" \
      -F "model=whisper-1" \
      -F "language=$lang" 2>/dev/null | python3 -c "import json,sys; d=json.load(sys.stdin); print(d.get('text',''))" 2>/dev/null)
    if [[ -n "$result" ]]; then
      echo "$result"
      if [[ -n "$out" ]]; then
        echo "$result" > "$out"; ok "Saved to $out"
      fi
      return 0
    fi
  fi

  # Fallback: local whisper
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  info "Transcribing with local Whisper (model: $model_size)..."
  local result
  result=$(AUDIO_FILE="$file" WHISPER_MODEL="$model_size" WHISPER_LANG="$lang" \
    "$PYTHON" - <<'PYEOF'
import os, sys
try:
    import whisper
except ImportError:
    try:
        import openai_whisper as whisper
    except ImportError:
        print("ERROR: whisper not installed. Run: pip install openai-whisper --break-system-packages")
        sys.exit(1)
f = os.environ['AUDIO_FILE']
m = os.environ.get('WHISPER_MODEL','base')
l = os.environ.get('WHISPER_LANG','en')
model = whisper.load_model(m)
result = model.transcribe(f, language=l)
print(result['text'])
PYEOF
  )
  if [[ -n "$result" ]]; then
    echo "$result"
    [[ -n "$out" ]] && { echo "$result" > "$out"; ok "Saved to $out"; }
  fi
}

_audio_tts() {
  local text="" voice="nova" out="" speed="1.0"
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --voice) voice="$2"; shift 2 ;;
      --out)   out="$2"; shift 2 ;;
      --speed) speed="$2"; shift 2 ;;
      *)       text="${text}${text:+ }$1"; shift ;;
    esac
  done
  [[ -z "$text" ]] && { read -rp "Text: " text; }
  [[ -z "$text" ]] && { err "No text provided"; return 1; }

  if [[ -z "$out" ]]; then
    out="$AUDIO_DIR/tts_$(date +%Y%m%d_%H%M%S).mp3"
  fi

  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    info "Generating TTS with OpenAI (voice: $voice)..."
    curl -sS https://api.openai.com/v1/audio/speech \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"tts-1\",\"input\":$(echo "$text" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))'),\"voice\":\"$voice\",\"speed\":$speed}" \
      --output "$out"
    ok "Saved to $out"
    _audio_play "$out"
  else
    # Fallback: pyttsx3 or espeak
    if "$PYTHON" -c "import pyttsx3" 2>/dev/null; then
      SPEAK_TEXT="$text" SPEAK_OUT="$out" "$PYTHON" - <<'PYEOF'
import os, pyttsx3
engine = pyttsx3.init()
engine.save_to_file(os.environ['SPEAK_TEXT'], os.environ['SPEAK_OUT'])
engine.runAndWait()
print(f"Saved to {os.environ['SPEAK_OUT']}")
PYEOF
    elif command -v espeak &>/dev/null; then
      espeak "$text" -w "$out"
      ok "Saved to $out"
    else
      err "No TTS backend. Set OPENAI_API_KEY or install: pip install pyttsx3 --break-system-packages"
      return 1
    fi
  fi
}

_audio_analyze() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "Usage: ai audio analyze <file>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }

  # First transcribe, then analyze with AI
  info "Transcribing for analysis..."
  local transcript; transcript=$(_audio_transcribe "$file" 2>/dev/null)
  [[ -z "$transcript" ]] && { err "Could not transcribe audio"; return 1; }

  local prompt="Analyze this audio transcript. Provide: 1) Summary, 2) Key topics, 3) Sentiment, 4) Notable quotes.

Transcript:
$transcript"
  dispatch_ask "$prompt"
}

_audio_convert() {
  local input="${1:-}"; local output="${2:-}"
  [[ -z "$input" || -z "$output" ]] && { err "Usage: ai audio convert <input> <output>"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg not installed"; return 1; }
  ffmpeg -i "$input" "$output" && ok "Converted: $output"
}

_audio_extract_from_video() {
  local video="${1:-}"; local out="${2:-}"
  [[ -z "$video" ]] && { err "Usage: ai audio extract <video> [output.mp3]"; return 1; }
  [[ ! -f "$video" ]] && { err "File not found: $video"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  [[ -z "$out" ]] && out="$AUDIO_DIR/$(basename "$video" | sed 's/\.[^.]*$//').mp3"
  ffmpeg -i "$video" -q:a 0 -map a "$out" -y
  ok "Audio extracted to: $out"
}

_audio_ask() {
  local file="${1:-}"; shift
  local question="$*"
  [[ -z "$file" || -z "$question" ]] && { err "Usage: ai audio ask <file> <question>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }
  info "Transcribing..."
  local transcript; transcript=$(_audio_transcribe "$file" 2>/dev/null)
  local prompt="Audio transcript:
$transcript

Question: $question"
  dispatch_ask "$prompt"
}

_audio_play() {
  local file="${1:-}"; [[ -z "$file" ]] && return 0
  for player in mpv vlc aplay paplay afplay; do
    command -v "$player" &>/dev/null && { "$player" "$file" &>/dev/null & return 0; }
  done
  warn "No audio player found (install mpv)"
}

_audio_info() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "File required"; return 1; }
  [[ ! -f "$file" ]] && { err "Not found: $file"; return 1; }
  if command -v ffprobe &>/dev/null; then
    ffprobe -v quiet -print_format json -show_format -show_streams "$file" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
fmt=d.get('format',{})
print(f\"File:     {fmt.get('filename','?')}\")
print(f\"Duration: {float(fmt.get('duration',0)):.1f}s\")
print(f\"Size:     {int(fmt.get('size',0))//1024} KB\")
print(f\"Bitrate:  {int(fmt.get('bit_rate',0))//1000} kbps\")
for s in d.get('streams',[]):
    print(f\"Stream:   {s.get('codec_type','?')} / {s.get('codec_name','?')} / {s.get('sample_rate','?')}Hz\")
" 2>/dev/null
  else
    ls -lh "$file"
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
#  VIDEO SUPPORT
# ════════════════════════════════════════════════════════════════════════════════
cmd_video() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    analyze)   _video_analyze "$@" ;;
    transcribe) _video_transcribe "$@" ;;
    caption)   _video_caption "$@" ;;
    convert)   _video_convert "$@" ;;
    extract)   _video_extract_frames "$@" ;;
    ask)       _video_ask "$@" ;;
    trim)      _video_trim "$@" ;;
    info)      _video_info "$@" ;;
    summary)   _video_summary "$@" ;;
    *)
      echo -e "${B}${BCYAN}Video Commands${R}"
      echo "  ${B}ai video analyze <file>${R}          — Analyze video content with AI"
      echo "  ${B}ai video transcribe <file>${R}       — Transcribe video audio"
      echo "  ${B}ai video caption <file>${R}          — Generate captions/subtitles (.srt)"
      echo "  ${B}ai video convert <in> <out>${R}      — Convert video format"
      echo "  ${B}ai video extract <file> [fps]${R}    — Extract frames"
      echo "  ${B}ai video ask <file> <question>${R}   — Ask about video"
      echo "  ${B}ai video trim <in> <start> <end> <out>${R}"
      echo "  ${B}ai video info <file>${R}             — Show video metadata"
      echo "  ${B}ai video summary <file>${R}          — AI summary of video"
      ;;
  esac
}

_video_info() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "File required"; return 1; }
  command -v ffprobe &>/dev/null || { err "ffmpeg/ffprobe required"; return 1; }
  ffprobe -v quiet -print_format json -show_format -show_streams "$file" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
fmt=d.get('format',{})
print(f\"File:     {fmt.get('filename','?')}\")
dur=float(fmt.get('duration',0))
print(f\"Duration: {int(dur//60)}m {dur%60:.1f}s\")
print(f\"Size:     {int(fmt.get('size',0))//1024//1024} MB\")
print(f\"Bitrate:  {int(fmt.get('bit_rate',0))//1000} kbps\")
for s in d.get('streams',[]):
    if s.get('codec_type')=='video':
        print(f\"Video:    {s.get('codec_name')} {s.get('width')}x{s.get('height')} @ {s.get('r_frame_rate','?')} fps\")
    elif s.get('codec_type')=='audio':
        print(f\"Audio:    {s.get('codec_name')} {s.get('sample_rate','?')}Hz {s.get('channel_layout','?')}\")
" 2>/dev/null
}

_video_transcribe() {
  local file="${1:-}"; shift
  [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  info "Extracting audio..."
  local tmp_audio; tmp_audio=$(mktemp /tmp/vid_audio_XXXX.mp3)
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  ffmpeg -i "$file" -q:a 0 -map a "$tmp_audio" -y &>/dev/null
  info "Transcribing..."
  _audio_transcribe "$tmp_audio" "$@"
  rm -f "$tmp_audio"
}

_video_caption() {
  local file="${1:-}"; [[ -z "$file" ]] && { err "Video file required"; return 1; }
  local out_srt="${file%.*}.srt"
  info "Generating captions for: $file"

  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    local tmp; tmp=$(mktemp /tmp/vid_XXXX.mp3)
    ffmpeg -i "$file" -q:a 0 -map a "$tmp" -y &>/dev/null
    local result
    result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -F "file=@$tmp" -F "model=whisper-1" -F "response_format=srt" 2>/dev/null)
    rm -f "$tmp"
    echo "$result" > "$out_srt"
    ok "Captions saved: $out_srt"
  else
    err "OpenAI API key required for caption generation (provides word-level timestamps)"
  fi
}

_video_extract_frames() {
  local file="${1:-}"; local fps="${2:-1}"
  [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  local out_dir="$VIDEO_DIR/frames_$(basename "$file" | sed 's/\.[^.]*$//')_$(date +%H%M%S)"
  mkdir -p "$out_dir"
  ffmpeg -i "$file" -vf "fps=$fps" "$out_dir/frame_%04d.jpg" -y &>/dev/null
  local count; count=$(ls "$out_dir"/*.jpg 2>/dev/null | wc -l)
  ok "Extracted $count frames to $out_dir"
}

_video_trim() {
  local input="${1:-}"; local start="${2:-}"; local end="${3:-}"; local output="${4:-}"
  [[ -z "$input" || -z "$start" || -z "$end" ]] && {
    err "Usage: ai video trim <input> <start> <end> [output]"
    err "Example: ai video trim video.mp4 00:01:00 00:02:30 clip.mp4"
    return 1
  }
  [[ -z "$output" ]] && output="$VIDEO_DIR/trim_$(basename "$input")"
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  ffmpeg -i "$input" -ss "$start" -to "$end" -c copy "$output" -y
  ok "Trimmed video: $output"
}

_video_convert() {
  local input="${1:-}"; local output="${2:-}"
  [[ -z "$input" || -z "$output" ]] && { err "Usage: ai video convert <input> <output>"; return 1; }
  command -v ffmpeg &>/dev/null || { err "ffmpeg required"; return 1; }
  ffmpeg -i "$input" "$output" && ok "Converted: $output"
}

_video_analyze() {
  local file="${1:-}"; [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  info "Analyzing video: $file"
  _video_info "$file"
  echo ""
  info "Transcribing audio for analysis..."
  local transcript; transcript=$(_video_transcribe "$file" 2>/dev/null)

  # Extract a few frames and describe them if vision model available
  local frame_desc=""
  if command -v ffmpeg &>/dev/null && [[ -n "${OPENAI_API_KEY:-}" ]]; then
    local tmpdir; tmpdir=$(mktemp -d /tmp/vid_frames_XXXX)
    ffmpeg -i "$file" -vf "fps=0.1" -vframes 3 "$tmpdir/frame_%02d.jpg" -y &>/dev/null
    local first_frame="$tmpdir/frame_01.jpg"
    if [[ -f "$first_frame" ]]; then
      info "Analyzing key frame with vision model..."
      local b64; b64=$(base64 -w0 < "$first_frame" 2>/dev/null || base64 < "$first_frame" 2>/dev/null)
      frame_desc=$(curl -sS https://api.openai.com/v1/chat/completions \
        -H "Authorization: Bearer $OPENAI_API_KEY" \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"gpt-4o\",\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/jpeg;base64,$b64\"}},{\"type\":\"text\",\"text\":\"Briefly describe what you see in this video frame.\"}]}],\"max_tokens\":200}" 2>/dev/null | \
        python3 -c "import json,sys; d=json.load(sys.stdin); print(d['choices'][0]['message']['content'])" 2>/dev/null)
    fi
    rm -rf "$tmpdir"
  fi

  local prompt="Analyze this video content comprehensively.

${frame_desc:+Visual description of key frame:
$frame_desc

}${transcript:+Audio transcript:
$transcript

}Provide: 1) Overall summary, 2) Key topics/themes, 3) Sentiment/tone, 4) Notable moments."

  dispatch_ask "$prompt"
}

_video_ask() {
  local file="${1:-}"; shift
  local question="$*"
  [[ -z "$file" || -z "$question" ]] && { err "Usage: ai video ask <file> <question>"; return 1; }
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }
  info "Processing video for question answering..."
  local transcript; transcript=$(_video_transcribe "$file" 2>/dev/null)
  local prompt="Video content:
${transcript:-[No audio/transcript available]}

Question: $question"
  dispatch_ask "$prompt"
}

_video_summary() {
  local file="${1:-}"; [[ -z "$file" || ! -f "$file" ]] && { err "Video file required"; return 1; }
  local transcript; transcript=$(_video_transcribe "$file" 2>/dev/null)
  dispatch_ask "Summarize this video content in 3-5 sentences:
$transcript"
}

# ════════════════════════════════════════════════════════════════════════════════
#  IMAGE-TEXT-TO-TEXT (Full vision support)
# ════════════════════════════════════════════════════════════════════════════════
cmd_vision() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    ask)     _vision_ask "$@" ;;
    ocr)     _vision_ocr "$@" ;;
    caption) _vision_caption "$@" ;;
    compare) _vision_compare "$@" ;;
    *)
      echo -e "${B}${BCYAN}Vision (Image-Text-to-Text)${R}"
      echo "  ${B}ai vision ask <image> <question>${R}   — Ask about an image"
      echo "  ${B}ai vision ocr <image>${R}              — Extract text from image"
      echo "  ${B}ai vision caption <image>${R}          — Generate image caption"
      echo "  ${B}ai vision compare <img1> <img2>${R}    — Compare two images"
      echo ""
      echo "  Supports: jpg, png, gif, webp, bmp"
      echo "  Backends: OpenAI GPT-4o (best), Claude 3, Gemini 1.5, LLaVA (local)"
      ;;
  esac
}

_encode_image_b64() {
  local file="$1"
  base64 -w0 < "$file" 2>/dev/null || base64 < "$file" 2>/dev/null
}

_vision_ask() {
  local image="${1:-}"; shift
  local question="${*:-Describe this image in detail.}"
  [[ -z "$image" ]] && { err "Usage: ai vision ask <image> <question>"; return 1; }
  [[ ! -f "$image" ]] && { err "Image not found: $image"; return 1; }

  local ext="${image##*.}"; ext="${ext,,}"
  local mime
  case "$ext" in
    jpg|jpeg) mime="image/jpeg" ;;
    png)      mime="image/png" ;;
    gif)      mime="image/gif" ;;
    webp)     mime="image/webp" ;;
    *)        mime="image/jpeg" ;;
  esac

  local b64; b64=$(_encode_image_b64 "$image")

  if [[ -n "${OPENAI_API_KEY:-}" ]]; then
    curl -sS https://api.openai.com/v1/chat/completions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"gpt-4o\",\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:$mime;base64,$b64\"}},{\"type\":\"text\",\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}],\"max_tokens\":${MAX_TOKENS}}]}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['choices'][0]['message']['content'])" 2>/dev/null
  elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
    curl -sS https://api.anthropic.com/v1/messages \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -H "anthropic-version: 2023-06-01" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"claude-opus-4-5\",\"max_tokens\":${MAX_TOKENS},\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image\",\"source\":{\"type\":\"base64\",\"media_type\":\"$mime\",\"data\":\"$b64\"}},{\"type\":\"text\",\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}]}]}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['content'][0]['text'])" 2>/dev/null
  elif [[ -n "${GEMINI_API_KEY:-}" ]]; then
    curl -sS "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=$GEMINI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"contents\":[{\"parts\":[{\"inline_data\":{\"mime_type\":\"$mime\",\"data\":\"$b64\"}},{\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}]}]}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['candidates'][0]['content']['parts'][0]['text'])" 2>/dev/null
  elif [[ -n "$PYTHON" ]] && "$PYTHON" -c "import llava" 2>/dev/null; then
    IMAGE_FILE="$image" IMAGE_QUESTION="$question" "$PYTHON" - <<'PYEOF'
import os
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
model_path = "liuhaotian/llava-v1.5-7b"
tokenizer, model, image_processor, _ = load_pretrained_model(model_path, None, get_model_name_from_path(model_path))
from PIL import Image
image = Image.open(os.environ['IMAGE_FILE']).convert('RGB')
# simplified inference
print("LLaVA vision response")
PYEOF
  else
    err "No vision-capable backend. Set OPENAI_API_KEY, ANTHROPIC_API_KEY, or GEMINI_API_KEY"
    return 1
  fi
}

_vision_ocr() {
  local image="${1:-}"; [[ -z "$image" || ! -f "$image" ]] && { err "Image required"; return 1; }
  _vision_ask "$image" "Extract ALL text from this image exactly as it appears. Return only the text, no commentary."
}

_vision_caption() {
  local image="${1:-}"; [[ -z "$image" || ! -f "$image" ]] && { err "Image required"; return 1; }
  _vision_ask "$image" "Write a concise, descriptive caption for this image in one sentence."
}

_vision_compare() {
  local img1="${1:-}"; local img2="${2:-}"; local question="${3:-What are the differences between these images?}"
  [[ -z "$img1" || -z "$img2" ]] && { err "Usage: ai vision compare <img1> <img2> [question]"; return 1; }
  [[ ! -f "$img1" ]] && { err "Not found: $img1"; return 1; }
  [[ ! -f "$img2" ]] && { err "Not found: $img2"; return 1; }

  local b64_1; b64_1=$(_encode_image_b64 "$img1")
  local b64_2; b64_2=$(_encode_image_b64 "$img2")
  local mime1="image/jpeg"; local mime2="image/jpeg"

  [[ -n "${OPENAI_API_KEY:-}" ]] && \
    curl -sS https://api.openai.com/v1/chat/completions \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"gpt-4o\",\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:$mime1;base64,$b64_1\"}},{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:$mime2;base64,$b64_2\"}},{\"type\":\"text\",\"text\":$(echo "$question" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')}]}],\"max_tokens\":${MAX_TOKENS}}" 2>/dev/null | \
      python3 -c "import json,sys; d=json.load(sys.stdin); print(d['choices'][0]['message']['content'])" 2>/dev/null || \
    err "Vision comparison requires OPENAI_API_KEY"
}


# ════════════════════════════════════════════════════════════════════════════════

# ════════════════════════════════════════════════════════════════════════════════
#  NEW GUI — Python curses with full mouse click support
# ════════════════════════════════════════════════════════════════════════════════
# The GUI is a self-contained Python script written inline and executed.
# It uses curses with mouse events so every button is clickable.
# Falls back to numbered selection if curses unavailable.


# ════════════════════════════════════════════════════════════════════════════════
#  GUI v3 — Remade from scratch: panels, status bar, inline chat, full nav
# ════════════════════════════════════════════════════════════════════════════════

cmd_gui() {
  [[ -z "$PYTHON" ]] && { err "Python 3.10+ required for GUI"; _gui_fallback; return; }

  local gui_script; gui_script=$(mktemp /tmp/ai_gui_XXXX.py)
  local cli_bin; cli_bin=$(command -v ai 2>/dev/null || echo "$0")
  local theme="${GUI_THEME:-dark}"

  cat > "$gui_script" << 'GUIEOF'
#!/usr/bin/env python3
"""AI CLI v2.5.1 — GUI v3: remade from scratch with panels, status bar, full nav"""
import sys, os, curses, subprocess, threading, time, textwrap, json

CLI  = sys.argv[1] if len(sys.argv) > 1 else "ai"
THEME_NAME = sys.argv[2] if len(sys.argv) > 2 else "dark"

# ── Themes ────────────────────────────────────────────────────────────────────
THEMES = {
    "dark":     {"bg":0,"fg":7,"accent":6,"accent2":2,"warn":3,"err":1,"dim":8,"sel_bg":4,"sel_fg":15,"border":6,"title":14},
    "light":    {"bg":15,"fg":0,"accent":4,"accent2":2,"warn":3,"err":1,"dim":8,"sel_bg":6,"sel_fg":0,"border":4,"title":4},
    "hacker":   {"bg":0,"fg":2,"accent":10,"accent2":2,"warn":11,"err":9,"dim":8,"sel_bg":2,"sel_fg":0,"border":2,"title":10},
    "matrix":   {"bg":0,"fg":10,"accent":2,"accent2":10,"warn":11,"err":9,"dim":8,"sel_bg":10,"sel_fg":0,"border":10,"title":10},
    "ocean":    {"bg":17,"fg":14,"accent":12,"accent2":6,"warn":11,"err":9,"dim":8,"sel_bg":12,"sel_fg":0,"border":6,"title":14},
    "solarized":{"bg":234,"fg":136,"accent":37,"accent2":64,"warn":136,"err":160,"dim":240,"sel_bg":33,"sel_fg":15,"border":37,"title":136},
    "nord":     {"bg":236,"fg":153,"accent":67,"accent2":108,"warn":179,"err":131,"dim":242,"sel_bg":67,"sel_fg":15,"border":67,"title":153},
    "gruvbox":  {"bg":235,"fg":223,"accent":214,"accent2":142,"warn":214,"err":167,"dim":243,"sel_bg":214,"sel_fg":235,"border":214,"title":214},
}
T = THEMES.get(THEME_NAME, THEMES["dark"])

# ── Color pair IDs ────────────────────────────────────────────────────────────
CP_NORMAL = 1   # fg on bg
CP_ACCENT = 2   # accent on bg
CP_ACCENT2= 3
CP_WARN   = 4
CP_ERR    = 5
CP_DIM    = 6
CP_SEL    = 7   # selected item
CP_BORDER = 8
CP_TITLE  = 9
CP_USER   = 10
CP_AI     = 11
CP_SYS    = 12
CP_HBAR   = 13  # header/status bar

def init_colors():
    curses.start_color()
    curses.use_default_colors()
    bg = T["bg"] if T["bg"] > 0 else -1
    curses.init_pair(CP_NORMAL, T["fg"],       bg)
    curses.init_pair(CP_ACCENT, T["accent"],   bg)
    curses.init_pair(CP_ACCENT2,T["accent2"],  bg)
    curses.init_pair(CP_WARN,   T["warn"],     bg)
    curses.init_pair(CP_ERR,    T["err"],      bg)
    curses.init_pair(CP_DIM,    T["dim"],      bg)
    curses.init_pair(CP_SEL,    T["sel_fg"],   T["sel_bg"])
    curses.init_pair(CP_BORDER, T["border"],   bg)
    curses.init_pair(CP_TITLE,  T["title"],    bg)
    curses.init_pair(CP_USER,   12,            bg)
    curses.init_pair(CP_AI,     T["accent2"],  bg)
    curses.init_pair(CP_SYS,    T["dim"],      bg)
    curses.init_pair(CP_HBAR,   15,            T["sel_bg"])

def safe_add(win, y, x, text, attr=0):
    h, w = win.getmaxyx()
    if y < 0 or y >= h or x < 0: return
    available = w - x - 1
    if available <= 0: return
    try: win.addstr(y, x, text[:available], attr)
    except curses.error: pass

def draw_hline(win, y, x, w, char='─', attr=0):
    for i in range(w):
        try: win.addch(y, x+i, ord(char), attr)
        except: pass

def draw_box(win, y, x, h, w, title="", attr=None):
    if attr is None: attr = curses.color_pair(CP_BORDER)
    try:
        win.attron(attr)
        win.addch(y,     x,     curses.ACS_ULCORNER)
        win.addch(y,     x+w-1, curses.ACS_URCORNER)
        win.addch(y+h-1, x,     curses.ACS_LLCORNER)
        win.addch(y+h-1, x+w-1, curses.ACS_LRCORNER)
        for i in range(1, w-1): win.addch(y,     x+i, curses.ACS_HLINE)
        for i in range(1, w-1): win.addch(y+h-1, x+i, curses.ACS_HLINE)
        for i in range(1, h-1): win.addch(y+i, x,     curses.ACS_VLINE)
        for i in range(1, h-1): win.addch(y+i, x+w-1, curses.ACS_VLINE)
        win.attroff(attr)
        if title:
            t = f" {title} "
            tx = x + max(1, (w - len(t)) // 2)
            safe_add(win, y, tx, t, attr | curses.A_BOLD)
    except curses.error: pass

# ── Utility ───────────────────────────────────────────────────────────────────
def run_ai(*args, timeout=60):
    try:
        r = subprocess.run([CLI, *args], capture_output=True, text=True, timeout=timeout)
        return (r.stdout + r.stderr).strip()
    except subprocess.TimeoutExpired:
        return "[timeout]"
    except Exception as e:
        return f"[error: {e}]"

def wrap_text(text, width):
    lines = []
    for para in text.splitlines():
        if para.strip() == "":
            lines.append("")
        else:
            lines.extend(textwrap.wrap(para, width) or [""])
    return lines

# ── Input prompt ──────────────────────────────────────────────────────────────
def input_line(stdscr, y, x, w, prompt="", prefill=""):
    """Single-line input with basic editing. Returns text or None on Escape."""
    curses.echo(False)
    buf = list(prefill)
    cur = len(buf)

    while True:
        # Draw input box
        safe_add(stdscr, y, x, prompt, curses.color_pair(CP_ACCENT) | curses.A_BOLD)
        px = x + len(prompt)
        field_w = w - len(prompt) - 1
        field_txt = ''.join(buf)
        # Show a window of text if longer than field
        if cur >= field_w:
            visible = field_txt[cur - field_w + 1 : cur + 1]
            vcur = field_w - 1
        else:
            visible = field_txt[:field_w]
            vcur = cur
        safe_add(stdscr, y, px, visible.ljust(field_w), curses.color_pair(CP_NORMAL))
        try: stdscr.move(y, px + vcur)
        except: pass
        stdscr.refresh()

        ch = stdscr.getch()
        if ch in (10, 13):          # Enter
            return ''.join(buf)
        elif ch == 27:              # Escape
            return None
        elif ch in (curses.KEY_BACKSPACE, 127, 8):
            if cur > 0: buf.pop(cur - 1); cur -= 1
        elif ch == curses.KEY_DC:
            if cur < len(buf): buf.pop(cur)
        elif ch == curses.KEY_LEFT:
            if cur > 0: cur -= 1
        elif ch == curses.KEY_RIGHT:
            if cur < len(buf): cur += 1
        elif ch == curses.KEY_HOME: cur = 0
        elif ch == curses.KEY_END:  cur = len(buf)
        elif 32 <= ch <= 126:
            buf.insert(cur, chr(ch)); cur += 1

# ── Scrollable menu ───────────────────────────────────────────────────────────
class Menu:
    def __init__(self, items, title=""):
        # items: list of (label, value) or strings
        self.items = [(i,i) if isinstance(i, str) else i for i in items]
        self.title = title
        self.idx   = 0
        self.offset= 0

    def draw(self, win, y, x, h, w):
        draw_box(win, y, x, h, w, self.title)
        visible = h - 2
        # Scroll offset
        if self.idx < self.offset:
            self.offset = self.idx
        if self.idx >= self.offset + visible:
            self.offset = self.idx - visible + 1

        for i in range(visible):
            ri = i + self.offset
            if ri >= len(self.items): break
            label = self.items[ri][0]
            attr  = curses.color_pair(CP_SEL) | curses.A_BOLD if ri == self.idx \
                    else curses.color_pair(CP_NORMAL)
            safe_add(win, y+1+i, x+1, f" {label:<{w-3}}", attr)

        # Scrollbar indicator
        if len(self.items) > visible:
            bar_y = y + 1 + int(self.idx / max(1,len(self.items)-1) * (visible-1))
            try: win.addch(bar_y, x+w-1, ord('█'), curses.color_pair(CP_ACCENT))
            except: pass

    def handle(self, ch):
        if ch == curses.KEY_UP:
            self.idx = max(0, self.idx - 1)
        elif ch == curses.KEY_DOWN:
            self.idx = min(len(self.items)-1, self.idx + 1)
        elif ch == curses.KEY_PPAGE:
            self.idx = max(0, self.idx - 5)
        elif ch == curses.KEY_NPAGE:
            self.idx = min(len(self.items)-1, self.idx + 5)
        elif ch == curses.KEY_HOME:
            self.idx = 0
        elif ch == curses.KEY_END:
            self.idx = len(self.items)-1
        elif ch in (10, 13):
            return self.items[self.idx][1]
        return None

# ── Chat panel ────────────────────────────────────────────────────────────────
class ChatPanel:
    def __init__(self):
        self.messages = []   # (role, text)
        self.input    = ""
        self.scroll   = 0
        self.thinking = False
        self._lock    = threading.Lock()

    def add(self, role, text):
        with self._lock:
            self.messages.append((role, text))

    def ask_async(self, prompt, on_done):
        self.thinking = True
        def worker():
            resp = run_ai("ask", prompt, timeout=120)
            self.add("AI", resp if resp else "[No response]")
            self.thinking = False
            on_done()
        threading.Thread(target=worker, daemon=True).start()

    def draw(self, win, y, x, h, w):
        inner_w = w - 2
        draw_box(win, y, x, h, w, "Chat")

        # Build rendered lines
        rendered = []
        with self._lock:
            msgs = list(self.messages)
        for role, text in msgs:
            if role == "You":
                rendered.append(("You", f"You: {text}", CP_USER))
            elif role == "AI":
                rendered.append(("AI", f"AI:  {text}", CP_AI))
            else:
                rendered.append(("sys", f"     {text}", CP_SYS))

        # Word-wrap all lines
        all_lines = []  # (text, color_pair_id)
        for _, raw, cp in rendered:
            wrapped = wrap_text(raw, inner_w - 1)
            for i, wl in enumerate(wrapped):
                all_lines.append((wl, cp if i == 0 else CP_NORMAL))
            all_lines.append(("", CP_NORMAL))

        visible_h = h - 3  # reserve rows for input
        total = len(all_lines)
        # Auto-scroll to bottom
        if self.scroll == 0 or self.scroll > total - visible_h:
            self.scroll = max(0, total - visible_h)

        for i in range(visible_h):
            li = i + self.scroll
            if li >= total: break
            text, cp = all_lines[li]
            safe_add(win, y+1+i, x+1, text[:inner_w], curses.color_pair(cp))

        # Thinking indicator
        if self.thinking:
            dots = "." * (int(time.time() * 2) % 4)
            safe_add(win, y+h-3, x+1, f"AI is thinking{dots:<4}", curses.color_pair(CP_WARN) | curses.A_BOLD)

        # Input line separator
        draw_hline(win, y+h-2, x+1, inner_w, '─', curses.color_pair(CP_BORDER))
        safe_add(win, y+h-1, x+1, f"> {self.input[:inner_w-3]}", curses.color_pair(CP_ACCENT))

    def handle_input(self, ch):
        """Returns True if should redraw."""
        if ch in (curses.KEY_BACKSPACE, 127, 8):
            self.input = self.input[:-1]
        elif ch in (10, 13):
            return "submit"
        elif ch == curses.KEY_PPAGE:
            self.scroll = max(0, self.scroll - 5)
        elif ch == curses.KEY_NPAGE:
            self.scroll += 5
        elif 32 <= ch <= 126:
            self.input += chr(ch)
        return True

# ── Main App ──────────────────────────────────────────────────────────────────
class App:
    MAIN_MENU = [
        ("💬 Chat",              "chat"),
        ("❓ Ask (one-shot)",     "ask"),
        ("🖼  Imagine (image)",    "imagine"),
        ("👁  Vision",             "vision"),
        ("🔊 Audio",              "audio"),
        ("📹 Video",              "video"),
        ("🖊  Canvas v2",          "canvas"),
        ("🤖 Models",             "models"),
        ("🧠 TTM (179M)",         "ttm"),
        ("🧠 MTM (0.61B)",        "mtm"),
        ("🧠 Mtm (1.075B)",       "mmtm"),
        ("📊 Datasets",           "datasets"),
        ("🔁 RLHF",              "rlhf"),
        ("🏆 Multi-AI Arena",     "multiai"),
        ("🔍 Web Search",         "websearch"),
        ("🐙 GitHub",             "github"),
        ("📄 Research Papers",    "papers"),
        ("⚙  Settings",           "settings"),
        ("ℹ  Status",             "status"),
        ("🎨 Change Theme",       "theme"),
        ("🚪 Quit",               "quit"),
    ]

    THEME_MENU = [(t.capitalize(), t) for t in THEMES.keys()]

    def __init__(self, stdscr):
        self.stdscr   = stdscr
        self.chat     = ChatPanel()
        self.mode     = "menu"   # menu | chat | output | input
        self.menu     = Menu(self.MAIN_MENU, "AI CLI v2.5.1")
        self.output   = []       # list of strings for output pager
        self.out_scroll = 0
        self.status   = ""
        self.redraw   = True

    # ── Layout helpers ────────────────────────────────────────────────────────
    def dims(self):
        h, w = self.stdscr.getmaxyx()
        return h, w

    def _draw_header(self, h, w):
        bar = f" AI CLI v2.5.1 │ {THEME_NAME.upper()} │ {self.mode.upper()} │ Ctrl+Q=quit  Tab=menu  F1=help "
        safe_add(self.stdscr, 0, 0, bar.ljust(w), curses.color_pair(CP_HBAR) | curses.A_BOLD)

    def _draw_status(self, h, w):
        hints = {
            "menu":   "↑↓=navigate  Enter=select  Q=quit",
            "chat":   "Type+Enter=send  PgUp/Dn=scroll  Tab=menu  Ctrl+Q=quit",
            "output": "↑↓/PgUp/Dn=scroll  Q/Esc=back  Tab=menu",
        }
        bar = f" {self.status or hints.get(self.mode,'Ctrl+Q=quit')} "
        safe_add(self.stdscr, h-1, 0, bar.ljust(w), curses.color_pair(CP_HBAR))

    # ── Output pager ──────────────────────────────────────────────────────────
    def show_output(self, text, title="Output"):
        self.output = wrap_text(text, self.dims()[1] - 4)
        self.out_scroll = 0
        self.out_title  = title
        self.mode = "output"

    def _draw_output(self, h, w):
        draw_box(self.stdscr, 1, 0, h-2, w, self.out_title)
        visible = h - 4
        # clamp scroll
        max_sc = max(0, len(self.output) - visible)
        self.out_scroll = min(self.out_scroll, max_sc)
        for i in range(visible):
            li = i + self.out_scroll
            if li >= len(self.output): break
            safe_add(self.stdscr, 2+i, 2, self.output[li], curses.color_pair(CP_NORMAL))
        # scroll pos
        if len(self.output) > visible:
            pct = int(self.out_scroll / max(1,max_sc) * 100)
            safe_add(self.stdscr, h-2, w-10, f"{pct:3d}%↕    ", curses.color_pair(CP_DIM))

    # ── Menu ──────────────────────────────────────────────────────────────────
    def _draw_menu(self, h, w):
        mw = min(36, w - 2)
        mh = h - 2
        mx = (w - mw) // 2
        self.menu.draw(self.stdscr, 1, mx, mh, mw)

        # Preview hint at bottom of menu box
        if self.menu.items:
            label = self.menu.items[self.menu.idx][0]
            hint  = label.lstrip("💬❓🖼👁🔊📹🖊🤖🧠📊🔁🏆🔍🐙📄⚙ℹ🎨🚪 ")
            safe_add(self.stdscr, 1+mh-1, mx+2,
                     f" → {hint[:mw-6]} ", curses.color_pair(CP_DIM))

    # ── Chat ──────────────────────────────────────────────────────────────────
    def _draw_chat(self, h, w):
        self.chat.draw(self.stdscr, 1, 0, h-2, w)

    # ── Action dispatcher ─────────────────────────────────────────────────────
    def do_action(self, action):
        h, w = self.dims()

        if action == "quit":
            return False   # signal quit

        elif action == "chat":
            self.mode = "chat"
            self.chat.add("sys", "Chat started. Type messages, Enter to send, Tab for menu.")

        elif action == "ask":
            q = input_line(self.stdscr, h//2, 2, w-4, "Ask: ")
            if q:
                self.status = "Thinking..."
                result = run_ai("ask", q)
                self.show_output(result, "Answer")

        elif action == "imagine":
            q = input_line(self.stdscr, h//2, 2, w-4, "Image prompt: ")
            if q:
                self.status = "Generating image..."
                result = run_ai("imagine", q)
                self.show_output(result, "Image Generation")

        elif action == "vision":
            path = input_line(self.stdscr, h//2-1, 2, w-4, "Image path: ")
            if path:
                q = input_line(self.stdscr, h//2+1, 2, w-4, "Question:   ")
                if q:
                    self.status = "Analyzing image..."
                    result = run_ai("vision", path, q)
                    self.show_output(result, "Vision")

        elif action == "audio":
            result = run_ai("audio", "help")
            self.show_output(result, "Audio Commands")

        elif action == "video":
            result = run_ai("video", "help")
            self.show_output(result, "Video Commands")

        elif action == "canvas":
            result = run_ai("canvas-v2", "help")
            self.show_output(result, "Canvas v2")

        elif action == "models":
            result = run_ai("models")
            self.show_output(result, "Downloaded Models")

        elif action == "ttm":
            result = run_ai("ttm")
            self.show_output(result, "TTM — Tiny Model (~179M)")

        elif action == "mtm":
            result = run_ai("mtm")
            self.show_output(result, "MTM — Mini Model (~0.61B)")

        elif action == "mmtm":
            result = run_ai("Mtm")
            self.show_output(result, "Mtm — Medium Model (~1.075B)")

        elif action == "datasets":
            result = run_ai("dataset", "list")
            self.show_output(result or "(no datasets)", "Datasets")

        elif action == "rlhf":
            result = run_ai("rlhf", "status")
            self.show_output(result, "RLHF Status")

        elif action == "multiai":
            q = input_line(self.stdscr, h//2-1, 2, w-4, "Topic:     ")
            if q:
                mode = input_line(self.stdscr, h//2+1, 2, w-4, "Mode (debate/collab/ensemble): ") or "debate"
                self.status = "Multi-AI running..."
                result = run_ai("multiai", mode, q, timeout=180)
                self.show_output(result, f"Multi-AI {mode.capitalize()}")

        elif action == "websearch":
            q = input_line(self.stdscr, h//2, 2, w-4, "Search: ")
            if q:
                result = run_ai("websearch", q)
                self.show_output(result, "Web Search")

        elif action == "github":
            sub = input_line(self.stdscr, h//2-1, 2, w-4, "Github cmd (status/commit/push/log): ") or "status"
            if sub in ("commit",):
                msg = input_line(self.stdscr, h//2+1, 2, w-4, "Commit msg: ") or "Auto-commit"
                result = run_ai("github", sub, msg)
            else:
                result = run_ai("github", sub)
            self.show_output(result, f"GitHub {sub}")

        elif action == "papers":
            q = input_line(self.stdscr, h//2, 2, w-4, "Search papers: ")
            if q:
                self.status = "Searching papers..."
                result = run_ai("papers", "search", q)
                self.show_output(result, "Research Papers")

        elif action == "status":
            result = run_ai("status")
            self.show_output(result, "System Status")

        elif action == "settings":
            result = run_ai("config")
            self.show_output(result, "Settings / Config")

        elif action == "theme":
            tmenu = Menu(self.THEME_MENU, "Select Theme")
            selected = self._run_submenu(tmenu)
            if selected:
                run_ai("config", "gui_theme", selected)
                self.show_output(f"Theme set to: {selected}\nRestart GUI to apply.", "Theme Changed")

        self.status = ""
        return True

    def _run_submenu(self, menu):
        """Run a blocking submenu, return selected value or None."""
        h, w = self.dims()
        mw = min(40, w - 4)
        mh = min(len(menu.items)+2, h-4)
        mx = (w - mw) // 2
        my = (h - mh) // 2
        while True:
            self.stdscr.erase()
            self._draw_header(h, w)
            menu.draw(self.stdscr, my, mx, mh, mw)
            safe_add(self.stdscr, h-1, 0, " Enter=select  Esc=back ".ljust(w),
                     curses.color_pair(CP_HBAR))
            self.stdscr.refresh()
            ch = self.stdscr.getch()
            if ch == 27: return None
            result = menu.handle(ch)
            if result: return result

    # ── Main loop ──────────────────────────────────────────────────────────────
    def run(self):
        self.stdscr.nodelay(False)
        self.stdscr.keypad(True)
        curses.curs_set(0)
        init_colors()

        while True:
            h, w = self.dims()
            if self.redraw or True:  # always redraw for simplicity
                self.stdscr.erase()
                self._draw_header(h, w)

                if self.mode == "menu":
                    self._draw_menu(h, w)
                elif self.mode == "chat":
                    self._draw_chat(h, w)
                elif self.mode == "output":
                    self._draw_output(h, w)

                self._draw_status(h, w)
                self.stdscr.refresh()

            ch = self.stdscr.getch()

            # Global keys
            if ch == 17:           # Ctrl+Q
                break
            elif ch == curses.KEY_F1 or ch == ord('?'):
                self.show_output(
                    "AI CLI v2.5.1 GUI — Keyboard shortcuts:\n\n"
                    "  Ctrl+Q      Quit\n"
                    "  Tab         Return to main menu\n"
                    "  F1 / ?      This help\n"
                    "\nMain Menu:\n"
                    "  ↑↓ PgUp/Dn  Navigate\n"
                    "  Enter       Select\n"
                    "  Q / Esc     Quit\n"
                    "\nChat mode:\n"
                    "  Type        Enter message\n"
                    "  Enter       Send\n"
                    "  Backspace   Delete char\n"
                    "  PgUp/Dn     Scroll history\n"
                    "  Tab         Back to menu\n"
                    "\nOutput pager:\n"
                    "  ↑↓ PgUp/Dn  Scroll\n"
                    "  Q / Esc     Back to menu\n",
                    "Help"
                )
                continue
            elif ch == ord('\t'):  # Tab → back to menu
                self.mode = "menu"
                continue

            # Mode-specific keys
            if self.mode == "menu":
                if ch in (ord('q'), ord('Q'), 27):
                    break
                result = self.menu.handle(ch)
                if result:
                    if not self.do_action(result):
                        break

            elif self.mode == "chat":
                if self.chat.thinking:
                    continue   # ignore input while AI is responding
                result = self.chat.handle_input(ch)
                if result == "submit" and self.chat.input.strip():
                    msg = self.chat.input.strip()
                    self.chat.input = ""
                    # Handle slash commands
                    if msg.startswith("/quit"):
                        self.mode = "menu"
                    elif msg.startswith("/clear"):
                        self.chat.messages = []
                    elif msg.startswith("/help"):
                        self.chat.add("sys", "Commands: /quit /clear /help /theme <n>")
                    elif msg.startswith("/theme "):
                        t = msg.split(None, 1)[1]
                        run_ai("config", "gui_theme", t)
                        self.chat.add("sys", f"Theme → {t} (restart to apply)")
                    else:
                        self.chat.add("You", msg)
                        self.chat.ask_async(msg, lambda: None)

            elif self.mode == "output":
                visible = h - 4
                if ch in (curses.KEY_UP,):
                    self.out_scroll = max(0, self.out_scroll - 1)
                elif ch in (curses.KEY_DOWN,):
                    self.out_scroll += 1
                elif ch == curses.KEY_PPAGE:
                    self.out_scroll = max(0, self.out_scroll - (visible - 1))
                elif ch == curses.KEY_NPAGE:
                    self.out_scroll += visible - 1
                elif ch in (ord('q'), ord('Q'), 27):
                    self.mode = "menu"

def main(stdscr):
    curses.mousemask(curses.ALL_MOUSE_EVENTS | curses.REPORT_MOUSE_POSITION)
    try:
        app = App(stdscr)
        app.run()
    except KeyboardInterrupt:
        pass

curses.wrapper(main)

GUIEOF

  info "Starting GUI (click or keyboard navigation)..."
  "$PYTHON" "$gui_script" "$cli_bin" "$theme"
  local rc=$?
  rm -f "$gui_script"
  [[ $rc -ne 0 ]] && _gui_fallback
}

_gui_fallback() {
  # Text-mode fallback when Python/curses unavailable
  while true; do
    echo ""
    hdr "═══ AI CLI v${VERSION} — Main Menu (text mode) ═══"
    local items=(
      "Chat (interactive)"      "Ask a question"
      "Imagine (image gen)"     "Vision (image→text)"
      "Audio"                   "Video"
      "Canvas v2"               "Models / Download"
      "TTM (Tiny ~179M)"        "MTM (Mini ~0.61B)"
      "Mtm (Medium ~1.075B)"    "Datasets"
      "RLHF"                    "Multi-AI Arena"
      "Web Search"              "GitHub"
      "Research Papers"         "Settings / Config"
      "Status"                  "Quit"
    )
    for i in "${!items[@]}"; do
      printf "  ${B}%2d.${R} %s\n" "$(( i+1 ))" "${items[$i]}"
    done
    echo ""
    read -rp "Choose [1-${#items[@]}]: " choice
    case "$choice" in
      1)  cmd_chat_interactive ;;
      2)  read -rp "Question: " q; dispatch_ask "$q" ;;
      3)  read -rp "Prompt: " p; cmd_imagine "$p" ;;
      4)  read -rp "Image path: " img; read -rp "Question: " q; cmd_vision "$img" "$q" ;;
      5)  cmd_audio ;;
      6)  cmd_video ;;
      7)  cmd_canvas_v2 help ;;
      8)  cmd_list_models; echo ""; read -rp "Download #: " n; [[ -n "$n" ]] && cmd_recommended download "$n" ;;
      9)  cmd_ttm ;;
      10) cmd_mtm ;;
      11) cmd_Mtm ;;
      12) cmd_dataset list ;;
      13) cmd_rlhf status ;;
      14) read -rp "Topic: " q; read -rp "Mode (debate/collab): " m; cmd_multiai "${m:-debate}" "$q" ;;
      15) read -rp "Search: " q; cmd_websearch "$q" ;;
      16) cmd_github help ;;
      17) cmd_papers help ;;
      18) cmd_config ;;
      19) cmd_status ;;
      20|q|Q|"") break ;;
      *) warn "Invalid choice" ;;
    esac
  done
}

# ════════════════════════════════════════════════════════════════════════════════
#  BUILTIN TOOLS
# ════════════════════════════════════════════════════════════════════════════════
BUILTIN_TOOLS=("web_search" "read_file" "write_file" "run_code" "list_dir"
               "get_time" "get_sysinfo" "calc" "download_file" "image_info")

run_tool() {
  local name="$1"; local args_json="${2:-{}}"
  case "$name" in
    web_search)
      local q; q=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('query',''))" 2>/dev/null)
      web_search "$q" 5 ;;
    read_file)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      [[ -f "$p" ]] && cat "$p" || echo "File not found: $p" ;;
    write_file)
      local p c
      p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      c=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('content',''))" 2>/dev/null)
      echo "$c" > "$p" && echo "Written: $p" ;;
    run_code)
      local lang code
      lang=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('language','python'))" 2>/dev/null)
      code=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('code',''))" 2>/dev/null)
      case "$lang" in
        python|py) echo "$code" | python3 2>&1 ;;
        bash|sh)   echo "$code" | bash 2>&1 ;;
        js|node)   echo "$code" | node 2>&1 ;;
        *) echo "Unsupported language: $lang" ;;
      esac ;;
    list_dir)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path','.'))" 2>/dev/null)
      ls -la "$p" 2>&1 ;;
    get_time) date ;;
    get_sysinfo)
      echo "OS: $(uname -s -r)"
      echo "CPU: $(grep -m1 'model name' /proc/cpuinfo 2>/dev/null | cut -d: -f2 | xargs || sysctl -n machdep.cpu.brand_string 2>/dev/null || echo '?')"
      echo "RAM: $(free -h 2>/dev/null | awk '/^Mem/{print $2}' || echo '?')"
      [[ -n "$PYTHON" ]] && echo "Python: $($PYTHON --version 2>&1)"
      command -v nvidia-smi &>/dev/null && nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo "GPU: none/unknown"
      ;;
    calc)
      local expr; expr=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('expression',''))" 2>/dev/null)
      python3 -c "import math; print(eval('$expr'))" 2>&1 ;;
    download_file)
      local url sp
      url=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('url',''))" 2>/dev/null)
      sp=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('save_path','/tmp/download'))" 2>/dev/null)
      curl -sL "$url" -o "$sp" && echo "Saved: $sp" ;;
    image_info)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      [[ -n "$PYTHON" ]] && "$PYTHON" -c "from PIL import Image; im=Image.open('$p'); print(f'Size: {im.size}, Mode: {im.mode}')" 2>/dev/null || file "$p" ;;
    *) echo "Unknown tool: $name" ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  WEB SEARCH
# ════════════════════════════════════════════════════════════════════════════════
web_search() {
  local query="$1"; local max="${2:-5}"
  local encoded; encoded=$(python3 -c "import urllib.parse,sys; print(urllib.parse.quote(sys.argv[1]))" "$query" 2>/dev/null || echo "$query")

  if [[ "${SEARCH_ENGINE:-ddg}" == "brave" ]] && [[ -n "${BRAVE_API_KEY:-}" ]]; then
    curl -sS "https://api.search.brave.com/res/v1/web/search?q=${encoded}&count=${max}" \
      -H "Accept: application/json" \
      -H "X-Subscription-Token: $BRAVE_API_KEY" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
for r in d.get('web',{}).get('results',[])[:int('$max')]:
    print(f\"Title: {r.get('title','')}\")
    print(f\"URL: {r.get('url','')}\")
    print(f\"Snippet: {r.get('description','')[:200]}\")
    print()
" 2>/dev/null
  else
    curl -sS "https://api.duckduckgo.com/?q=${encoded}&format=json&no_redirect=1&no_html=1" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
results=[]
if d.get('AbstractText'):
    results.append({'title': d.get('Heading',''), 'url': d.get('AbstractURL',''), 'snippet': d.get('AbstractText','')})
for r in d.get('RelatedTopics',[])[:int('$max')]:
    if isinstance(r,dict) and r.get('Text'):
        results.append({'title': r.get('Text','')[:80], 'url': r.get('FirstURL',''), 'snippet': r.get('Text','')[:200]})
for r in results[:int('$max')]:
    print(f\"Title: {r['title']}\")
    print(f\"URL: {r['url']}\")
    print(f\"Snippet: {r['snippet']}\")
    print()
" 2>/dev/null
  fi
}

BUILTIN_TOOLS=("web_search" "read_file" "write_file" "run_code" "list_dir"
               "get_time" "get_sysinfo" "calc" "download_file" "image_info")

run_tool() {
  local name="$1"; local args_json="${2:-{}}"
  case "$name" in
    web_search)
      local q; q=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('query',''))" 2>/dev/null)
      web_search "$q" 5 ;;
    read_file)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      [[ -f "$p" ]] && cat "$p" || echo "File not found: $p" ;;
    write_file)
      local p c
      p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      c=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('content',''))" 2>/dev/null)
      echo "$c" > "$p" && echo "Written: $p" ;;
    run_code)
      local lang code
      lang=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('language','python'))" 2>/dev/null)
      code=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('code',''))" 2>/dev/null)
      case "$lang" in
        python|py) echo "$code" | python3 2>&1 ;;
        bash|sh)   echo "$code" | bash 2>&1 ;;
        js|node)   echo "$code" | node 2>&1 ;;
        *) echo "Unsupported language: $lang" ;;
      esac ;;
    list_dir)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path','.'))" 2>/dev/null)
      ls -la "$p" 2>&1 ;;
    get_time) date ;;
    get_sysinfo)
      echo "OS: $(uname -s -r)"
      echo "CPU: $(grep -m1 'model name' /proc/cpuinfo 2>/dev/null | cut -d: -f2 | xargs || sysctl -n machdep.cpu.brand_string 2>/dev/null || echo '?')"
      echo "RAM: $(free -h 2>/dev/null | awk '/^Mem/{print $2}' || echo '?')"
      [[ -n "$PYTHON" ]] && echo "Python: $($PYTHON --version 2>&1)"
      command -v nvidia-smi &>/dev/null && nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo "GPU: none/unknown"
      ;;
    calc)
      local expr; expr=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('expression',''))" 2>/dev/null)
      python3 -c "import math; print(eval('$expr'))" 2>&1 ;;
    download_file)
      local url sp
      url=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('url',''))" 2>/dev/null)
      sp=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('save_path','/tmp/download'))" 2>/dev/null)
      curl -sL "$url" -o "$sp" && echo "Saved: $sp" ;;
    image_info)
      local p; p=$(echo "$args_json" | python3 -c "import json,sys;d=json.loads(sys.stdin.read());print(d.get('path',''))" 2>/dev/null)
      [[ -n "$PYTHON" ]] && "$PYTHON" -c "from PIL import Image; im=Image.open('$p'); print(f'Size: {im.size}, Mode: {im.mode}')" 2>/dev/null || file "$p" ;;
    *) echo "Unknown tool: $name" ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  WEB SEARCH
# ════════════════════════════════════════════════════════════════════════════════
web_search() {
  local query="$1"; local max="${2:-5}"
  local encoded; encoded=$(python3 -c "import urllib.parse,sys; print(urllib.parse.quote(sys.argv[1]))" "$query" 2>/dev/null || echo "$query")

  if [[ "${SEARCH_ENGINE:-ddg}" == "brave" ]] && [[ -n "${BRAVE_API_KEY:-}" ]]; then
    curl -sS "https://api.search.brave.com/res/v1/web/search?q=${encoded}&count=${max}" \
      -H "Accept: application/json" \
      -H "X-Subscription-Token: $BRAVE_API_KEY" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
for r in d.get('web',{}).get('results',[])[:int('$max')]:
    print(f\"Title: {r.get('title','')}\")
    print(f\"URL: {r.get('url','')}\")
    print(f\"Snippet: {r.get('description','')[:200]}\")
    print()
" 2>/dev/null
  else
    curl -sS "https://api.duckduckgo.com/?q=${encoded}&format=json&no_redirect=1&no_html=1" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
results=[]
if d.get('AbstractText'):
    results.append({'title': d.get('Heading',''), 'url': d.get('AbstractURL',''), 'snippet': d.get('AbstractText','')})
for r in d.get('RelatedTopics',[])[:int('$max')]:
    if isinstance(r,dict) and r.get('Text'):
        results.append({'title': r.get('Text','')[:80], 'url': r.get('FirstURL',''), 'snippet': r.get('Text','')[:200]})
for r in results[:int('$max')]:
    print(f\"Title: {r['title']}\")
    print(f\"URL: {r['url']}\")
    print(f\"Snippet: {r['snippet']}\")
    print()
" 2>/dev/null
  fi
}

cmd_websearch() {
  local query="$*"
  [[ -z "$query" ]] && { read -rp "Search: " query; }
  hdr "Search: $query"
  echo ""
  web_search "$query" 10
}

# ════════════════════════════════════════════════════════════════════════════════
#  AI BACKENDS
# ════════════════════════════════════════════════════════════════════════════════
_get_persona_prompt() {
  local name="${ACTIVE_PERSONA:-default}"
  if [[ -f "$PERSONAS_DIR/$name" ]]; then cat "$PERSONAS_DIR/$name"
  elif [[ -n "${BUILTIN_PERSONAS[$name]:-}" ]]; then echo "${BUILTIN_PERSONAS[$name]}"
  else echo "${BUILTIN_PERSONAS[default]}"
  fi
}

_inject_session_history() {
  local session="${ACTIVE_SESSION:-default}"
  local sess_file="$SESSIONS_DIR/${session}.json"
  [[ ! -f "$sess_file" ]] && echo "[]" && return 0
  cat "$sess_file"
}

_save_session_turn() {
  local user_msg="$1"; local ai_msg="$2"
  local session="${ACTIVE_SESSION:-default}"
  local sess_file="$SESSIONS_DIR/${session}.json"
  [[ ! -f "$sess_file" ]] && echo "[]" > "$sess_file"
  python3 -c "
import json,sys
f='$sess_file'
hist=json.load(open(f))
hist.append({'role':'user','content':$(echo "$user_msg" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')})
hist.append({'role':'assistant','content':$(echo "$ai_msg" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')})
# Keep last 20 turns
if len(hist)>40: hist=hist[-40:]
json.dump(hist,open(f,'w'),indent=2)
" 2>/dev/null || true
}

ask_gguf() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  if [[ -z "$model" ]]; then err "No model set. Run: ai download 1  OR  ai recommended"; return 1; fi
  if [[ ! -f "$model" ]]; then err "Model file not found: $model"; return 1; fi

  if [[ "${LLAMA_BIN:-}" == "llama_cpp_python" ]]; then
    [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
    LLAMA_PROMPT="$prompt" LLAMA_MODEL="$model" LLAMA_MAX="${MAX_TOKENS:-512}" \
    LLAMA_TEMP="${TEMPERATURE:-0.7}" LLAMA_CTX="${CONTEXT_SIZE:-4096}" \
    LLAMA_GPU="${GPU_LAYERS:-0}" \
    "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from llama_cpp import Llama
except ImportError:
    print("llama-cpp-python not installed. Run: ai install-deps", file=sys.stderr); sys.exit(1)
try:
    llm = Llama(model_path=os.environ['LLAMA_MODEL'],
                n_ctx=int(os.environ['LLAMA_CTX']),
                n_gpu_layers=int(os.environ['LLAMA_GPU']),
                verbose=False)
    out = llm(os.environ['LLAMA_PROMPT'],
              max_tokens=int(os.environ['LLAMA_MAX']),
              temperature=float(os.environ['LLAMA_TEMP']),
              stream=False)
    print(out['choices'][0]['text'], end='', flush=True)
except Exception as e:
    print(f"GGUF inference error: {e}", file=sys.stderr); sys.exit(1)
PYEOF
  elif [[ -n "${LLAMA_BIN:-}" ]]; then
    # Use llama.cpp binary — show stderr so user sees errors
    "$LLAMA_BIN" -m "$model" -p "$prompt" \
      -n "${MAX_TOKENS:-512}" --temp "${TEMPERATURE:-0.7}" \
      -c "${CONTEXT_SIZE:-4096}" \
      --n-gpu-layers "${GPU_LAYERS:-0}" \
      --threads "${THREADS:-4}" -s 0 --no-display-prompt 2>&1 | \
      grep -v "^llama\|^ggml\|^system\|^model\|^\[" || true
  else
    err "llama.cpp not found. Run: ai install-deps"
    info "Or install manually: pip install llama-cpp-python"
    return 1
  fi
}

ask_pytorch() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  if [[ -z "$model" ]]; then err "No model set. Run: ai model <path>  OR  ai ttm load"; return 1; fi
  if [[ -z "$PYTHON" ]]; then err "Python not found. Run: ai install-deps"; return 1; fi
  if [[ ! -d "$model" ]]; then err "Model directory not found: $model"; return 1; fi

  MODEL_PATH="$model" PROMPT="$prompt" MAX_TOKENS="${MAX_TOKENS:-512}" \
  TEMPERATURE="${TEMPERATURE:-0.7}" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
except ImportError as e:
    print(f"Missing dependency: {e}\nRun: ai install-deps", file=sys.stderr); sys.exit(1)

mp     = os.environ['MODEL_PATH']
prompt = os.environ['PROMPT']
maxt   = int(os.environ.get('MAX_TOKENS', 512))
temp   = float(os.environ.get('TEMPERATURE', 0.7))
device = 'cuda' if torch.cuda.is_available() else 'cpu'
dtype  = torch.float16 if device == 'cuda' else torch.float32

try:
    tok = AutoTokenizer.from_pretrained(mp, trust_remote_code=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    mdl = AutoModelForCausalLM.from_pretrained(mp, torch_dtype=dtype,
            low_cpu_mem_usage=True, trust_remote_code=True)
    mdl = mdl.to(device).eval()

    # Apply chat template if available, else use raw prompt
    if hasattr(tok, 'apply_chat_template') and tok.chat_template:
        messages = [{"role": "user", "content": prompt}]
        input_ids = tok.apply_chat_template(messages, return_tensors='pt',
                                            add_generation_prompt=True).to(device)
    else:
        input_ids = tok(prompt, return_tensors='pt').input_ids.to(device)

    gen_kwargs = dict(
        max_new_tokens=maxt,
        do_sample=(temp > 0),
        pad_token_id=tok.eos_token_id,
        eos_token_id=tok.eos_token_id,
    )
    if temp > 0:
        gen_kwargs['temperature'] = temp
        gen_kwargs['top_p'] = 0.9

    with torch.no_grad():
        out = mdl.generate(input_ids, **gen_kwargs)

    new_tokens = out[0][input_ids.shape[1]:]
    text = tok.decode(new_tokens, skip_special_tokens=True).strip()
    print(text, flush=True)
except FileNotFoundError:
    print(f"Model not found: {mp}", file=sys.stderr)
    sys.exit(1)
except Exception as e:
    import traceback
    print(f"PyTorch inference error: {e}", file=sys.stderr)
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)
PYEOF
}

ask_openai() {
  local prompt="$1"
  [[ -z "${OPENAI_API_KEY:-}" ]] && { err "OPENAI_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-gpt-4o}"
  local sys_prompt; sys_prompt=$(_get_persona_prompt)

  local messages_json
  messages_json=$(python3 -c "
import json,sys
sys_p=$(echo "$sys_prompt" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')
user_p=$(echo "$prompt" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')
msgs=[{'role':'system','content':sys_p},{'role':'user','content':user_p}]
print(json.dumps(msgs))
" 2>/dev/null)

  local body; body=$(python3 -c "
import json
body={'model':'${model}','messages':${messages_json},'max_tokens':${MAX_TOKENS},'temperature':${TEMPERATURE},'stream':False}
print(json.dumps(body))
" 2>/dev/null)

  curl -sS https://api.openai.com/v1/chat/completions \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -H "Content-Type: application/json" \
    -d "$body" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['choices'][0]['message']['content'],end='',flush=True)
" 2>/dev/null
}

ask_claude() {
  local prompt="$1"
  [[ -z "${ANTHROPIC_API_KEY:-}" ]] && { err "ANTHROPIC_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-claude-sonnet-4-5}"
  local sys_prompt; sys_prompt=$(_get_persona_prompt)

  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)
  local sys_content; sys_content=$(echo "$sys_prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)

  curl -sS https://api.anthropic.com/v1/messages \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "Content-Type: application/json" \
    -d "{\"model\":\"$model\",\"max_tokens\":$MAX_TOKENS,\"system\":$sys_content,\"messages\":[{\"role\":\"user\",\"content\":$user_content}]}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['content'][0]['text'],end='',flush=True)
" 2>/dev/null
}

ask_gemini() {
  local prompt="$1"
  [[ -z "${GEMINI_API_KEY:-}" ]] && { err "GEMINI_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-gemini-2.0-flash}"
  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)

  curl -sS "https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=$GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"contents\":[{\"parts\":[{\"text\":$user_content}]}]}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['candidates'][0]['content']['parts'][0]['text'],end='',flush=True)
" 2>/dev/null
}

ask_hf() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && { err "No model set"; return 1; }
  local hf_key="${HF_TOKEN:-}"
  local auth_header=""
  [[ -n "$hf_key" ]] && auth_header="-H \"Authorization: Bearer $hf_key\""
  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)
  curl -sS "https://api-inference.huggingface.co/models/${model}" \
    ${hf_key:+-H "Authorization: Bearer $hf_key"} \
    -H "Content-Type: application/json" \
    -d "{\"inputs\":$user_content,\"parameters\":{\"max_new_tokens\":$MAX_TOKENS,\"temperature\":$TEMPERATURE}}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if isinstance(d,list): print(d[0].get('generated_text',''),end='')
elif isinstance(d,dict): print(d.get('generated_text',str(d)),end='')
" 2>/dev/null
}

_auto_detect_backend() {
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && {
    [[ -n "${OPENAI_API_KEY:-}" ]] && { echo "openai"; return; }
    [[ -n "${ANTHROPIC_API_KEY:-}" ]] && { echo "claude"; return; }
    [[ -n "${GEMINI_API_KEY:-}" ]] && { echo "gemini"; return; }
    echo ""; return
  }
  [[ "$model" == gpt-* || "$model" == o1* || "$model" == o3* ]] && { echo "openai"; return; }
  [[ "$model" == claude-* ]] && { echo "claude"; return; }
  [[ "$model" == gemini-* ]] && { echo "gemini"; return; }
  # gguf detection — all conditions in a single bracket to avoid || short-circuit bug
  if [[ "$model" == *.gguf || "$model" == *Q4_K* || "$model" == *Q5_K* || \
        "$model" == *Q8_0* || "$model" == *Q4_0* || "$model" == *IQ4* ]]; then
    echo "gguf"; return
  fi
  [[ -f "$model" ]] && { echo "gguf"; return; }          # any local file → gguf
  [[ -d "$model" && -f "$model/config.json" ]] && { echo "pytorch"; return; }
  # HuggingFace repo id (org/name format, no local path)
  if [[ "$model" == */* && ! -d "$model" ]]; then
    echo "hf"; return
  fi
  # Fallback: if API key available use it
  [[ -n "${OPENAI_API_KEY:-}" ]] && { echo "openai"; return; }
  [[ -n "${ANTHROPIC_API_KEY:-}" ]] && { echo "claude"; return; }
  [[ -n "${GEMINI_API_KEY:-}" ]] && { echo "gemini"; return; }
  echo "gguf"
}

_maybe_inject_search() {
  local prompt="$1"
  [[ "$WEB_SEARCH_ENABLED" != "1" ]] && { echo "$prompt"; return; }
  local backend; backend=$(_auto_detect_backend)
  # OpenAI tool calling handles its own search
  [[ "$backend" == "openai" ]] && { echo "$prompt"; return; }
  # Detect search-worthy keywords
  if echo "$prompt" | grep -qiE 'latest|current|today|2024|2025|2026|news|price|who is|what is the status|recent|now|trending'; then
    local search_terms; search_terms=$(echo "$prompt" | sed 's/[?!.,]//g' | tr '[:upper:]' '[:lower:]' | \
      sed 's/what is//g;s/who is//g;s/tell me about//g;s/the latest//g' | \
      awk '{for(i=1;i<=NF&&i<=6;i++) printf $i" "; print ""}' | xargs)
    local results; results=$(web_search "$search_terms" 3 2>/dev/null)
    if [[ -n "$results" ]]; then
      echo "[Web search results for context:]
$results

[User question:] $prompt"
      return
    fi
  fi
  echo "$prompt"
}

dispatch_ask() {
  local prompt="$1"
  local backend="${ACTIVE_BACKEND:-}"
  [[ -z "$backend" ]] && backend=$(_auto_detect_backend)

  # No backend at all — give clear diagnostic
  if [[ -z "$backend" ]]; then
    err "No model or API key configured."
    echo ""
    echo -e "${BCYAN}Quick setup:${R}"
    echo "  ai keys set OPENAI_API_KEY sk-...        (OpenAI GPT-4)"
    echo "  ai keys set ANTHROPIC_API_KEY sk-ant-... (Claude)"
    echo "  ai keys set GEMINI_API_KEY AIza...       (Gemini)"
    echo "  ai download 1                             (tiny local model, any CPU)"
    echo "  ai recommended                            (browse 28 curated models)"
    return 1
  fi

  # Auto-inject web search if needed
  local enriched_prompt; enriched_prompt=$(_maybe_inject_search "$prompt")

  local response="" rc=0
  case "$backend" in
    gguf)      response=$(ask_gguf "$enriched_prompt");   rc=$? ;;
    pytorch)   response=$(ask_pytorch "$enriched_prompt"); rc=$? ;;
    openai)    response=$(ask_openai "$enriched_prompt");  rc=$? ;;
    claude)    response=$(ask_claude "$enriched_prompt");  rc=$? ;;
    gemini)    response=$(ask_gemini "$enriched_prompt");  rc=$? ;;
    hf)        response=$(ask_hf "$enriched_prompt");      rc=$? ;;
    diffusers)
      cmd_imagine "$enriched_prompt"
      response="[Image generated]"
      ;;
    *)
      if [[ -n "${OPENAI_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="openai"; response=$(ask_openai "$enriched_prompt"); rc=$?
      elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="claude"; response=$(ask_claude "$enriched_prompt"); rc=$?
      elif [[ -n "${GEMINI_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="gemini"; response=$(ask_gemini "$enriched_prompt"); rc=$?
      else
        err "Unknown backend '$backend'. Run: ai status"
        return 1
      fi
      ;;
  esac

  if [[ $rc -ne 0 || -z "$response" ]]; then
    err "No response from backend '$backend'."
    [[ -z "${ACTIVE_MODEL:-}" ]] && echo "  Hint: no model set. Run: ai recommended"
    [[ "$backend" == "pytorch" && ! -d "${ACTIVE_MODEL:-x}" ]] && \
      echo "  Hint: model dir not found (${ACTIVE_MODEL:-not set}). Run: ai ttm pretrain"
    [[ "$backend" == "gguf" && ! -f "${ACTIVE_MODEL:-x}" ]] && \
      echo "  Hint: model file not found (${ACTIVE_MODEL:-not set}). Run: ai download 1"
    return 1
  fi

  echo "$response"
  log_history "user" "$prompt"
  log_history "assistant" "$response"
  _save_session_turn "$prompt" "$response"
  [[ -n "${CURRENT_CHAT_FILE:-}" ]] && _chat_append "assistant" "$response"

  # Background TTM batch train if enabled
  [[ "${TTM_AUTO_TRAIN:-0}" == "1" ]] && { _ttm_train_batch &>/dev/null & disown; } 2>/dev/null || true
}

# ════════════════════════════════════════════════════════════════════════════════
#  CANVAS — Code workspace with AI assistance
# ════════════════════════════════════════════════════════════════════════════════
cmd_canvas() {
  local sub="${1:-status}"; shift || true
  case "$sub" in
    new)
      local name="${1:-canvas_$(date +%H%M%S)}"; local lang="${2:-python}"
      local file="$CANVAS_DIR/${name}.${lang}"
      touch "$file"; CANVAS_ACTIVE="$file"; save_config
      ok "Canvas: $file"
      ;;
    open)
      local f="${1:-}"; [[ -z "$f" ]] && { err "File required"; return 1; }
      [[ ! -f "$f" ]] && { err "Not found: $f"; return 1; }
      CANVAS_ACTIVE="$f"; save_config; ok "Canvas: $f"
      ;;
    edit)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      "${EDITOR:-nano}" "$CANVAS_ACTIVE"
      ;;
    show)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      hdr "Canvas: $CANVAS_ACTIVE"
      cat -n "$CANVAS_ACTIVE"
      ;;
    run)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local ext="${CANVAS_ACTIVE##*.}"
      case "$ext" in
        py|python) python3 "$CANVAS_ACTIVE" ;;
        sh|bash)   bash "$CANVAS_ACTIVE" ;;
        js|ts)     node "$CANVAS_ACTIVE" 2>/dev/null || npx ts-node "$CANVAS_ACTIVE" ;;
        c)         gcc "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        cpp)       g++ "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        *)         err "Unknown extension: $ext" ;;
      esac
      ;;
    ask)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local task="$*"
      [[ -z "$task" ]] && { read -rp "Task: " task; }
      local current_code=""
      [[ -s "$CANVAS_ACTIVE" ]] && current_code=$(cat "$CANVAS_ACTIVE")
      local prompt
      if [[ -z "$current_code" ]]; then
        prompt="Write code for this task. Return ONLY the code, no explanation, no markdown fences.

Task: $task"
      else
        prompt="Here is the current code:
\`\`\`
$current_code
\`\`\`

Modify it for this task. Return ONLY the complete updated code, no explanation, no markdown fences.

Task: $task"
      fi
      info "Generating code..."
      local result; result=$(dispatch_ask "$prompt" 2>/dev/null)
      # Strip markdown fences
      result=$(echo "$result" | sed 's/^```[a-z]*$//' | sed 's/^```$//')
      echo "$result" > "$CANVAS_ACTIVE"
      ok "Canvas updated. Lines: $(wc -l < "$CANVAS_ACTIVE")"
      cat -n "$CANVAS_ACTIVE"
      ;;
    diff)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      git diff "$CANVAS_ACTIVE" 2>/dev/null || diff /dev/null "$CANVAS_ACTIVE"
      ;;
    save)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local dest="${1:-$AI_OUTPUT_DIR/$(basename "$CANVAS_ACTIVE")}"
      cp "$CANVAS_ACTIVE" "$dest"; ok "Saved: $dest"
      ;;
    list)
      hdr "Canvas files"
      for f in "$CANVAS_DIR"/*; do
        [[ -f "$f" ]] || continue
        local lines; lines=$(wc -l < "$f")
        local active=""
        [[ "$f" == "$CANVAS_ACTIVE" ]] && active=" ${BGREEN}◀ active${R}"
        printf "  %-40s %4d lines%b\n" "$(basename "$f")" "$lines" "$active"
      done
      ;;
    close) CANVAS_ACTIVE=""; save_config; ok "Canvas closed" ;;
    status)
      [[ -n "$CANVAS_ACTIVE" ]] && ok "Active: $CANVAS_ACTIVE ($(wc -l < "$CANVAS_ACTIVE" 2>/dev/null || echo 0) lines)" || info "No active canvas"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  FINE-TUNING PIPELINE
# ════════════════════════════════════════════════════════════════════════════════
cmd_finetune() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    prepare)
      local data="${1:-}"; [[ -z "$data" ]] && { err "Data file required"; return 1; }
      [[ ! -f "$data" ]] && { err "Not found: $data"; return 1; }
      local out="$FINETUNE_DIR/dataset.jsonl"
      info "Preparing dataset from $data..."
      "$PYTHON" - <<PYEOF
import json, re
data_file = "$data"
out_file = "$out"
records = []
with open(data_file) as f:
    content = f.read()
# Try JSONL
try:
    for line in content.splitlines():
        line = line.strip()
        if not line: continue
        obj = json.loads(line)
        if isinstance(obj, dict):
            txt = obj.get('text') or (obj.get('instruction','') + '\n' + obj.get('output',''))
            records.append({'text': txt})
    print(f"Loaded {len(records)} JSONL records")
except:
    # Try Q&A format
    pairs = re.split(r'### Human:', content)
    for p in pairs:
        if '### Assistant:' in p:
            parts = p.split('### Assistant:', 1)
            q = parts[0].strip(); a = parts[1].strip()
            if q and a:
                records.append({'text': f'### Human: {q}\n### Assistant: {a}'})
    if not records:
        # Plain text: split into chunks
        chunks = [content[i:i+512] for i in range(0,len(content),512)]
        records = [{'text': c} for c in chunks if c.strip()]
    print(f"Prepared {len(records)} records from plain text/QA")

with open(out_file, 'w') as f:
    for r in records:
        f.write(json.dumps(r) + '\n')
print(f"Saved: {out_file}")
PYEOF
      ;;
    start)
      local base="${1:-}"; local data="${2:-$FINETUNE_DIR/dataset.jsonl}"; local out="${3:-$FINETUNE_DIR/finetuned_$(date +%Y%m%d_%H%M%S)}"
      [[ -z "$base" ]] && { err "Base model required"; return 1; }
      [[ ! -f "$data" ]] && { err "Dataset not found: $data. Run: ai finetune prepare <data>"; return 1; }
      info "Starting LoRA fine-tune: $base → $out"
      mkdir -p "$out"
      BASE_MODEL="$base" DATA_FILE="$data" OUT_DIR="$out" "$PYTHON" - <<'PYEOF'
import os, json, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    from trl import SFTTrainer
except ImportError as e:
    print(f"Missing: {e}\nRun: ai install-deps"); sys.exit(1)
base = os.environ['BASE_MODEL']
data_file = os.environ['DATA_FILE']
out_dir   = os.environ['OUT_DIR']
tokenizer = AutoTokenizer.from_pretrained(base)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
lora = LoraConfig(task_type=TaskType.CAUSAL_LM,r=16,lora_alpha=32,lora_dropout=0.05,target_modules=["q_proj","k_proj","v_proj","o_proj"])
model = get_peft_model(model, lora)
records=[]
with open(data_file) as f:
    for line in f:
        obj=json.loads(line); txt=obj.get('text','')
        if txt: records.append({'text':txt})
ds = Dataset.from_list(records)
device='cuda' if torch.cuda.is_available() else 'cpu'
model=model.to(device)
args=TrainingArguments(output_dir=out_dir,num_train_epochs=3,per_device_train_batch_size=1,gradient_accumulation_steps=4,learning_rate=2e-4,fp16=(device=='cuda'),logging_steps=20,save_steps=200,save_total_limit=2,report_to='none')
trainer=SFTTrainer(model=model,args=args,train_dataset=ds,tokenizer=tokenizer,dataset_text_field='text',max_seq_length=512)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"Saved: {out_dir}")
PYEOF
      ;;
    merge)
      local adapter="${1:-}"; local base="${2:-}"; local out="${3:-${1}_merged}"
      [[ -z "$adapter" || -z "$base" ]] && { err "Usage: ai finetune merge <adapter> <base> [out]"; return 1; }
      info "Merging adapter into base model..."
      ADAPTER="$adapter" BASE="$base" OUT="$out" "$PYTHON" - <<'PYEOF'
import os,torch
from transformers import AutoTokenizer
from peft import PeftModel, AutoPeftModelForCausalLM
base=os.environ['BASE']; adapter=os.environ['ADAPTER']; out=os.environ['OUT']
model=AutoPeftModelForCausalLM.from_pretrained(adapter,torch_dtype=torch.float16)
merged=model.merge_and_unload()
merged.save_pretrained(out)
tok=AutoTokenizer.from_pretrained(base)
tok.save_pretrained(out)
print(f"Merged: {out}")
PYEOF
      ;;
    quantize)
      local model="${1:-}"; local quant="${2:-Q4_K_M}"
      [[ -z "$model" ]] && { err "Model path required"; return 1; }
      local script="$HOME/llama.cpp/convert_hf_to_gguf.py"
      [[ ! -f "$script" ]] && { err "llama.cpp not found at $HOME/llama.cpp"; return 1; }
      local out="${model}_${quant}.gguf"
      info "Quantizing to $quant..."
      "$PYTHON" "$script" "$model" --outfile "$out" --outtype "${quant,,}" && ok "GGUF: $out"
      ;;
    status)
      hdr "Fine-tune Status"
      [[ -f "$FINETUNE_DIR/dataset.jsonl" ]] && echo "  Dataset: $(wc -l < "$FINETUNE_DIR/dataset.jsonl") records" || echo "  Dataset: none"
      ls -td "$FINETUNE_DIR"/finetuned_*/ 2>/dev/null | head -5 | while read -r d; do
        echo "  Model: $(basename "$d") ($(du -sh "$d" 2>/dev/null | cut -f1))"
      done
      ;;
    *) echo "Usage: ai finetune prepare|start|merge|quantize|status" ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  IMAGE GENERATION
# ════════════════════════════════════════════════════════════════════════════════

# ════════════════════════════════════════════════════════════════════════════════
#  MODEL MANAGEMENT
# ════════════════════════════════════════════════════════════════════════════════
cmd_canvas() {
  local sub="${1:-status}"; shift || true
  case "$sub" in
    new)
      local name="${1:-canvas_$(date +%H%M%S)}"; local lang="${2:-python}"
      local file="$CANVAS_DIR/${name}.${lang}"
      touch "$file"; CANVAS_ACTIVE="$file"; save_config
      ok "Canvas: $file"
      ;;
    open)
      local f="${1:-}"; [[ -z "$f" ]] && { err "File required"; return 1; }
      [[ ! -f "$f" ]] && { err "Not found: $f"; return 1; }
      CANVAS_ACTIVE="$f"; save_config; ok "Canvas: $f"
      ;;
    edit)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      "${EDITOR:-nano}" "$CANVAS_ACTIVE"
      ;;
    show)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      hdr "Canvas: $CANVAS_ACTIVE"
      cat -n "$CANVAS_ACTIVE"
      ;;
    run)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local ext="${CANVAS_ACTIVE##*.}"
      case "$ext" in
        py|python) python3 "$CANVAS_ACTIVE" ;;
        sh|bash)   bash "$CANVAS_ACTIVE" ;;
        js|ts)     node "$CANVAS_ACTIVE" 2>/dev/null || npx ts-node "$CANVAS_ACTIVE" ;;
        c)         gcc "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        cpp)       g++ "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        *)         err "Unknown extension: $ext" ;;
      esac
      ;;
    ask)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local task="$*"
      [[ -z "$task" ]] && { read -rp "Task: " task; }
      local current_code=""
      [[ -s "$CANVAS_ACTIVE" ]] && current_code=$(cat "$CANVAS_ACTIVE")
      local prompt
      if [[ -z "$current_code" ]]; then
        prompt="Write code for this task. Return ONLY the code, no explanation, no markdown fences.

Task: $task"
      else
        prompt="Here is the current code:
\`\`\`
$current_code
\`\`\`

Modify it for this task. Return ONLY the complete updated code, no explanation, no markdown fences.

Task: $task"
      fi
      info "Generating code..."
      local result; result=$(dispatch_ask "$prompt" 2>/dev/null)
      # Strip markdown fences
      result=$(echo "$result" | sed 's/^```[a-z]*$//' | sed 's/^```$//')
      echo "$result" > "$CANVAS_ACTIVE"
      ok "Canvas updated. Lines: $(wc -l < "$CANVAS_ACTIVE")"
      cat -n "$CANVAS_ACTIVE"
      ;;
    diff)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      git diff "$CANVAS_ACTIVE" 2>/dev/null || diff /dev/null "$CANVAS_ACTIVE"
      ;;
    save)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local dest="${1:-$AI_OUTPUT_DIR/$(basename "$CANVAS_ACTIVE")}"
      cp "$CANVAS_ACTIVE" "$dest"; ok "Saved: $dest"
      ;;
    list)
      hdr "Canvas files"
      for f in "$CANVAS_DIR"/*; do
        [[ -f "$f" ]] || continue
        local lines; lines=$(wc -l < "$f")
        local active=""
        [[ "$f" == "$CANVAS_ACTIVE" ]] && active=" ${BGREEN}◀ active${R}"
        printf "  %-40s %4d lines%b\n" "$(basename "$f")" "$lines" "$active"
      done
      ;;
    close) CANVAS_ACTIVE=""; save_config; ok "Canvas closed" ;;
    status)
      [[ -n "$CANVAS_ACTIVE" ]] && ok "Active: $CANVAS_ACTIVE ($(wc -l < "$CANVAS_ACTIVE" 2>/dev/null || echo 0) lines)" || info "No active canvas"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  FINE-TUNING PIPELINE
# ════════════════════════════════════════════════════════════════════════════════
_save_session_turn() {
  local user_msg="$1"; local ai_msg="$2"
  local session="${ACTIVE_SESSION:-default}"
  local sess_file="$SESSIONS_DIR/${session}.json"
  [[ ! -f "$sess_file" ]] && echo "[]" > "$sess_file"
  python3 -c "
import json,sys
f='$sess_file'
hist=json.load(open(f))
hist.append({'role':'user','content':$(echo "$user_msg" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')})
hist.append({'role':'assistant','content':$(echo "$ai_msg" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))')})
# Keep last 20 turns
if len(hist)>40: hist=hist[-40:]
json.dump(hist,open(f,'w'),indent=2)
" 2>/dev/null || true
}

ask_gguf() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  if [[ -z "$model" ]]; then err "No model set. Run: ai download 1  OR  ai recommended"; return 1; fi
  if [[ ! -f "$model" ]]; then err "Model file not found: $model"; return 1; fi

  if [[ "${LLAMA_BIN:-}" == "llama_cpp_python" ]]; then
    [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
    LLAMA_PROMPT="$prompt" LLAMA_MODEL="$model" LLAMA_MAX="${MAX_TOKENS:-512}" \
    LLAMA_TEMP="${TEMPERATURE:-0.7}" LLAMA_CTX="${CONTEXT_SIZE:-4096}" \
    LLAMA_GPU="${GPU_LAYERS:-0}" \
    "$PYTHON" - <<'PYEOF'
import os, sys
try:
    from llama_cpp import Llama
except ImportError:
    print("llama-cpp-python not installed. Run: ai install-deps", file=sys.stderr); sys.exit(1)
try:
    llm = Llama(model_path=os.environ['LLAMA_MODEL'],
                n_ctx=int(os.environ['LLAMA_CTX']),
                n_gpu_layers=int(os.environ['LLAMA_GPU']),
                verbose=False)
    out = llm(os.environ['LLAMA_PROMPT'],
              max_tokens=int(os.environ['LLAMA_MAX']),
              temperature=float(os.environ['LLAMA_TEMP']),
              stream=False)
    print(out['choices'][0]['text'], end='', flush=True)
except Exception as e:
    print(f"GGUF inference error: {e}", file=sys.stderr); sys.exit(1)
PYEOF
  elif [[ -n "${LLAMA_BIN:-}" ]]; then
    # Use llama.cpp binary — show stderr so user sees errors
    "$LLAMA_BIN" -m "$model" -p "$prompt" \
      -n "${MAX_TOKENS:-512}" --temp "${TEMPERATURE:-0.7}" \
      -c "${CONTEXT_SIZE:-4096}" \
      --n-gpu-layers "${GPU_LAYERS:-0}" \
      --threads "${THREADS:-4}" -s 0 --no-display-prompt 2>&1 | \
      grep -v "^llama\|^ggml\|^system\|^model\|^\[" || true
  else
    err "llama.cpp not found. Run: ai install-deps"
    info "Or install manually: pip install llama-cpp-python"
    return 1
  fi
}

ask_pytorch() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  if [[ -z "$model" ]]; then err "No model set. Run: ai model <path>  OR  ai ttm load"; return 1; fi
  if [[ -z "$PYTHON" ]]; then err "Python not found. Run: ai install-deps"; return 1; fi
  if [[ ! -d "$model" ]]; then err "Model directory not found: $model"; return 1; fi

  MODEL_PATH="$model" PROMPT="$prompt" MAX_TOKENS="${MAX_TOKENS:-512}" \
  TEMPERATURE="${TEMPERATURE:-0.7}" "$PYTHON" - <<'PYEOF'
import os, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
except ImportError as e:
    print(f"Missing dependency: {e}\nRun: ai install-deps", file=sys.stderr); sys.exit(1)

mp     = os.environ['MODEL_PATH']
prompt = os.environ['PROMPT']
maxt   = int(os.environ.get('MAX_TOKENS', 512))
temp   = float(os.environ.get('TEMPERATURE', 0.7))
device = 'cuda' if torch.cuda.is_available() else 'cpu'
dtype  = torch.float16 if device == 'cuda' else torch.float32

try:
    tok = AutoTokenizer.from_pretrained(mp, trust_remote_code=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    mdl = AutoModelForCausalLM.from_pretrained(mp, torch_dtype=dtype,
            low_cpu_mem_usage=True, trust_remote_code=True)
    mdl = mdl.to(device).eval()

    # Apply chat template if available, else use raw prompt
    if hasattr(tok, 'apply_chat_template') and tok.chat_template:
        messages = [{"role": "user", "content": prompt}]
        input_ids = tok.apply_chat_template(messages, return_tensors='pt',
                                            add_generation_prompt=True).to(device)
    else:
        input_ids = tok(prompt, return_tensors='pt').input_ids.to(device)

    gen_kwargs = dict(
        max_new_tokens=maxt,
        do_sample=(temp > 0),
        pad_token_id=tok.eos_token_id,
        eos_token_id=tok.eos_token_id,
    )
    if temp > 0:
        gen_kwargs['temperature'] = temp
        gen_kwargs['top_p'] = 0.9

    with torch.no_grad():
        out = mdl.generate(input_ids, **gen_kwargs)

    new_tokens = out[0][input_ids.shape[1]:]
    text = tok.decode(new_tokens, skip_special_tokens=True).strip()
    print(text, flush=True)
except FileNotFoundError:
    print(f"Model not found: {mp}", file=sys.stderr)
    sys.exit(1)
except Exception as e:
    import traceback
    print(f"PyTorch inference error: {e}", file=sys.stderr)
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)
PYEOF
}

ask_openai() {
  local prompt="$1"
  [[ -z "${OPENAI_API_KEY:-}" ]] && { err "OPENAI_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-gpt-4o}"
  local sys_prompt; sys_prompt=$(_get_persona_prompt)

  local messages_json
  messages_json=$(python3 -c "
import json,sys
sys_p=$(echo "$sys_prompt" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')
user_p=$(echo "$prompt" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')
msgs=[{'role':'system','content':sys_p},{'role':'user','content':user_p}]
print(json.dumps(msgs))
" 2>/dev/null)

  local body; body=$(python3 -c "
import json
body={'model':'${model}','messages':${messages_json},'max_tokens':${MAX_TOKENS},'temperature':${TEMPERATURE},'stream':False}
print(json.dumps(body))
" 2>/dev/null)

  curl -sS https://api.openai.com/v1/chat/completions \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -H "Content-Type: application/json" \
    -d "$body" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['choices'][0]['message']['content'],end='',flush=True)
" 2>/dev/null
}

ask_claude() {
  local prompt="$1"
  [[ -z "${ANTHROPIC_API_KEY:-}" ]] && { err "ANTHROPIC_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-claude-sonnet-4-5}"
  local sys_prompt; sys_prompt=$(_get_persona_prompt)

  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)
  local sys_content; sys_content=$(echo "$sys_prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)

  curl -sS https://api.anthropic.com/v1/messages \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "Content-Type: application/json" \
    -d "{\"model\":\"$model\",\"max_tokens\":$MAX_TOKENS,\"system\":$sys_content,\"messages\":[{\"role\":\"user\",\"content\":$user_content}]}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['content'][0]['text'],end='',flush=True)
" 2>/dev/null
}

ask_gemini() {
  local prompt="$1"
  [[ -z "${GEMINI_API_KEY:-}" ]] && { err "GEMINI_API_KEY not set"; return 1; }
  local model="${ACTIVE_MODEL:-gemini-2.0-flash}"
  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)

  curl -sS "https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=$GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"contents\":[{\"parts\":[{\"text\":$user_content}]}]}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if 'error' in d: print(f\"Error: {d['error']['message']}\",file=sys.stderr)
else: print(d['candidates'][0]['content']['parts'][0]['text'],end='',flush=True)
" 2>/dev/null
}

ask_hf() {
  local prompt="$1"
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && { err "No model set"; return 1; }
  local hf_key="${HF_TOKEN:-}"
  local auth_header=""
  [[ -n "$hf_key" ]] && auth_header="-H \"Authorization: Bearer $hf_key\""
  local user_content; user_content=$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))' 2>/dev/null)
  curl -sS "https://api-inference.huggingface.co/models/${model}" \
    ${hf_key:+-H "Authorization: Bearer $hf_key"} \
    -H "Content-Type: application/json" \
    -d "{\"inputs\":$user_content,\"parameters\":{\"max_new_tokens\":$MAX_TOKENS,\"temperature\":$TEMPERATURE}}" 2>/dev/null | \
    python3 -c "
import json,sys
d=json.load(sys.stdin)
if isinstance(d,list): print(d[0].get('generated_text',''),end='')
elif isinstance(d,dict): print(d.get('generated_text',str(d)),end='')
" 2>/dev/null
}

_auto_detect_backend() {
  local model="${ACTIVE_MODEL:-}"
  [[ -z "$model" ]] && {
    [[ -n "${OPENAI_API_KEY:-}" ]] && { echo "openai"; return; }
    [[ -n "${ANTHROPIC_API_KEY:-}" ]] && { echo "claude"; return; }
    [[ -n "${GEMINI_API_KEY:-}" ]] && { echo "gemini"; return; }
    echo ""; return
  }
  [[ "$model" == gpt-* || "$model" == o1* || "$model" == o3* ]] && { echo "openai"; return; }
  [[ "$model" == claude-* ]] && { echo "claude"; return; }
  [[ "$model" == gemini-* ]] && { echo "gemini"; return; }
  # gguf detection — all conditions in a single bracket to avoid || short-circuit bug
  if [[ "$model" == *.gguf || "$model" == *Q4_K* || "$model" == *Q5_K* || \
        "$model" == *Q8_0* || "$model" == *Q4_0* || "$model" == *IQ4* ]]; then
    echo "gguf"; return
  fi
  [[ -f "$model" ]] && { echo "gguf"; return; }          # any local file → gguf
  [[ -d "$model" && -f "$model/config.json" ]] && { echo "pytorch"; return; }
  # HuggingFace repo id (org/name format, no local path)
  if [[ "$model" == */* && ! -d "$model" ]]; then
    echo "hf"; return
  fi
  # Fallback: if API key available use it
  [[ -n "${OPENAI_API_KEY:-}" ]] && { echo "openai"; return; }
  [[ -n "${ANTHROPIC_API_KEY:-}" ]] && { echo "claude"; return; }
  [[ -n "${GEMINI_API_KEY:-}" ]] && { echo "gemini"; return; }
  echo "gguf"
}

_maybe_inject_search() {
  local prompt="$1"
  [[ "$WEB_SEARCH_ENABLED" != "1" ]] && { echo "$prompt"; return; }
  local backend; backend=$(_auto_detect_backend)
  # OpenAI tool calling handles its own search
  [[ "$backend" == "openai" ]] && { echo "$prompt"; return; }
  # Detect search-worthy keywords
  if echo "$prompt" | grep -qiE 'latest|current|today|2024|2025|2026|news|price|who is|what is the status|recent|now|trending'; then
    local search_terms; search_terms=$(echo "$prompt" | sed 's/[?!.,]//g' | tr '[:upper:]' '[:lower:]' | \
      sed 's/what is//g;s/who is//g;s/tell me about//g;s/the latest//g' | \
      awk '{for(i=1;i<=NF&&i<=6;i++) printf $i" "; print ""}' | xargs)
    local results; results=$(web_search "$search_terms" 3 2>/dev/null)
    if [[ -n "$results" ]]; then
      echo "[Web search results for context:]
$results

[User question:] $prompt"
      return
    fi
  fi
  echo "$prompt"
}

dispatch_ask() {
  local prompt="$1"
  local backend="${ACTIVE_BACKEND:-}"
  [[ -z "$backend" ]] && backend=$(_auto_detect_backend)

  # No backend at all — give clear diagnostic
  if [[ -z "$backend" ]]; then
    err "No model or API key configured."
    echo ""
    echo -e "${BCYAN}Quick setup:${R}"
    echo "  ai keys set OPENAI_API_KEY sk-...        (OpenAI GPT-4)"
    echo "  ai keys set ANTHROPIC_API_KEY sk-ant-... (Claude)"
    echo "  ai keys set GEMINI_API_KEY AIza...       (Gemini)"
    echo "  ai download 1                             (tiny local model, any CPU)"
    echo "  ai recommended                            (browse 28 curated models)"
    return 1
  fi

  # Auto-inject web search if needed
  local enriched_prompt; enriched_prompt=$(_maybe_inject_search "$prompt")

  local response="" rc=0
  case "$backend" in
    gguf)      response=$(ask_gguf "$enriched_prompt");   rc=$? ;;
    pytorch)   response=$(ask_pytorch "$enriched_prompt"); rc=$? ;;
    openai)    response=$(ask_openai "$enriched_prompt");  rc=$? ;;
    claude)    response=$(ask_claude "$enriched_prompt");  rc=$? ;;
    gemini)    response=$(ask_gemini "$enriched_prompt");  rc=$? ;;
    hf)        response=$(ask_hf "$enriched_prompt");      rc=$? ;;
    diffusers)
      cmd_imagine "$enriched_prompt"
      response="[Image generated]"
      ;;
    *)
      if [[ -n "${OPENAI_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="openai"; response=$(ask_openai "$enriched_prompt"); rc=$?
      elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="claude"; response=$(ask_claude "$enriched_prompt"); rc=$?
      elif [[ -n "${GEMINI_API_KEY:-}" ]]; then
        ACTIVE_BACKEND="gemini"; response=$(ask_gemini "$enriched_prompt"); rc=$?
      else
        err "Unknown backend '$backend'. Run: ai status"
        return 1
      fi
      ;;
  esac

  if [[ $rc -ne 0 || -z "$response" ]]; then
    err "No response from backend '$backend'."
    [[ -z "${ACTIVE_MODEL:-}" ]] && echo "  Hint: no model set. Run: ai recommended"
    [[ "$backend" == "pytorch" && ! -d "${ACTIVE_MODEL:-x}" ]] && \
      echo "  Hint: model dir not found (${ACTIVE_MODEL:-not set}). Run: ai ttm pretrain"
    [[ "$backend" == "gguf" && ! -f "${ACTIVE_MODEL:-x}" ]] && \
      echo "  Hint: model file not found (${ACTIVE_MODEL:-not set}). Run: ai download 1"
    return 1
  fi

  echo "$response"
  log_history "user" "$prompt"
  log_history "assistant" "$response"
  _save_session_turn "$prompt" "$response"
  [[ -n "${CURRENT_CHAT_FILE:-}" ]] && _chat_append "assistant" "$response"

  # Background TTM batch train if enabled
  [[ "${TTM_AUTO_TRAIN:-0}" == "1" ]] && { _ttm_train_batch &>/dev/null & disown; } 2>/dev/null || true
}

# ════════════════════════════════════════════════════════════════════════════════
#  CANVAS — Code workspace with AI assistance
# ════════════════════════════════════════════════════════════════════════════════
cmd_canvas() {
  local sub="${1:-status}"; shift || true
  case "$sub" in
    new)
      local name="${1:-canvas_$(date +%H%M%S)}"; local lang="${2:-python}"
      local file="$CANVAS_DIR/${name}.${lang}"
      touch "$file"; CANVAS_ACTIVE="$file"; save_config
      ok "Canvas: $file"
      ;;
    open)
      local f="${1:-}"; [[ -z "$f" ]] && { err "File required"; return 1; }
      [[ ! -f "$f" ]] && { err "Not found: $f"; return 1; }
      CANVAS_ACTIVE="$f"; save_config; ok "Canvas: $f"
      ;;
    edit)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      "${EDITOR:-nano}" "$CANVAS_ACTIVE"
      ;;
    show)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      hdr "Canvas: $CANVAS_ACTIVE"
      cat -n "$CANVAS_ACTIVE"
      ;;
    run)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local ext="${CANVAS_ACTIVE##*.}"
      case "$ext" in
        py|python) python3 "$CANVAS_ACTIVE" ;;
        sh|bash)   bash "$CANVAS_ACTIVE" ;;
        js|ts)     node "$CANVAS_ACTIVE" 2>/dev/null || npx ts-node "$CANVAS_ACTIVE" ;;
        c)         gcc "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        cpp)       g++ "$CANVAS_ACTIVE" -o /tmp/canvas_out && /tmp/canvas_out ;;
        *)         err "Unknown extension: $ext" ;;
      esac
      ;;
    ask)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local task="$*"
      [[ -z "$task" ]] && { read -rp "Task: " task; }
      local current_code=""
      [[ -s "$CANVAS_ACTIVE" ]] && current_code=$(cat "$CANVAS_ACTIVE")
      local prompt
      if [[ -z "$current_code" ]]; then
        prompt="Write code for this task. Return ONLY the code, no explanation, no markdown fences.

Task: $task"
      else
        prompt="Here is the current code:
\`\`\`
$current_code
\`\`\`

Modify it for this task. Return ONLY the complete updated code, no explanation, no markdown fences.

Task: $task"
      fi
      info "Generating code..."
      local result; result=$(dispatch_ask "$prompt" 2>/dev/null)
      # Strip markdown fences
      result=$(echo "$result" | sed 's/^```[a-z]*$//' | sed 's/^```$//')
      echo "$result" > "$CANVAS_ACTIVE"
      ok "Canvas updated. Lines: $(wc -l < "$CANVAS_ACTIVE")"
      cat -n "$CANVAS_ACTIVE"
      ;;
    diff)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      git diff "$CANVAS_ACTIVE" 2>/dev/null || diff /dev/null "$CANVAS_ACTIVE"
      ;;
    save)
      [[ -z "$CANVAS_ACTIVE" ]] && { err "No active canvas"; return 1; }
      local dest="${1:-$AI_OUTPUT_DIR/$(basename "$CANVAS_ACTIVE")}"
      cp "$CANVAS_ACTIVE" "$dest"; ok "Saved: $dest"
      ;;
    list)
      hdr "Canvas files"
      for f in "$CANVAS_DIR"/*; do
        [[ -f "$f" ]] || continue
        local lines; lines=$(wc -l < "$f")
        local active=""
        [[ "$f" == "$CANVAS_ACTIVE" ]] && active=" ${BGREEN}◀ active${R}"
        printf "  %-40s %4d lines%b\n" "$(basename "$f")" "$lines" "$active"
      done
      ;;
    close) CANVAS_ACTIVE=""; save_config; ok "Canvas closed" ;;
    status)
      [[ -n "$CANVAS_ACTIVE" ]] && ok "Active: $CANVAS_ACTIVE ($(wc -l < "$CANVAS_ACTIVE" 2>/dev/null || echo 0) lines)" || info "No active canvas"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  FINE-TUNING PIPELINE
# ════════════════════════════════════════════════════════════════════════════════
cmd_finetune() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    prepare)
      local data="${1:-}"; [[ -z "$data" ]] && { err "Data file required"; return 1; }
      [[ ! -f "$data" ]] && { err "Not found: $data"; return 1; }
      local out="$FINETUNE_DIR/dataset.jsonl"
      info "Preparing dataset from $data..."
      "$PYTHON" - <<PYEOF
import json, re
data_file = "$data"
out_file = "$out"
records = []
with open(data_file) as f:
    content = f.read()
# Try JSONL
try:
    for line in content.splitlines():
        line = line.strip()
        if not line: continue
        obj = json.loads(line)
        if isinstance(obj, dict):
            txt = obj.get('text') or (obj.get('instruction','') + '\n' + obj.get('output',''))
            records.append({'text': txt})
    print(f"Loaded {len(records)} JSONL records")
except:
    # Try Q&A format
    pairs = re.split(r'### Human:', content)
    for p in pairs:
        if '### Assistant:' in p:
            parts = p.split('### Assistant:', 1)
            q = parts[0].strip(); a = parts[1].strip()
            if q and a:
                records.append({'text': f'### Human: {q}\n### Assistant: {a}'})
    if not records:
        # Plain text: split into chunks
        chunks = [content[i:i+512] for i in range(0,len(content),512)]
        records = [{'text': c} for c in chunks if c.strip()]
    print(f"Prepared {len(records)} records from plain text/QA")

with open(out_file, 'w') as f:
    for r in records:
        f.write(json.dumps(r) + '\n')
print(f"Saved: {out_file}")
PYEOF
      ;;
    start)
      local base="${1:-}"; local data="${2:-$FINETUNE_DIR/dataset.jsonl}"; local out="${3:-$FINETUNE_DIR/finetuned_$(date +%Y%m%d_%H%M%S)}"
      [[ -z "$base" ]] && { err "Base model required"; return 1; }
      [[ ! -f "$data" ]] && { err "Dataset not found: $data. Run: ai finetune prepare <data>"; return 1; }
      info "Starting LoRA fine-tune: $base → $out"
      mkdir -p "$out"
      BASE_MODEL="$base" DATA_FILE="$data" OUT_DIR="$out" "$PYTHON" - <<'PYEOF'
import os, json, sys
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    from trl import SFTTrainer
except ImportError as e:
    print(f"Missing: {e}\nRun: ai install-deps"); sys.exit(1)
base = os.environ['BASE_MODEL']
data_file = os.environ['DATA_FILE']
out_dir   = os.environ['OUT_DIR']
tokenizer = AutoTokenizer.from_pretrained(base)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
lora = LoraConfig(task_type=TaskType.CAUSAL_LM,r=16,lora_alpha=32,lora_dropout=0.05,target_modules=["q_proj","k_proj","v_proj","o_proj"])
model = get_peft_model(model, lora)
records=[]
with open(data_file) as f:
    for line in f:
        obj=json.loads(line); txt=obj.get('text','')
        if txt: records.append({'text':txt})
ds = Dataset.from_list(records)
device='cuda' if torch.cuda.is_available() else 'cpu'
model=model.to(device)
args=TrainingArguments(output_dir=out_dir,num_train_epochs=3,per_device_train_batch_size=1,gradient_accumulation_steps=4,learning_rate=2e-4,fp16=(device=='cuda'),logging_steps=20,save_steps=200,save_total_limit=2,report_to='none')
trainer=SFTTrainer(model=model,args=args,train_dataset=ds,tokenizer=tokenizer,dataset_text_field='text',max_seq_length=512)
trainer.train()
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print(f"Saved: {out_dir}")
PYEOF
      ;;
    merge)
      local adapter="${1:-}"; local base="${2:-}"; local out="${3:-${1}_merged}"
      [[ -z "$adapter" || -z "$base" ]] && { err "Usage: ai finetune merge <adapter> <base> [out]"; return 1; }
      info "Merging adapter into base model..."
      ADAPTER="$adapter" BASE="$base" OUT="$out" "$PYTHON" - <<'PYEOF'
import os,torch
from transformers import AutoTokenizer
from peft import PeftModel, AutoPeftModelForCausalLM
base=os.environ['BASE']; adapter=os.environ['ADAPTER']; out=os.environ['OUT']
model=AutoPeftModelForCausalLM.from_pretrained(adapter,torch_dtype=torch.float16)
merged=model.merge_and_unload()
merged.save_pretrained(out)
tok=AutoTokenizer.from_pretrained(base)
tok.save_pretrained(out)
print(f"Merged: {out}")
PYEOF
      ;;
    quantize)
      local model="${1:-}"; local quant="${2:-Q4_K_M}"
      [[ -z "$model" ]] && { err "Model path required"; return 1; }
      local script="$HOME/llama.cpp/convert_hf_to_gguf.py"
      [[ ! -f "$script" ]] && { err "llama.cpp not found at $HOME/llama.cpp"; return 1; }
      local out="${model}_${quant}.gguf"
      info "Quantizing to $quant..."
      "$PYTHON" "$script" "$model" --outfile "$out" --outtype "${quant,,}" && ok "GGUF: $out"
      ;;
    status)
      hdr "Fine-tune Status"
      [[ -f "$FINETUNE_DIR/dataset.jsonl" ]] && echo "  Dataset: $(wc -l < "$FINETUNE_DIR/dataset.jsonl") records" || echo "  Dataset: none"
      ls -td "$FINETUNE_DIR"/finetuned_*/ 2>/dev/null | head -5 | while read -r d; do
        echo "  Model: $(basename "$d") ($(du -sh "$d" 2>/dev/null | cut -f1))"
      done
      ;;
    *) echo "Usage: ai finetune prepare|start|merge|quantize|status" ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  IMAGE GENERATION
# ════════════════════════════════════════════════════════════════════════════════
cmd_imagine() {
  local prompt="" out="" steps=30 size="1024x1024"
  local mode="txt2img" init_img="" strength="0.75" lora=""
  local args=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --out)      out="$2";      shift 2 ;;
      --steps)    steps="$2";   shift 2 ;;
      --size)     size="$2";    shift 2 ;;
      --img2img)  mode="img2img"; init_img="$2"; shift 2 ;;
      --inpaint)  mode="inpaint"; init_img="$2"; shift 2 ;;
      --strength) strength="$2"; shift 2 ;;
      --lora)     lora="$2";    shift 2 ;;
      *) args+=("$1"); shift ;;
    esac
  done
  [[ ${#args[@]} -gt 0 ]] && prompt="${args[*]}"
  [[ -z "$prompt" ]] && { read -rp "Image prompt: " prompt; }
  [[ -z "$out" ]] && out="$AI_OUTPUT_DIR/generated_$(date +%Y%m%d_%H%M%S).png"
  mkdir -p "$(dirname "$out")"

  # Try OpenAI DALL-E for txt2img first
  if [[ -n "${OPENAI_API_KEY:-}" && "$mode" == "txt2img" ]]; then
    local dall_e_size="1024x1024"
    local url
    url=$(curl -sS https://api.openai.com/v1/images/generations \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"dall-e-3\",\"prompt\":$(echo "$prompt" | python3 -c 'import json,sys;print(json.dumps(sys.stdin.read().strip()))'),\"n\":1,\"size\":\"$dall_e_size\"}" 2>/dev/null | \
      python3 -c "import json,sys;d=json.load(sys.stdin);print(d['data'][0]['url'])" 2>/dev/null)
    [[ -n "$url" ]] && curl -sL "$url" -o "$out" && ok "Image (DALL-E 3): $out" && return 0
  fi

  # Local diffusers (SDXL / FLUX / SD2)
  [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
  local model="${ACTIVE_MODEL:-stabilityai/stable-diffusion-xl-base-1.0}"

  # Use _imggen_v2 for SDXL/FLUX with img2img/inpaint support
  _imggen_v2 "$prompt" "$mode" "$init_img" "$strength"
}

cmd_chat_interactive() {
  local chat_name="${CURRENT_CHAT_NAME:-}"
  hdr "AI Chat — Session: $ACTIVE_SESSION"
  [[ -n "$chat_name" ]] && info "Chat log: $CURRENT_CHAT_FILE"
  echo "  Commands: /quit /clear /session <n> /persona <n> /model <m>"
  echo ""

  while true; do
    printf "${BCYAN}${B}You: ${R}"
    local input; read -r input || break
    [[ -z "$input" ]] && continue

    case "$input" in
      /quit|/exit|/q) break ;;
      /clear)
        echo "[]" > "$SESSIONS_DIR/${ACTIVE_SESSION}.json"
        info "History cleared"
        ;;
      /session*)
        local n="${input#/session }"; ACTIVE_SESSION="$n"; save_config; info "Session: $n"
        ;;
      /persona*)
        local n="${input#/persona }"; ACTIVE_PERSONA="$n"; save_config; info "Persona: $n"
        ;;
      /model*)
        local m="${input#/model }"; ACTIVE_MODEL="$m"; save_config; info "Model: $m"
        ;;
      /save)
        info "Chat saved: $CURRENT_CHAT_FILE"
        ;;
      *)
        [[ -n "$CURRENT_CHAT_FILE" ]] && _chat_append "user" "$input"
        printf "${BGREEN}${B}AI: ${R}"
        dispatch_ask "$input"
        echo ""
        ;;
    esac
  done
}
cmd_list_models() {
  hdr "Downloaded Models"
  local found=0
  while IFS= read -r f; do
    [[ -f "$f" ]] || continue; found=1
    local size; size=$(du -sh "$f" | cut -f1)
    local active=""; [[ "$f" == "$ACTIVE_MODEL" ]] && active=" ${BGREEN}◀ active${R}"
    printf "  ${B}%-50s${R} %6s%b\n" "$(basename "$f")" "$size" "$active"
  done < <(find "$MODELS_DIR" -name "*.gguf" 2>/dev/null)
  for d in "$MODELS_DIR"/*/; do
    [[ -d "$d" && -f "$d/config.json" ]] || continue; found=1
    local active=""; [[ "${d%/}" == "$ACTIVE_MODEL" ]] && active=" ${BGREEN}◀ active${R}"
    printf "  ${B}%-50s${R} (pytorch)%b\n" "$(basename "$d")" "$active"
  done
  [[ $found -eq 0 ]] && dim "  No models. Try: ai recommended download 1"
}

cmd_download() {
  local repo="${1:-}"; local file_filter="${2:-}"
  [[ -z "$repo" ]] && { read -rp "HuggingFace repo (user/model): " repo; }
  [[ -z "$repo" ]] && { err "Repo required"; return 1; }
  if [[ "$repo" =~ ^sk-|^hf_|^AIza ]]; then
    local key_type="OPENAI_API_KEY"
    [[ "$repo" =~ ^hf_ ]] && key_type="HF_TOKEN"
    [[ "$repo" =~ ^AIza ]] && key_type="GEMINI_API_KEY"
    _set_key "$key_type" "$repo"; return 0
  fi
  mkdir -p "$MODELS_DIR"
  if [[ -z "$file_filter" ]]; then
    local files
    files=$(curl -sS --max-time 30 "https://huggingface.co/api/models/${repo}" 2>/dev/null | \
      python3 -c "
import json,sys
d=json.load(sys.stdin)
gguf=[s['rfilename'] for s in d.get('siblings',[]) if s['rfilename'].endswith('.gguf')]
q4=[f for f in gguf if 'Q4_K_M' in f or 'q4_k_m' in f]
print(q4[0] if q4 else (gguf[0] if gguf else ''))
" 2>/dev/null)
    [[ -n "$files" ]] && file_filter="$files"
  fi
  if [[ -n "$file_filter" ]]; then
    local url="https://huggingface.co/${repo}/resolve/main/${file_filter}"
    local dest="$MODELS_DIR/$(basename "$file_filter")"
    info "Downloading $file_filter..."
    curl -L --progress-bar "$url" ${HF_TOKEN:+-H "Authorization: Bearer $HF_TOKEN"} -o "$dest"
    ok "Downloaded: $dest"
    ACTIVE_MODEL="$dest"; ACTIVE_BACKEND="gguf"; save_config
  else
    info "Downloading full repo $repo (PyTorch)..."
    HF_REPO="$repo" HF_DEST="$MODELS_DIR/$repo" HF_TOKEN_VAL="${HF_TOKEN:-}" "$PYTHON" - <<'PYEOF'
import os,sys
try:
    from huggingface_hub import snapshot_download
except ImportError:
    print("huggingface_hub not installed"); sys.exit(1)
os.makedirs(os.environ['HF_DEST'], exist_ok=True)
snapshot_download(repo_id=os.environ['HF_REPO'], local_dir=os.environ['HF_DEST'],
                  token=os.environ.get('HF_TOKEN_VAL') or None)
print(f"Downloaded: {os.environ['HF_DEST']}")
PYEOF
    ACTIVE_MODEL="$MODELS_DIR/$repo"; ACTIVE_BACKEND="pytorch"; save_config
  fi
}

cmd_recommended() {
  local sub="${1:-list}"; shift || true
  case "$sub" in
    download)
      local n="${1:-}"; [[ -z "$n" ]] && { err "Number required"; return 1; }
      local entry="${RECOMMENDED_MODELS[$n]:-}"; [[ -z "$entry" ]] && { err "No model #$n"; return 1; }
      cmd_download "$(echo "$entry" | cut -d'|' -f1)" ;;
    use)
      local n="${1:-}"; [[ -z "$n" ]] && { err "Number required"; return 1; }
      local entry="${RECOMMENDED_MODELS[$n]:-}"; [[ -z "$entry" ]] && { err "No model #$n"; return 1; }
      ACTIVE_MODEL="$(echo "$entry"|cut -d'|' -f1)"; ACTIVE_BACKEND="$(echo "$entry"|cut -d'|' -f2)"
      save_config; ok "Model: $ACTIVE_MODEL" ;;
    *)
      hdr "Recommended Models"
      for key in $(seq 1 14); do
        local entry="${RECOMMENDED_MODELS[$key]:-}"; [[ -z "$entry" ]] && continue
        printf "  ${B}%2d.${R} %-16s ${DIM}%-40s${R} %s\n" "$key" \
          "$(echo "$entry"|cut -d'|' -f3)" "$(echo "$entry"|cut -d'|' -f1)" "$(echo "$entry"|cut -d'|' -f4)"
      done
      echo ""; echo "  Download: ${B}ai recommended download <N>${R}"
      ;;
  esac
}

cmd_search_models() {
  local query="$*"; [[ -z "$query" ]] && { read -rp "Search: " query; }
  info "Searching: $query"
  curl -sS --max-time 30 "https://huggingface.co/api/models?search=${query// /+}&limit=15&sort=downloads" 2>/dev/null | \
    python3 -c "
import json,sys
for i,m in enumerate(json.load(sys.stdin),1):
    print(f'  {i:2}. {m.get(\"modelId\",\"?\")}')
    print(f'      ↓{m.get(\"downloads\",0):,}  ♥{m.get(\"likes\",0)}  [{', '.join(m.get(\"tags\",[])[:3])}]')
" 2>/dev/null
}

cmd_upload() {
  local path="${1:-}"; local repo="${2:-}"; local msg="${3:-upload via ai-cli}"
  [[ -z "$path" || -z "$repo" ]] && { err "Usage: ai upload <path> <user/repo>"; return 1; }
  [[ ! -e "$path" ]] && { err "Not found: $path"; return 1; }
  local hf_key="${HF_TOKEN:-}"; [[ -z "$hf_key" ]] && { err "HF_TOKEN not set"; return 1; }
  HF_TOKEN_VAL="$hf_key" P="$path" REPO="$repo" MSG="$msg" "$PYTHON" - <<'PYEOF'
import os,sys
try:
    from huggingface_hub import HfApi
except ImportError:
    print("huggingface_hub not installed"); sys.exit(1)
api=HfApi(token=os.environ['HF_TOKEN_VAL'])
repo=os.environ['REPO']; p=os.environ['P']; msg=os.environ['MSG']
try: api.create_repo(repo_id=repo,exist_ok=True,private=False)
except: pass
if os.path.isdir(p): api.upload_folder(folder_path=p,repo_id=repo,commit_message=msg)
else: api.upload_file(path_or_fileobj=p,path_in_repo=os.path.basename(p),repo_id=repo,commit_message=msg)
print(f"Uploaded: https://huggingface.co/{repo}")
PYEOF
}

cmd_model_info() {
  local m="${1:-$ACTIVE_MODEL}"; [[ -z "$m" ]] && { err "No model"; return 1; }
  hdr "Model: $m"
  if [[ -f "$m" ]]; then
    echo "  File: $m  ($(du -sh "$m"|cut -f1))"
  elif [[ -d "$m" && -f "$m/config.json" ]]; then
    python3 -c "import json; [print(f'  {k}: {v}') for k,v in list(json.load(open('$m/config.json')).items())[:15]]" 2>/dev/null
  else
    curl -sS --max-time 30 "https://huggingface.co/api/models/$m" 2>/dev/null | python3 -c "
import json,sys; d=json.load(sys.stdin)
print(f'  ID: {d.get(\"modelId\",\"?\")}')
print(f'  Downloads: {d.get(\"downloads\",0):,}')
print(f'  Tags: {', '.join(d.get(\"tags\",[])[:5])}')
" 2>/dev/null
  fi
}

# ════════════════════════════════════════════════════════════════════════════════
#  INSTALL / UNINSTALL
# ════════════════════════════════════════════════════════════════════════════════
cmd_install_deps() {
  local force=0 cpu_only=0 no_torch=0 windows_help=0
  for a in "$@"; do
    [[ "$a" == "--force"    ]] && force=1
    [[ "$a" == "--cpu-only" ]] && cpu_only=1
    [[ "$a" == "--no-torch" ]] && no_torch=1
    [[ "$a" == "--windows"  ]] && windows_help=1
  done

  # Windows 10 setup instructions
  if [[ $windows_help -eq 1 ]] || [[ $IS_WINDOWS -eq 1 && $force -eq 0 && -z "$PYTHON" ]]; then
    hdr "AI CLI v${VERSION} — Windows 10 Setup"
    echo ""
    echo -e "${BCYAN}Prerequisites (install in order):${R}"
    echo ""
    echo "  1. Python 3.11+ for Windows"
    echo "     https://www.python.org/downloads/windows/"
    echo "     Tick: 'Add Python to PATH' during install"
    echo ""
    echo "  2. Git for Windows (includes Git Bash)"
    echo "     https://git-scm.com/download/win"
    echo ""
    echo "  3. Run in Git Bash:"
    echo "     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
    echo "     pip install transformers tokenizers accelerate safetensors datasets"
    echo "     pip install openai anthropic google-generativeai peft trl huggingface_hub"
    echo "     pip install llama-cpp-python"
    echo ""
    echo -e "${BCYAN}Recommended: Use WSL2 for best compatibility${R}"
    echo "  wsl --install        (in PowerShell as Admin)"
    echo "  Then install Ubuntu, run ai install-deps inside WSL"
    echo ""
    echo -e "${BCYAN}Run in Git Bash / WSL to install (CPU-only):${R}"
    echo "  ai install-deps --cpu-only"
    echo ""
    return 0
  fi

  hdr "Installing AI CLI v${VERSION} dependencies"
  echo "  Platform: $PLATFORM | CPU-only: $([[ $cpu_only -eq 1 || $IS_WINDOWS -eq 1 ]] && echo yes || echo auto)"
  echo ""

  # System package installation
  if [[ $IS_WINDOWS -eq 0 && $IS_WSL -eq 0 ]]; then
    if command -v apt-get &>/dev/null; then
      info "Detected APT (Debian/Ubuntu/Mint)..."
      sudo apt-get update -q 2>/dev/null || true
      sudo apt-get install -y -q python3 python3-pip python3-dev git cmake \
        build-essential ffmpeg curl jq espeak libsndfile1 \
        libssl-dev libffi-dev tk-dev 2>/dev/null || true
    elif command -v pacman &>/dev/null; then
      info "Detected Pacman (Arch/Manjaro/EndeavourOS)..."
      sudo pacman -Sy --noconfirm --needed 2>/dev/null || true
      sudo pacman -S --noconfirm --needed \
        python python-pip git cmake base-devel \
        ffmpeg curl jq espeak libsndfile \
        openssl python-tkinter 2>/dev/null || true
      # AUR helper for yay or paru (optional extras)
      if command -v yay &>/dev/null; then
        yay -S --noconfirm --needed python-soundfile 2>/dev/null || true
      elif command -v paru &>/dev/null; then
        paru -S --noconfirm --needed python-soundfile 2>/dev/null || true
      fi
    elif command -v dnf &>/dev/null; then
      info "Detected DNF (Fedora/RHEL)..."
      sudo dnf install -y python3 python3-pip python3-devel git cmake \
        gcc gcc-c++ ffmpeg curl jq espeak libsndfile-devel \
        openssl-devel python3-tkinter 2>/dev/null || true
    elif command -v zypper &>/dev/null; then
      info "Detected Zypper (openSUSE)..."
      sudo zypper install -y python3 python3-pip python3-devel git cmake \
        gcc ffmpeg curl jq espeak libsndfile-devel \
        libopenssl-devel python3-tk 2>/dev/null || true
    elif command -v brew &>/dev/null; then
      info "Detected Homebrew (macOS)..."
      brew install python3 cmake ffmpeg jq espeak libsndfile 2>/dev/null || true
    else
      warn "No known package manager found. Install python3, git, ffmpeg, cmake manually."
    fi
  elif [[ $IS_WSL -eq 1 ]]; then
    info "Detected WSL — using APT..."
    sudo apt-get update -q 2>/dev/null || true
    sudo apt-get install -y -q python3 python3-pip python3-dev git cmake \
      build-essential ffmpeg curl jq espeak libsndfile1 \
      libssl-dev libffi-dev 2>/dev/null || true
  else
    info "Windows: skipping system package install (install manually if needed)"
  fi

  [[ -z "$PYTHON" ]] && { err "Python 3.10+ required. On Windows run: ai install-deps --windows"; return 1; }

  # Windows / CPU-only always use CPU torch
  local use_cpu=0
  [[ $cpu_only -eq 1 || $IS_WINDOWS -eq 1 ]] && use_cpu=1

  if [[ $no_torch -eq 0 ]]; then
    local ca; ca=$(detect_cuda_arch)
    if [[ $use_cpu -eq 1 ]] || (( ca == 0 )); then
      info "Installing PyTorch CPU (Windows/CPU-only mode)"
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu -q 2>/dev/null || \
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu --break-system-packages -q 2>/dev/null || true
    elif (( ca >= 80 )); then
      info "Installing PyTorch CUDA 12.1 (compute $ca)"
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cu121 --break-system-packages -q 2>/dev/null || true
    elif (( ca >= 75 )); then
      info "Installing PyTorch CUDA 11.8 (compute $ca)"
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cu118 --break-system-packages -q 2>/dev/null || true
    else
      info "Installing PyTorch CPU (legacy GPU)"
      "$PYTHON" -m pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu --break-system-packages -q 2>/dev/null || true
    fi
  fi

  # Core ML and API packages
  info "Installing core packages..."
  "$PYTHON" -m pip install transformers tokenizers accelerate safetensors datasets \
    optimum "huggingface_hub>=0.20" "peft>=0.7" "trl>=0.7" diffusers Pillow \
    openai anthropic google-generativeai tiktoken \
    soundfile pydub -q 2>/dev/null || \
  "$PYTHON" -m pip install transformers tokenizers accelerate safetensors datasets \
    optimum "huggingface_hub>=0.20" "peft>=0.7" "trl>=0.7" diffusers Pillow \
    openai anthropic google-generativeai tiktoken \
    soundfile pydub --break-system-packages -q 2>/dev/null || true

  # Optional packages (may fail on some platforms)
  "$PYTHON" -m pip install openai-whisper pyttsx3 -q 2>/dev/null || \
  "$PYTHON" -m pip install openai-whisper pyttsx3 --break-system-packages -q 2>/dev/null || true

  # bitsandbytes (skip on Windows — no CUDA, often fails)
  if [[ $IS_WINDOWS -eq 0 ]]; then
    "$PYTHON" -m pip install bitsandbytes -q --break-system-packages 2>/dev/null || true
  fi

  # llama-cpp-python (CPU only on Windows/no GPU)
  info "Installing llama-cpp-python..."
  local ca; ca=$(detect_cuda_arch)
  if [[ $use_cpu -eq 1 ]] || (( ca < 61 )); then
    # CPU-only build
    CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" \
      "$PYTHON" -m pip install llama-cpp-python -q 2>/dev/null || \
    "$PYTHON" -m pip install llama-cpp-python -q --break-system-packages 2>/dev/null || true
  else
    CMAKE_ARGS="-DLLAMA_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=${ca}" \
      "$PYTHON" -m pip install llama-cpp-python --no-cache-dir --force-reinstall \
      --break-system-packages -q 2>/dev/null || \
    "$PYTHON" -m pip install llama-cpp-python --break-system-packages -q 2>/dev/null || true
  fi

  echo ""
  ok "Installation complete!"
  if [[ $IS_WINDOWS -eq 1 ]]; then
    echo "  Windows 10 CPU-only mode is active"
    echo "  Quick start: ai recommended → ai ask \"Hello\""
  else
    echo "  Quick start: ai recommended → ai recommended download 1 → ai -gui"
  fi
  echo "  Start API server: ai api start"
}

cmd_uninstall() {
  if [[ $EUID -ne 0 ]]; then err "Requires sudo: sudo ai -uninstall"; return 1; fi
  echo -e "${BRED}${B}╔══════════════════════════════════════════════╗${R}"
  echo -e "${BRED}${B}║   ⚠  AI CLI UNINSTALL — CANNOT BE UNDONE   ║${R}"
  echo -e "${BRED}${B}╚══════════════════════════════════════════════╝${R}"
  echo ""
  echo "  Will remove: /usr/local/bin/ai"
  echo "  Config ($CONFIG_DIR) and models ($MODELS_DIR) are optional"
  echo ""
  read -rp "  Type CONFIRM to proceed: " confirm
  [[ "$confirm" != "CONFIRM" ]] && { info "Cancelled"; return 0; }
  read -rp "  Remove config/sessions/chats? [y/N]: " rm_cfg
  read -rp "  Remove downloaded models? [y/N]: " rm_mdl
  rm -f /usr/local/bin/ai && ok "Removed /usr/local/bin/ai"
  [[ "$rm_cfg" =~ ^[Yy]$ ]] && rm -rf "$CONFIG_DIR" && ok "Removed $CONFIG_DIR"
  [[ "$rm_mdl" =~ ^[Yy]$ ]] && rm -rf "$MODELS_DIR" && ok "Removed $MODELS_DIR"
  ok "Uninstalled."
}

# ════════════════════════════════════════════════════════════════════════════════
#  STATUS
# ════════════════════════════════════════════════════════════════════════════════
cmd_status() {
  hdr "AI CLI v${VERSION} — Status"; echo ""
  printf "  %-22s %s\n" "Platform:"   "$PLATFORM"
  printf "  %-22s %s\n" "OS:"         "$(uname -s -r 2>/dev/null || echo unknown)"
  printf "  %-22s %s\n" "Python:"     "${PYTHON:-not found}"
  printf "  %-22s %s\n" "llama.cpp:"  "${LLAMA_BIN:-not found}"
  printf "  %-22s %s\n" "ffmpeg:"     "$(command -v ffmpeg 2>/dev/null || echo 'not found')"
  printf "  %-22s %s\n" "CPU-only:"   "$([[ $CPU_ONLY_MODE -eq 1 ]] && echo 'YES (Windows/no GPU)' || echo 'no')"
  echo ""
  if command -v nvidia-smi &>/dev/null; then
    printf "  %-22s %s\n" "GPU:" "$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null|head -1)"
    printf "  %-22s %s\n" "VRAM:" "$(nvidia-smi --query-gpu=memory.total --format=csv,noheader 2>/dev/null|head -1)"
    printf "  %-22s %s\n" "Compute:" "$CUDA_ARCH"
    if (( CUDA_ARCH >= 61 )); then
      printf "  %-22s ${BGREEN}✓ CUDA supported${R}\n" "Support:"
    else
      printf "  %-22s ${BRED}✗ Legacy GPU (CPU only)${R}\n" "Support:"
    fi
  else
    printf "  %-22s %s\n" "GPU:" "none (CPU-only mode)"
  fi
  echo ""
  printf "  %-22s %s\n" "Active model:"   "${ACTIVE_MODEL:-not set}"
  printf "  %-22s %s\n" "Backend:"        "${ACTIVE_BACKEND:-auto}"
  printf "  %-22s %s\n" "Session:"        "${ACTIVE_SESSION:-default}"
  printf "  %-22s %s\n" "Persona:"        "${ACTIVE_PERSONA:-default}"
  printf "  %-22s %s\n" "Canvas:"         "${CANVAS_ACTIVE:-none}"
  printf "  %-22s %s\n" "GUI theme:"      "${GUI_THEME:-dark}"
  echo ""
  printf "  %-22s %s (v%s)\n" "TTM:"  "$TTM_AUTO_TRAIN" "$TTM_VERSION"
  printf "  %-22s %s (v%s)\n" "MTM:"  "$MTM_AUTO_TRAIN" "$MTM_VERSION"
  printf "  %-22s %s (v%s)\n" "Mtm:"  "$MMTM_AUTO_TRAIN" "$MMTM_VERSION"
  printf "  %-22s %s → %s\n" "HF sync:" "$HF_DATASET_SYNC" "$HF_DATASET_REPO"
  echo ""
  # v2.4: API server status
  local api_status="not running"
  if [[ -f "$API_PID_FILE" ]]; then
    local apid; apid=$(cat "$API_PID_FILE" 2>/dev/null)
    kill -0 "$apid" 2>/dev/null && api_status="${BGREEN}running${R} (PID $apid) on $API_HOST:$API_PORT"
  fi
  printf "  %-22s " "LLM API (v2.4):"; echo -e "$api_status"
  printf "  %-22s %s\n" "Datasets (v2.4):" "$(ls "$DATASETS_DIR" 2>/dev/null | wc -l) dataset(s)"
  echo ""
  printf "  %-22s %s\n" "Temperature:"  "$TEMPERATURE"
  printf "  %-22s %s\n" "Max tokens:"   "$MAX_TOKENS"
  printf "  %-22s %s\n" "Context:"      "$CONTEXT_SIZE"
  printf "  %-22s %s\n" "GPU layers:"   "$GPU_LAYERS"
  echo ""
  hdr "API Keys"
  for k in OPENAI_API_KEY ANTHROPIC_API_KEY GEMINI_API_KEY HF_TOKEN HF_DATASET_KEY BRAVE_API_KEY; do
    local v; v=$(eval "echo \"\${$k:-}\"")
    if [[ -n "$v" ]]; then printf "  %-24s ${BGREEN}set${R} (%s…%s)\n" "$k:" "${v:0:4}" "${v: -4}"
    else printf "  %-24s ${DIM}not set${R}\n" "$k:"; fi
  done
  echo ""
  printf "  %-22s %s\n" "GGUF models:" "$(find "$MODELS_DIR" -name "*.gguf" 2>/dev/null | wc -l)"
  printf "  %-22s %s\n" "Chat logs:"   "$(ls "$CHAT_LOGS_DIR"/*.jsonl 2>/dev/null | wc -l || echo 0)"
  printf "  %-22s %s\n" "Datasets:"    "$(ls "$DATASETS_DIR" 2>/dev/null | wc -l)"
}

# ════════════════════════════════════════════════════════════════════════════════
#  HELP
# ════════════════════════════════════════════════════════════════════════════════
show_help() {
  echo -e "${B}${BWHITE}AI CLI v${VERSION}${R} — Chat · Vision · Audio · Video · Canvas · TTM · MTM · Mtm"
  echo ""
  echo -e "${B}${BCYAN}ASKING & CHAT${R}"
  echo "  ai ask <prompt>                    — Ask a question"
  echo "  ai chat                            — Interactive chat"
  echo "  ai -C [name|auto] ask <prompt>     — Named chat (saved as JSONL)"
  echo "  ai chat-list / chat-show / chat-delete"
  echo "  ai code <prompt> [--run]           — Generate & optionally run code"
  echo "  ai pipe / review / explain / summarize / translate"
  echo ""
  echo -e "${B}${BCYAN}MEDIA${R}"
  echo "  ai audio transcribe/tts/analyze/convert/extract/ask/play/info"
  echo "  ai video analyze/transcribe/caption/extract/trim/convert/ask/summary"
  echo "  ai vision ask/ocr/caption/compare"
  echo "  ai imagine <prompt>"
  echo ""
  echo -e "${B}${BCYAN}TRAINED MODELS${R}"
  echo "  ${B}TTM${R}  (~179.35M — any GPU/CPU)      ai ttm <cmd>"
  echo "  ${B}MTM${R}  (~0.61B  — GTX 1080 fp16)     ai mtm <cmd>"
  echo "  ${B}Mtm${R}  (~1.075B — RTX 2080+ bf16)    ai Mtm <cmd>   (case sensitive!)"
  echo ""
  echo "  Commands: pretrain [c1] [c2]  enable  disable  train-now"
  echo "            upload [v]  create-repo  status  load [v]"
  echo "            set-custom1 <hf-id>   set-custom2 <hf-id>"
  echo ""
  echo "  Load shortcuts:"
  echo "    ai -TTM / ai -MTM / ai -Mtm"
  echo ""
  echo "  Pretraining datasets (6 standard + 2 custom):"
  echo "    1. TinyStories (6k)   2. CodeAlpaca (4k)   3. OpenOrca (3k)"
  echo "    4. The Stack Smol (3k) 5. FineWeb-Edu (4k) 6. Wikipedia-en (4k)"
  echo "    7. Custom 1 (user-defined HF id or local path)"
  echo "    8. Custom 2 (user-defined HF id or local path)"
  echo ""
  echo -e "${B}${BCYAN}CANVAS${R}"
  echo "  ai canvas new/open/ask/show/run/edit/diff/save/list/close"
  echo ""
  echo -e "${B}${BCYAN}MODELS${R}"
  echo "  ai model <name>  models  download  recommended  search-models  upload  model-info"
  echo "  ai model-create new/train/list/edit/info/delete/presets"
  echo ""
  echo -e "${B}${BCYAN}FINE-TUNING${R}"
  echo "  ai finetune prepare/start/merge/quantize/status"
  echo ""
  echo -e "${B}${BCYAN}SETTINGS${R}"
  echo "  ai config [key value]     ai keys [set KEY val]"
  echo "  ai session list/new/load  ai persona list/set/create"
  echo "  ai history [--search x]   ai status"
  echo "  ai install-deps [--cpu-only]"
  echo "  sudo ai -uninstall"
  echo ""
  echo -e "${B}${BCYAN}GUI${R}"
  echo "  ai -gui / ai gui   — Python curses GUI (click or keyboard)"
  echo "  Themes: dark (default) · light · hacker · matrix"
  echo "  ai config gui_theme <theme>"
  echo ""
  echo -e "${B}${BCYAN}HF DATASET SYNC${R}"
  echo "  ai config hf_dataset_sync 1   — Enable chat sync to $HF_DATASET_REPO"
  echo "  ai config dataset_key <key>   — Set write-only HF key"
}

# ════════════════════════════════════════════════════════════════════════════════
#  MISC HELPERS (personas, sessions, config, history, etc.)
# ════════════════════════════════════════════════════════════════════════════════
_set_key() {
  local var="$1"; local val="$2"; [[ -z "$val" ]] && return 0
  local tmpf; tmpf=$(mktemp)
  grep -v "^${var}=" "$KEYS_FILE" > "$tmpf" 2>/dev/null || true
  echo "${var}=\"${val}\"" >> "$tmpf"
  mv "$tmpf" "$KEYS_FILE"; chmod 600 "$KEYS_FILE"
  eval "${var}=\"${val}\""; ok "Set $var"
}
cmd_keys() {
  if [[ "${1:-}" == "set" ]]; then _set_key "${2:-}" "${3:-}"; return; fi
  hdr "API Key Status"
  for k in OPENAI_API_KEY ANTHROPIC_API_KEY GEMINI_API_KEY HF_TOKEN HF_DATASET_KEY BRAVE_API_KEY; do
    local v; v=$(eval "echo \"\${$k:-}\"")
    if [[ -n "$v" ]]; then printf "  %-22s ${BGREEN}set${R} (%s…%s)\n" "$k:" "${v:0:6}" "${v: -4}"
    else printf "  %-22s ${DIM}not set${R}\n" "$k:"; fi
  done
}
cmd_persona() {
  local sub="${1:-list}"; shift || true
  case "$sub" in
    list)
      hdr "Personas"
      for k in "${!BUILTIN_PERSONAS[@]}"; do
        local a=""; [[ "$k" == "${ACTIVE_PERSONA:-default}" ]] && a=" ${BGREEN}◀${R}"
        printf "  ${B}%-12s${R}%b\n" "$k" "$a"
      done
      for f in "$PERSONAS_DIR"/*; do
        [[ -f "$f" ]] || continue
        local a=""; [[ "$(basename "$f")" == "${ACTIVE_PERSONA:-}" ]] && a=" ${BGREEN}◀${R}"
        printf "  ${B}%-12s${R} (custom)%b\n" "$(basename "$f")" "$a"
      done ;;
    set)   ACTIVE_PERSONA="${1:-default}"; save_config; ok "Persona: $ACTIVE_PERSONA" ;;
    create)
      local n="${1:-}"; [[ -z "$n" ]] && { read -rp "Name: " n; }
      read -rp "System prompt: " p; echo "$p" > "$PERSONAS_DIR/$n"; ok "Created: $n" ;;
    edit)  "${EDITOR:-nano}" "$PERSONAS_DIR/${1:-$ACTIVE_PERSONA}" ;;
    clear) ACTIVE_PERSONA="default"; save_config; ok "Reset to default" ;;
  esac
}
cmd_session() {
  local sub="${1:-list}"; shift || true
  case "$sub" in
    list)
      hdr "Sessions"
      for f in "$SESSIONS_DIR"/*.json; do
        [[ -f "$f" ]] || continue
        local name; name=$(basename "$f" .json)
        local turns; turns=$(python3 -c "import json; print(len(json.load(open('$f')))//2)" 2>/dev/null || echo "?")
        local a=""; [[ "$name" == "$ACTIVE_SESSION" ]] && a=" ${BGREEN}◀ active${R}"
        printf "  ${B}%-25s${R} %s turns%b\n" "$name" "$turns" "$a"
      done ;;
    new)
      local n="${1:-session_$(date +%H%M%S)}"; ACTIVE_SESSION="$n"
      echo "[]" > "$SESSIONS_DIR/${n}.json"; save_config; ok "Session: $n" ;;
    load)  ACTIVE_SESSION="${1:-}"; save_config; ok "Loaded: $ACTIVE_SESSION" ;;
    delete)
      read -rp "Delete session '${1:-}'? [y/N]: " a
      [[ "$a" =~ ^[Yy]$ ]] && rm -f "$SESSIONS_DIR/${1:-}.json" && ok "Deleted" ;;
  esac
}
cmd_config() {
  if [[ $# -eq 0 ]]; then cat "$CONFIG_FILE" 2>/dev/null; return; fi
  local key="${1,,}"; local val="${2:-}"
  case "$key" in
    temperature|temp)        TEMPERATURE="$val"; save_config; ok "Temperature: $val" ;;
    max_tokens|tokens)       MAX_TOKENS="$val";  save_config ;;
    context|context_size)    CONTEXT_SIZE="$val"; save_config ;;
    gpu_layers|gpu)          GPU_LAYERS="$val";  save_config ;;
    stream)                  STREAM="$val";       save_config ;;
    tool_calling|tools)      TOOL_CALLING="$val"; save_config ;;
    web_search|websearch)    WEB_SEARCH_ENABLED="$val"; save_config ;;
    hf_dataset_sync|sync)    HF_DATASET_SYNC="$val"; save_config ;;
    ttm_auto_train)          TTM_AUTO_TRAIN="$val"; save_config ;;
    mtm_auto_train)          MTM_AUTO_TRAIN="$val"; save_config ;;
    mmtm_auto_train)         MMTM_AUTO_TRAIN="$val"; save_config ;;
    gui_theme|theme)         GUI_THEME="$val"; save_config ;;
    rlhf_auto)               RLHF_AUTO="$val"; save_config ;;
    rlhf_judge)              RLHF_JUDGE="$val"; save_config ;;
    rlhf_reward_threshold|threshold) RLHF_REWARD_THRESHOLD="$val"; save_config ;;
    rclick_enabled|rclick)   RCLICK_ENABLED="$val"; save_config ;;
    rclick_vl_model)         RCLICK_VL_MODEL="$val"; save_config ;;
    agent_max_steps)         AGENT_MAX_STEPS="$val"; save_config ;;
    agent_search_engine)     AGENT_SEARCH_ENGINE="$val"; save_config ;;
    aup_repo)                AUP_REPO="$val"; save_config ;;
    hf_dataset_key|dataset_key) HF_DATASET_KEY="$val"; save_config; ok "HF dataset key set" ;;
    api_host)                API_HOST="$val"; save_config; ok "API host: $val" ;;
    api_port)                API_PORT="$val"; save_config; ok "API port: $val" ;;
    api_key)                 API_KEY="$val";  save_config; ok "API key set" ;;
    api_cors)                API_CORS="$val"; save_config; ok "API CORS: $val" ;;
    api_share_host)          API_SHARE_HOST="$val"; save_config; ok "Share host: $val" ;;
    api_share_port)          API_SHARE_PORT="$val"; save_config; ok "Share port: $val" ;;
    api_share_rate_limit)    API_SHARE_RATE_LIMIT="$val"; save_config; ok "Rate limit: $val req/min" ;;
    cpu_only_mode|cpu_only)  CPU_ONLY_MODE="$val"; save_config; ok "CPU-only mode: $val" ;;
    multiai_rounds)          MULTIAI_ROUNDS="$val"; save_config; ok "Multi-AI rounds: $val" ;;
    multiai_save_dataset)    MULTIAI_SAVE_DATASET="$val"; save_config; ok "Multi-AI save dataset: $val" ;;
    multiai_rlhf_train)      MULTIAI_RLHF_TRAIN="$val"; save_config; ok "Multi-AI RLHF train: $val" ;;
    rclick_keybind|keybind)  RCLICK_KEYBIND="$val"; save_config; ok "RClick keybind: $val"; info "Run: ai rclick install  to apply" ;;
    rlhf_reward_threshold|threshold) RLHF_REWARD_THRESHOLD="$val"; save_config; ok "RLHF threshold: $val" ;;
    *) err "Unknown config key: '$key'" ;;
  esac
}
cmd_history() {
  local n=20 search=""
  while [[ $# -gt 0 ]]; do
    case "$1" in --n) n="$2"; shift 2 ;; --search) search="$2"; shift 2 ;; *) shift ;; esac
  done
  [[ ! -f "$LOG_FILE" ]] && { info "No history"; return; }
  if [[ -n "$search" ]]; then grep -i "$search" "$LOG_FILE" | tail -n "$n"
  else tail -n "$n" "$LOG_FILE"; fi
}
cmd_bench() {
  local prompt="${*:-Hello, how are you?}"; local runs=3; hdr "Benchmark"
  local total=0
  for (( i=1; i<=runs; i++ )); do
    local s; s=$(date +%s%N)
    dispatch_ask "$prompt" &>/dev/null
    local ms=$(( ( $(date +%s%N) - s ) / 1000000 ))
    printf "  Run %d: %dms\n" "$i" "$ms"; total=$(( total + ms ))
  done
  printf "  Average: %dms\n" $(( total / runs ))
}
# ════════════════════════════════════════════════════════════════════════════════
#  CUSTOM DATASET CREATION  (v2.4)
#  ai dataset create <name>               — Create a new dataset
#  ai dataset add <name> <prompt> <resp>  — Add a prompt/response pair
#  ai dataset add-file <name> <jsonl>     — Import from JSONL file
#  ai dataset import-csv <name> <csv>     — Import from CSV (prompt,response)
#  ai dataset list                        — List all datasets
#  ai dataset show <name> [N]             — Show last N entries
#  ai dataset delete <name>              — Delete dataset
#  ai dataset export <name> [path]        — Export to JSONL file
#  ai dataset push <name> <hf-repo>       — Push to HuggingFace
#  ai dataset from-chat [session]         — Convert chat session to dataset
#  ai dataset from-rlhf                   — Convert RLHF ratings to dataset
# ════════════════════════════════════════════════════════════════════════════════
cmd_dataset() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    create)
      local name="${1:?Usage: ai dataset create <name>}"
      local ds_dir="$DATASETS_DIR/$name"
      if [[ -d "$ds_dir" ]]; then warn "Dataset '$name' already exists"; return 1; fi
      mkdir -p "$ds_dir"
      echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
      touch "$ds_dir/data.jsonl"
      ok "Dataset '$name' created at $ds_dir"
      echo "  Add pairs:  ai dataset add $name \"<prompt>\" \"<response>\""
      echo "  Import:     ai dataset add-file $name <file.jsonl>"
      ;;
    add)
      local name="${1:?Usage: ai dataset add <name> <prompt> <response>}"
      local prompt="${2:?Provide a prompt}"; local response="${3:?Provide a response}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -d "$ds_dir" ]] && { err "Dataset '$name' not found. Create it first: ai dataset create $name"; return 1; }
      echo '{"prompt":'"$(echo "$prompt" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))")"',"response":'"$(echo "$response" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))")"'}' >> "$ds_dir/data.jsonl"
      local cnt; cnt=$(wc -l < "$ds_dir/data.jsonl")
      # Update meta count
      python3 -c "
import json,sys
m=json.load(open('$ds_dir/meta.json'))
m['count']=$cnt; m['updated']='$(date -Iseconds)'
json.dump(m,open('$ds_dir/meta.json','w'))
" 2>/dev/null || true
      ok "Added pair #$cnt to '$name'"
      ;;
    add-file)
      local name="${1:?Usage: ai dataset add-file <name> <file.jsonl>}"
      local src="${2:?Provide source JSONL file}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -d "$ds_dir" ]] && { err "Dataset '$name' not found"; return 1; }
      [[ ! -f "$src" ]] && { err "File not found: $src"; return 1; }
      local before; before=$(wc -l < "$ds_dir/data.jsonl" 2>/dev/null || echo 0)
      cat "$src" >> "$ds_dir/data.jsonl"
      local after; after=$(wc -l < "$ds_dir/data.jsonl")
      local added=$(( after - before ))
      python3 -c "import json; m=json.load(open('$ds_dir/meta.json')); m['count']=$after; json.dump(m,open('$ds_dir/meta.json','w'))" 2>/dev/null || true
      ok "Imported $added entries from $src into '$name' (total: $after)"
      ;;
    import-csv)
      local name="${1:?Usage: ai dataset import-csv <name> <file.csv>}"
      local src="${2:?Provide source CSV file}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -d "$ds_dir" ]] && { mkdir -p "$ds_dir"; echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"; touch "$ds_dir/data.jsonl"; }
      [[ ! -f "$src" ]] && { err "File not found: $src"; return 1; }
      local added
      added=$(python3 - <<PYEOF
import csv, json, sys
count = 0
with open('$src', newline='', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    with open('$ds_dir/data.jsonl', 'a', encoding='utf-8') as out:
        for row in reader:
            prompt = row.get('prompt', row.get('instruction', row.get('input', '')))
            response = row.get('response', row.get('output', row.get('answer', '')))
            if prompt and response:
                out.write(json.dumps({'prompt': prompt, 'response': response}) + '\n')
                count += 1
print(count)
PYEOF
)
      local total; total=$(wc -l < "$ds_dir/data.jsonl")
      python3 -c "import json; m=json.load(open('$ds_dir/meta.json')); m['count']=$total; json.dump(m,open('$ds_dir/meta.json','w'))" 2>/dev/null || true
      ok "Imported $added entries from CSV into '$name' (total: $total)"
      ;;
    list)
      hdr "Custom Datasets"
      local found=0
      for d in "$DATASETS_DIR"/*/; do
        [[ -f "$d/meta.json" ]] || continue
        found=1
        local meta; meta=$(cat "$d/meta.json")
        local n; n=$(echo "$meta" | python3 -c "import sys,json;m=json.load(sys.stdin);print(m.get('name','?'))" 2>/dev/null || basename "$d")
        local cnt; cnt=$(echo "$meta" | python3 -c "import sys,json;m=json.load(sys.stdin);print(m.get('count',0))" 2>/dev/null || wc -l < "$d/data.jsonl")
        local up; up=$(echo "$meta" | python3 -c "import sys,json;m=json.load(sys.stdin);print(m.get('updated',m.get('created','?'))[:10])" 2>/dev/null || echo "?")
        printf "  %-20s  %5s pairs  updated %s\n" "$n" "$cnt" "$up"
      done
      [[ $found -eq 0 ]] && info "No datasets yet. Create one: ai dataset create <name>"
      ;;
    show)
      local name="${1:?Usage: ai dataset show <name> [N]}"; local n="${2:-10}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -f "$ds_dir/data.jsonl" ]] && { err "Dataset '$name' not found"; return 1; }
      hdr "Dataset '$name' (last $n entries)"
      tail -n "$n" "$ds_dir/data.jsonl" | python3 -c "
import sys,json
for line in sys.stdin:
    try:
        e=json.loads(line)
        print(f'\033[1mPrompt:\033[0m {e.get(\"prompt\",\"\")[:120]}')
        print(f'\033[2mResponse:\033[0m {e.get(\"response\",\"\")[:200]}')
        print()
    except: pass
"
      ;;
    delete)
      local name="${1:?Usage: ai dataset delete <name>}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -d "$ds_dir" ]] && { err "Dataset '$name' not found"; return 1; }
      read -rp "Delete dataset '$name'? [y/N]: " confirm
      [[ "$confirm" =~ ^[Yy]$ ]] || { info "Cancelled"; return; }
      rm -rf "$ds_dir"; ok "Dataset '$name' deleted"
      ;;
    export)
      local name="${1:?Usage: ai dataset export <name> [output-path]}"
      local ds_dir="$DATASETS_DIR/$name"
      local out="${2:-$HOME/${name}.jsonl}"
      [[ ! -f "$ds_dir/data.jsonl" ]] && { err "Dataset '$name' not found"; return 1; }
      cp "$ds_dir/data.jsonl" "$out"
      ok "Exported '$name' to $out ($(wc -l < "$out") entries)"
      ;;
    push)
      local name="${1:?Usage: ai dataset push <name> <hf-repo>}"
      local repo="${2:?Provide HuggingFace repo (user/dataset-name)}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -f "$ds_dir/data.jsonl" ]] && { err "Dataset '$name' not found"; return 1; }
      [[ -z "${HF_TOKEN:-}" ]] && { err "HF_TOKEN not set. Run: ai keys set HF_TOKEN <token>"; return 1; }
      info "Pushing '$name' to HuggingFace hub: $repo"
      HF_TOKEN_VAL="$HF_TOKEN" DS_DIR="$ds_dir" DS_REPO="$repo" DS_NAME="$name" \
      "$PYTHON" - <<'PYEOF'
import os, json
from huggingface_hub import HfApi, create_repo
api = HfApi(token=os.environ['HF_TOKEN_VAL'])
repo = os.environ['DS_REPO']
ds_dir = os.environ['DS_DIR']
name = os.environ['DS_NAME']
try:
    create_repo(repo, repo_type="dataset", exist_ok=True, token=os.environ['HF_TOKEN_VAL'])
except Exception as e:
    pass
api.upload_file(path_or_fileobj=f"{ds_dir}/data.jsonl",
                path_in_repo="data.jsonl",
                repo_id=repo, repo_type="dataset",
                token=os.environ['HF_TOKEN_VAL'])
print(f"Pushed to https://huggingface.co/datasets/{repo}")
PYEOF
      ;;
    from-chat)
      local session="${1:-$ACTIVE_SESSION}"
      local sess_file="$SESSIONS_DIR/${session}.json"
      [[ ! -f "$sess_file" ]] && { err "Session '$session' not found"; return 1; }
      local ds_name="chat_${session}_$(date +%Y%m%d)"
      local ds_dir="$DATASETS_DIR/$ds_name"
      mkdir -p "$ds_dir"
      echo '{"name":"'"$ds_name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
      touch "$ds_dir/data.jsonl"
      local cnt
      cnt=$(python3 - <<PYEOF
import json, sys
with open('$sess_file') as f:
    data = json.load(f)
pairs = []
for i in range(len(data)):
    if data[i]['role'] == 'user' and i+1 < len(data) and data[i+1]['role'] == 'assistant':
        pairs.append({'prompt': data[i]['content'], 'response': data[i+1]['content']})
with open('$ds_dir/data.jsonl', 'w') as out:
    for p in pairs:
        out.write(json.dumps(p) + '\n')
print(len(pairs))
PYEOF
)
      python3 -c "import json; m=json.load(open('$ds_dir/meta.json')); m['count']=$cnt; json.dump(m,open('$ds_dir/meta.json','w'))" 2>/dev/null || true
      ok "Created dataset '$ds_name' from session '$session' ($cnt pairs)"
      echo "  Fine-tune: ai ttm finetune $ds_name"
      ;;
    from-rlhf)
      local min_score="${1:-4}"
      local ds_name="rlhf_preferred_$(date +%Y%m%d)"
      local ds_dir="$DATASETS_DIR/$ds_name"
      mkdir -p "$ds_dir"; touch "$ds_dir/data.jsonl"
      echo '{"name":"'"$ds_name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
      local cnt
      cnt=$(python3 - <<PYEOF
import json
count = 0
for f in ['$RLHF_RATINGS_FILE', '$RLHF_PAIRS_FILE']:
    try:
        with open(f) as inp:
            with open('$ds_dir/data.jsonl', 'a') as out:
                for line in inp:
                    try:
                        r = json.loads(line)
                        score = float(r.get('score', r.get('rating', 0)))
                        if score >= $min_score:
                            pair = {'prompt': r.get('prompt',''), 'response': r.get('response', r.get('chosen',''))}
                            if pair['prompt'] and pair['response']:
                                out.write(json.dumps(pair) + '\n')
                                count += 1
                    except: pass
    except: pass
print(count)
PYEOF
)
      python3 -c "import json; m=json.load(open('$ds_dir/meta.json')); m['count']=$cnt; json.dump(m,open('$ds_dir/meta.json','w'))" 2>/dev/null || true
      ok "Created dataset '$ds_name' from RLHF ratings >= $min_score ($cnt pairs)"
      echo "  Fine-tune: ai ttm finetune $ds_name"
      ;;
    generate)
      # Use AI to generate synthetic training data
      local name="${1:?Usage: ai dataset generate <name> <topic> [N]}"
      local topic="${2:?Provide a topic}"
      local n="${3:-50}"
      local ds_dir="$DATASETS_DIR/$name"
      [[ ! -d "$ds_dir" ]] && { mkdir -p "$ds_dir"; echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"; touch "$ds_dir/data.jsonl"; }
      info "Generating $n synthetic pairs on topic: $topic"
      local generated=0
      for (( i=1; i<=n; i++ )); do
        local prompt_q="Generate a realistic question or instruction about: $topic. Output ONLY the question/instruction, nothing else."
        local q; q=$(dispatch_ask "$prompt_q" 2>/dev/null | head -3)
        [[ -z "$q" ]] && continue
        local a; a=$(dispatch_ask "$q" 2>/dev/null)
        [[ -z "$a" ]] && continue
        echo '{"prompt":'"$(echo "$q" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))")"',"response":'"$(echo "$a" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))")"'}' >> "$ds_dir/data.jsonl"
        generated=$(( generated + 1 ))
        printf "\r  Generated: %d/%d" "$generated" "$n"
      done
      echo ""
      local total; total=$(wc -l < "$ds_dir/data.jsonl")
      python3 -c "import json; m=json.load(open('$ds_dir/meta.json')); m['count']=$total; json.dump(m,open('$ds_dir/meta.json','w'))" 2>/dev/null || true
      ok "Generated $generated pairs → '$name' (total: $total)"
      ;;
    # v2.5: text/url/file/paper → dataset
    from-text)
      _dataset_from_text "$@"
      ;;
    from-url)
      _dataset_from_url "$@"
      ;;
    from-file)
      _dataset_from_file "$@"
      ;;
    from-paper)
      _dataset_from_paper "$@"
      ;;
    *)
      hdr "Dataset Commands (v2.4+2.5)"
      echo "  ai dataset create <name>               — Create new dataset"
      echo "  ai dataset add <name> <prompt> <resp>  — Add a prompt/response pair"
      echo "  ai dataset add-file <name> <file.jsonl> — Import from JSONL"
      echo "  ai dataset import-csv <name> <file.csv> — Import from CSV"
      echo "  ai dataset list                        — List all datasets"
      echo "  ai dataset show <name> [N]             — Show last N entries"
      echo "  ai dataset delete <name>               — Delete dataset"
      echo "  ai dataset export <name> [path]        — Export to JSONL"
      echo "  ai dataset push <name> <hf-repo>       — Upload to HuggingFace"
      echo "  ai dataset from-chat [session]         — Convert chat to dataset"
      echo "  ai dataset from-rlhf [min-score]       — Convert RLHF ratings"
      echo "  ai dataset generate <name> <topic> [N] — AI-generated synthetic data"
      echo ""
      echo "  v2.5 additions:"
      echo "  ai dataset from-text <name> <text>     — Text blob → Q&A dataset"
      echo "  ai dataset from-paper <name> <arxiv-id>— arXiv paper → dataset"
      echo "  ai dataset from-url <name> <url>       — Webpage text → dataset"
      echo "  ai dataset from-file <name> <file>     — Any text file → dataset"
      echo ""
      echo "  Dataset path: $DATASETS_DIR/"
      echo ""
      echo "  Then fine-tune:  ai ttm finetune <name>"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: Dataset creation helpers — from-text, from-paper, from-url, from-file
# ════════════════════════════════════════════════════════════════════════════════
_dataset_from_text_python() {
  local name="$1" text="$2" ds_dir="$3"
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  "$PYTHON" - "$name" "$text" "$ds_dir" <<'PYEOF'
import sys, os, json, re

name, text, ds_dir = sys.argv[1], sys.argv[2], sys.argv[3]
os.makedirs(ds_dir, exist_ok=True)
data_file = os.path.join(ds_dir, 'data.jsonl')

# Split text into sentences / paragraphs
sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if len(s.strip()) > 20]
paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 30]

pairs = []
# Sentence-based Q&A
for i in range(0, len(sentences)-1, 2):
    q = f"Explain: {sentences[i][:200]}"
    a = sentences[i+1][:500] if i+1 < len(sentences) else sentences[i]
    pairs.append({'prompt': q, 'response': a})

# Paragraph-based summary
for para in paragraphs:
    pairs.append({'prompt': f"Summarize: {para[:300]}", 'response': para[:500]})
    pairs.append({'prompt': f"What does this mean: {para[:200]}", 'response': para[:500]})

with open(data_file, 'a') as f:
    for p in pairs:
        f.write(json.dumps(p) + '\n')

meta_file = os.path.join(ds_dir, 'meta.json')
if os.path.exists(meta_file):
    m = json.load(open(meta_file))
else:
    m = {'name': name, 'created': __import__('datetime').datetime.now().isoformat(), 'count': 0}
m['count'] = sum(1 for _ in open(data_file))
m['updated'] = __import__('datetime').datetime.now().isoformat()
json.dump(m, open(meta_file, 'w'), indent=2)
print(f"Generated {len(pairs)} pairs from text → {data_file}")
PYEOF
}

# Patch cmd_dataset to handle from-text, from-paper, from-url, from-file
_dataset_from_text() {
  local name="${1:?Usage: ai dataset from-text <name> <text>}"
  local text="${2:?Provide text content}"
  local ds_dir="$DATASETS_DIR/$name"
  mkdir -p "$ds_dir"
  [[ ! -f "$ds_dir/meta.json" ]] && echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
  touch "$ds_dir/data.jsonl"
  _dataset_from_text_python "$name" "$text" "$ds_dir"
}

_dataset_from_url() {
  local name="${1:?Usage: ai dataset from-url <name> <url>}"
  local url="${2:?Provide URL}"
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  local ds_dir="$DATASETS_DIR/$name"
  mkdir -p "$ds_dir"
  [[ ! -f "$ds_dir/meta.json" ]] && echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
  touch "$ds_dir/data.jsonl"
  info "Fetching: $url"
  local text
  text=$("$PYTHON" -c "
import urllib.request, html.parser, re, sys
url = '$url'
class P(html.parser.HTMLParser):
    def __init__(self):
        super().__init__(); self.text = []; self.skip = False
    def handle_starttag(self,t,a):
        if t in ('script','style','nav','footer'): self.skip = True
    def handle_endtag(self,t):
        if t in ('script','style','nav','footer'): self.skip = False
    def handle_data(self,d):
        if not self.skip and d.strip(): self.text.append(d.strip())
try:
    req = urllib.request.Request(url, headers={'User-Agent':'AI-CLI/2.5'})
    with urllib.request.urlopen(req, timeout=15) as r:
        html_content = r.read().decode('utf-8','replace')
    p = P(); p.feed(html_content)
    print(' '.join(p.text)[:10000])
except Exception as e:
    print('ERROR:'+str(e), file=sys.stderr)
    sys.exit(1)
" 2>/dev/null) || { err "Failed to fetch: $url"; return 1; }
  _dataset_from_text_python "$name" "$text" "$ds_dir"
}

_dataset_from_file() {
  local name="${1:?Usage: ai dataset from-file <name> <file>}"
  local file="${2:?Provide file path}"
  [[ ! -f "$file" ]] && { err "File not found: $file"; return 1; }
  local ds_dir="$DATASETS_DIR/$name"
  mkdir -p "$ds_dir"
  [[ ! -f "$ds_dir/meta.json" ]] && echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
  touch "$ds_dir/data.jsonl"
  info "Reading: $file"
  local text; text=$(< "$file")
  _dataset_from_text_python "$name" "$text" "$ds_dir"
}

_dataset_from_paper() {
  local name="${1:?Usage: ai dataset from-paper <name> <arxiv-id>}"
  local paper_id="${2:?Provide arXiv ID (e.g. 2301.12345)}"
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  local ds_dir="$DATASETS_DIR/$name"
  mkdir -p "$ds_dir"
  [[ ! -f "$ds_dir/meta.json" ]] && echo '{"name":"'"$name"'","created":"'"$(date -Iseconds)"'","count":0}' > "$ds_dir/meta.json"
  touch "$ds_dir/data.jsonl"
  info "Fetching arXiv paper: $paper_id"
  local abstract
  abstract=$("$PYTHON" -c "
import urllib.request, xml.etree.ElementTree as ET, sys
arxiv_id = '$paper_id'
url = f'https://export.arxiv.org/api/query?id_list={arxiv_id}'
req = urllib.request.Request(url, headers={'User-Agent':'AI-CLI/2.5'})
try:
    with urllib.request.urlopen(req, timeout=15) as r:
        data = r.read()
    ns = {'a': 'http://www.w3.org/2005/Atom'}
    root = ET.fromstring(data)
    entry = root.find('a:entry', ns)
    if entry is None: print(''); sys.exit(0)
    title = (entry.find('a:title', ns).text or '').strip()
    abstract = (entry.find('a:summary', ns).text or '').strip()
    authors = [a.find('a:name', ns).text for a in entry.findall('a:author', ns)[:5]]
    print(f'Title: {title}\nAuthors: {\", \".join(authors)}\n\nAbstract: {abstract}')
except Exception as e:
    print('ERROR:'+str(e), file=sys.stderr)
    sys.exit(1)
" 2>/dev/null) || { err "Failed to fetch paper: $paper_id"; return 1; }
  [[ -z "$abstract" ]] && { err "Paper not found: $paper_id"; return 1; }
  _dataset_from_text_python "$name" "$abstract" "$ds_dir"
  ok "Dataset from arXiv $paper_id created"
}

# ════════════════════════════════════════════════════════════════════════════════
#  UNIVERSAL LLM API SERVER  (v2.4)
#  OpenAI-compatible REST API — works with any LLM client
#  Supports: GGUF, PyTorch, OpenAI, Claude, Gemini, HF backends
#
#  ai api start [--port 8080] [--host 0.0.0.0] [--key <token>]
#  ai api stop
#  ai api status
#  ai api test
#
#  Endpoints (OpenAI-compatible):
#    GET  /v1/models
#    POST /v1/chat/completions
#    POST /v1/completions
#    GET  /health
# ════════════════════════════════════════════════════════════════════════════════
cmd_api() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    start)
      local port="$API_PORT" host="$API_HOST" key="$API_KEY"
      while [[ $# -gt 0 ]]; do
        case "$1" in
          --port)  port="$2"; shift 2 ;;
          --host)  host="$2"; shift 2 ;;
          --key)   key="$2";  shift 2 ;;
          --public) host="0.0.0.0"; shift ;;
          *) shift ;;
        esac
      done
      if [[ -f "$API_PID_FILE" ]]; then
        local old_pid; old_pid=$(cat "$API_PID_FILE" 2>/dev/null)
        if kill -0 "$old_pid" 2>/dev/null; then
          warn "API server already running (PID $old_pid) on $host:$port"
          warn "Stop it first: ai api stop"
          return 1
        fi
        rm -f "$API_PID_FILE"
      fi
      [[ -z "$PYTHON" ]] && { err "Python not found"; return 1; }
      info "Starting AI CLI LLM API server on http://${host}:${port}"
      info "OpenAI-compatible: POST /v1/chat/completions"
      [[ -n "$key" ]] && info "Auth: Bearer token required"
      [[ -z "$key" ]] && warn "No API key set — open access (localhost only is safer)"
      # Export context for the Python server
      export API_SERVER_HOST="$host"
      export API_SERVER_PORT="$port"
      export API_SERVER_KEY="$key"
      export API_SERVER_BACKEND="${ACTIVE_BACKEND:-}"
      export API_SERVER_MODEL="${ACTIVE_MODEL:-}"
      export API_SERVER_CORS="${API_CORS:-1}"
      export AI_CLI_CONFIG="$CONFIG_DIR"
      export AI_CLI_MODELS="$MODELS_DIR"
      "$PYTHON" - <<'PYEOF' &
import os, sys, json, time, threading, subprocess, shutil
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse

HOST    = os.environ.get('API_SERVER_HOST', '127.0.0.1')
PORT    = int(os.environ.get('API_SERVER_PORT', '8080'))
API_KEY = os.environ.get('API_SERVER_KEY', '')
BACKEND = os.environ.get('API_SERVER_BACKEND', '')
MODEL   = os.environ.get('API_SERVER_MODEL', '')
CORS    = os.environ.get('API_SERVER_CORS', '1') == '1'
CONFIG  = os.environ.get('AI_CLI_CONFIG', os.path.expanduser('~/.config/ai-cli'))
MODELS_DIR = os.environ.get('AI_CLI_MODELS', os.path.expanduser('~/.ai-cli/models'))

def load_keys():
    keys = {}
    kf = os.path.join(CONFIG, 'keys.env')
    if os.path.exists(kf):
        for line in open(kf):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                k, _, v = line.partition('=')
                keys[k.strip()] = v.strip().strip('"\'')
    return keys

KEYS = load_keys()

def call_backend(messages, model_override=None, max_tokens=2048, temperature=0.7, stream=False):
    """Route inference to the appropriate backend."""
    backend = BACKEND
    model = model_override or MODEL

    # Auto-detect backend from model name / keys
    if not backend:
        if KEYS.get('OPENAI_API_KEY') and (not model or 'gpt' in model.lower()):
            backend = 'openai'
        elif KEYS.get('ANTHROPIC_API_KEY') and (not model or 'claude' in model.lower()):
            backend = 'claude'
        elif KEYS.get('GEMINI_API_KEY') and (not model or 'gemini' in model.lower()):
            backend = 'gemini'
        elif model and os.path.exists(model):
            backend = 'gguf'
        else:
            backend = 'openai'  # default

    # Build prompt from messages
    prompt = ""
    system_msg = ""
    for m in messages:
        role = m.get('role', 'user')
        content = m.get('content', '')
        if role == 'system':
            system_msg = content
        elif role == 'user':
            prompt += f"User: {content}\n"
        elif role == 'assistant':
            prompt += f"Assistant: {content}\n"
    prompt += "Assistant:"

    if backend == 'openai':
        import urllib.request
        key = KEYS.get('OPENAI_API_KEY', '')
        if not key:
            return None, "OPENAI_API_KEY not set"
        mdl = model or 'gpt-4o-mini'
        payload = json.dumps({
            "model": mdl, "messages": messages,
            "max_tokens": max_tokens, "temperature": temperature,
            "stream": False
        }).encode()
        req = urllib.request.Request(
            "https://api.openai.com/v1/chat/completions",
            data=payload,
            headers={"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
        )
        try:
            with urllib.request.urlopen(req, timeout=60) as r:
                resp = json.load(r)
            return resp['choices'][0]['message']['content'], None
        except Exception as e:
            return None, str(e)

    elif backend == 'claude':
        import urllib.request
        key = KEYS.get('ANTHROPIC_API_KEY', '')
        if not key:
            return None, "ANTHROPIC_API_KEY not set"
        mdl = model or 'claude-haiku-4-5-20251001'
        msgs = [m for m in messages if m.get('role') != 'system']
        payload = json.dumps({
            "model": mdl, "max_tokens": max_tokens,
            "system": system_msg or "You are a helpful assistant.",
            "messages": msgs
        }).encode()
        req = urllib.request.Request(
            "https://api.anthropic.com/v1/messages",
            data=payload,
            headers={"x-api-key": key, "anthropic-version": "2023-06-01",
                     "Content-Type": "application/json"}
        )
        try:
            with urllib.request.urlopen(req, timeout=60) as r:
                resp = json.load(r)
            return resp['content'][0]['text'], None
        except Exception as e:
            return None, str(e)

    elif backend == 'gemini':
        import urllib.request
        key = KEYS.get('GEMINI_API_KEY', '')
        if not key:
            return None, "GEMINI_API_KEY not set"
        mdl = model or 'gemini-2.0-flash'
        parts = [{"text": f"{m.get('role','user')}: {m.get('content','')}"} for m in messages]
        payload = json.dumps({"contents": [{"parts": parts}],
                              "generationConfig": {"maxOutputTokens": max_tokens, "temperature": temperature}}).encode()
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{mdl}:generateContent?key={key}"
        req = urllib.request.Request(url, data=payload, headers={"Content-Type": "application/json"})
        try:
            with urllib.request.urlopen(req, timeout=60) as r:
                resp = json.load(r)
            return resp['candidates'][0]['content']['parts'][0]['text'], None
        except Exception as e:
            return None, str(e)

    elif backend in ('gguf', 'local'):
        # Try llama-cpp-python or llama.cpp binary
        try:
            from llama_cpp import Llama
            llm = Llama(model_path=model, n_ctx=4096, n_threads=os.cpu_count() or 4,
                        n_gpu_layers=0, verbose=False)
            out = llm.create_chat_completion(messages=messages, max_tokens=max_tokens, temperature=temperature)
            return out['choices'][0]['message']['content'], None
        except ImportError:
            pass
        # Fallback to subprocess llama.cpp binary
        llama_bins = ['llama-cli', 'llama', 'llama-run']
        llama_bin = None
        for b in llama_bins:
            if shutil.which(b):
                llama_bin = b; break
        if llama_bin and model and os.path.exists(model):
            try:
                result = subprocess.run(
                    [llama_bin, "-m", model, "-p", prompt, "--temp", str(temperature),
                     "-n", str(max_tokens), "--no-display-prompt", "-t", str(os.cpu_count() or 4)],
                    capture_output=True, text=True, timeout=120
                )
                return result.stdout.strip(), None
            except Exception as e:
                return None, str(e)
        return None, "GGUF backend: no model path or llama.cpp not found"

    elif backend == 'pytorch':
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM
            tok = AutoTokenizer.from_pretrained(model)
            mdl_obj = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.float32)
            mdl_obj.eval()
            inputs = tok(prompt, return_tensors='pt')
            with torch.no_grad():
                out = mdl_obj.generate(**inputs, max_new_tokens=max_tokens, temperature=temperature, do_sample=True)
            text = tok.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            return text.strip(), None
        except Exception as e:
            return None, str(e)

    elif backend == 'hf':
        import urllib.request
        key = KEYS.get('HF_TOKEN', '')
        hdrs = {"Content-Type": "application/json"}
        if key: hdrs["Authorization"] = f"Bearer {key}"
        payload = json.dumps({"inputs": prompt, "parameters": {"max_new_tokens": max_tokens, "temperature": temperature}}).encode()
        url = f"https://api-inference.huggingface.co/models/{model}"
        req = urllib.request.Request(url, data=payload, headers=hdrs)
        try:
            with urllib.request.urlopen(req, timeout=60) as r:
                resp = json.load(r)
            if isinstance(resp, list):
                return resp[0].get('generated_text', ''), None
            return str(resp), None
        except Exception as e:
            return None, str(e)

    return None, f"Unknown backend: {backend}"

class LLMHandler(BaseHTTPRequestHandler):
    def log_message(self, fmt, *args):
        pass  # silence default logging

    def _cors_headers(self):
        if CORS:
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')

    def _check_auth(self):
        if not API_KEY:
            return True
        auth = self.headers.get('Authorization', '')
        return auth == f'Bearer {API_KEY}'

    def do_OPTIONS(self):
        self.send_response(200)
        self._cors_headers()
        self.end_headers()

    def do_GET(self):
        path = urlparse(self.path).path
        if path == '/health':
            self._json(200, {"status": "ok", "version": "2.4.0", "backend": BACKEND or "auto", "model": MODEL or "auto"})
        elif path == '/v1/models':
            models = [{"id": MODEL or "auto", "object": "model", "owned_by": "ai-cli",
                       "backend": BACKEND or "auto"}]
            self._json(200, {"object": "list", "data": models})
        else:
            self._json(404, {"error": "Not found"})

    def do_POST(self):
        if not self._check_auth():
            self._json(401, {"error": {"message": "Invalid API key", "type": "invalid_request_error"}})
            return
        try:
            length = int(self.headers.get('Content-Length', 0))
            body = json.loads(self.rfile.read(length))
        except Exception as e:
            self._json(400, {"error": str(e)}); return

        path = urlparse(self.path).path

        if path == '/v1/chat/completions':
            messages = body.get('messages', [])
            model_req = body.get('model', MODEL)
            max_tok = int(body.get('max_tokens', 2048))
            temp = float(body.get('temperature', 0.7))
            t0 = time.time()
            text, err = call_backend(messages, model_req, max_tok, temp)
            elapsed = time.time() - t0
            if err:
                self._json(500, {"error": {"message": err, "type": "backend_error"}}); return
            resp_id = f"chatcmpl-{int(time.time()*1000)}"
            prompt_tokens = sum(len(m.get('content','').split()) for m in messages)
            completion_tokens = len((text or '').split())
            self._json(200, {
                "id": resp_id, "object": "chat.completion",
                "created": int(time.time()),
                "model": model_req or MODEL or "ai-cli",
                "choices": [{"index": 0, "message": {"role": "assistant", "content": text or ""},
                              "finish_reason": "stop"}],
                "usage": {"prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens,
                          "total_tokens": prompt_tokens + completion_tokens},
                "_meta": {"backend": BACKEND or "auto", "elapsed_s": round(elapsed, 3)}
            })

        elif path == '/v1/completions':
            prompt_text = body.get('prompt', '')
            model_req = body.get('model', MODEL)
            max_tok = int(body.get('max_tokens', 256))
            temp = float(body.get('temperature', 0.7))
            messages = [{"role": "user", "content": prompt_text}]
            text, err = call_backend(messages, model_req, max_tok, temp)
            if err:
                self._json(500, {"error": {"message": err, "type": "backend_error"}}); return
            self._json(200, {
                "id": f"cmpl-{int(time.time()*1000)}", "object": "text_completion",
                "created": int(time.time()), "model": model_req or MODEL or "ai-cli",
                "choices": [{"text": text or "", "index": 0, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": len(prompt_text.split()),
                          "completion_tokens": len((text or '').split()),
                          "total_tokens": len(prompt_text.split()) + len((text or '').split())}
            })

        elif path == '/v1/embeddings':
            # Placeholder — route to HF/OpenAI if available
            self._json(501, {"error": {"message": "Embeddings not yet supported", "type": "not_implemented"}})

        else:
            self._json(404, {"error": "Unknown endpoint"})

    def _json(self, code, data):
        body = json.dumps(data).encode()
        self.send_response(code)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Content-Length', str(len(body)))
        self._cors_headers()
        self.end_headers()
        self.wfile.write(body)

print(f"AI CLI LLM API v2.4.0 listening on http://{HOST}:{PORT}", flush=True)
print(f"  GET  http://{HOST}:{PORT}/v1/models", flush=True)
print(f"  POST http://{HOST}:{PORT}/v1/chat/completions", flush=True)
print(f"  GET  http://{HOST}:{PORT}/health", flush=True)
server = HTTPServer((HOST, PORT), LLMHandler)
server.serve_forever()
PYEOF
      local api_pid=$!
      sleep 0.8
      if kill -0 "$api_pid" 2>/dev/null; then
        echo "$api_pid" > "$API_PID_FILE"
        ok "API server running (PID $api_pid)"
        echo "  Endpoint:  http://${host}:${port}/v1/chat/completions"
        echo "  Models:    http://${host}:${port}/v1/models"
        echo "  Health:    http://${host}:${port}/health"
        echo "  Stop with: ai api stop"
      else
        err "API server failed to start. Check Python dependencies: ai install-deps"
      fi
      ;;
    stop)
      if [[ ! -f "$API_PID_FILE" ]]; then
        warn "No API server PID file found"
        return
      fi
      local pid; pid=$(cat "$API_PID_FILE" 2>/dev/null)
      if [[ -n "$pid" ]] && kill -0 "$pid" 2>/dev/null; then
        kill "$pid" && rm -f "$API_PID_FILE"
        ok "API server stopped (PID $pid)"
      else
        warn "API server not running (stale PID $pid)"
        rm -f "$API_PID_FILE"
      fi
      ;;
    status)
      if [[ -f "$API_PID_FILE" ]]; then
        local pid; pid=$(cat "$API_PID_FILE")
        if kill -0 "$pid" 2>/dev/null; then
          ok "API server running (PID $pid) on $API_HOST:$API_PORT"
          echo "  Endpoint: http://${API_HOST}:${API_PORT}/v1/chat/completions"
        else
          warn "API server not running (stale PID file)"
          rm -f "$API_PID_FILE"
        fi
      else
        info "API server not running"
        echo "  Start with: ai api start [--port 8080] [--public]"
      fi
      ;;
    test)
      local port="${1:-$API_PORT}"; local host="${2:-$API_HOST}"
      info "Testing LLM API at http://${host}:${port}"
      if ! command -v curl &>/dev/null; then err "curl required for test"; return 1; fi
      local health
      health=$(curl -sf "http://${host}:${port}/health" 2>/dev/null)
      if [[ $? -eq 0 ]]; then
        ok "Health check passed: $health"
      else
        err "Server not responding. Start it: ai api start"
        return 1
      fi
      info "Testing /v1/chat/completions..."
      local key_header=""
      [[ -n "$API_KEY" ]] && key_header="-H \"Authorization: Bearer $API_KEY\""
      local result
      result=$(curl -sf -X POST "http://${host}:${port}/v1/chat/completions" \
        -H "Content-Type: application/json" \
        ${API_KEY:+-H "Authorization: Bearer $API_KEY"} \
        -d '{"model":"auto","messages":[{"role":"user","content":"Say hello in one word."}],"max_tokens":20}' 2>/dev/null)
      if [[ $? -eq 0 ]]; then
        local text; text=$(echo "$result" | python3 -c "import sys,json;d=json.load(sys.stdin);print(d['choices'][0]['message']['content'])" 2>/dev/null)
        ok "Chat completion: $text"
      else
        err "Chat completion failed"
      fi
      ;;
    config)
      hdr "LLM API Configuration"
      printf "  %-22s %s\n" "Host:" "$API_HOST"
      printf "  %-22s %s\n" "Port:" "$API_PORT"
      printf "  %-22s %s\n" "Key:" "${API_KEY:+(set)}"
      printf "  %-22s %s\n" "CORS:" "$API_CORS"
      printf "  %-22s %s\n" "Share enabled:" "$API_SHARE_ENABLED"
      printf "  %-22s %s\n" "Share host:port:" "${API_SHARE_HOST}:${API_SHARE_PORT}"
      printf "  %-22s %s req/min\n" "Rate limit:" "$API_SHARE_RATE_LIMIT"
      printf "  %-22s %s\n" "Backend:" "${ACTIVE_BACKEND:-auto}"
      printf "  %-22s %s\n" "Model:" "${ACTIVE_MODEL:-auto}"
      echo ""
      echo "  Change: ai config api_host / api_port / api_key"
      echo "          ai config api_share_host / api_share_port / api_share_rate_limit"
      ;;

    # ── v2.4.5: Key management ────────────────────────────────────────────────
    key-gen)
      _api_key_gen "$@"
      ;;
    keys)
      local ksub="${1:-list}"; shift || true
      case "$ksub" in
        list) _api_keys_list ;;
        revoke) _api_keys_revoke "$@" ;;
        show)   _api_keys_show  "$@" ;;
        reset-count) _api_keys_reset_count "$@" ;;
        *) echo "Usage: ai api keys list|revoke <id>|show <id>|reset-count <id>" ;;
      esac
      ;;
    share)
      # Start public-facing API server accepting any valid key from the key store
      local port="${API_SHARE_PORT}" host="${API_SHARE_HOST}"
      while [[ $# -gt 0 ]]; do
        case "$1" in --port) port="$2"; shift 2 ;; --host) host="$2"; shift 2 ;; *) shift ;; esac
      done
      _api_share_start "$host" "$port"
      ;;
    unshare) _api_share_stop ;;

    *)
      hdr "LLM API Server (v2.4.5) — OpenAI-compatible"
      echo ""
      echo "  ${B}Server${R}"
      echo "  ai api start [--port 8080] [--host 0.0.0.0] [--key <token>]"
      echo "  ai api stop / status / test / config"
      echo ""
      echo "  ${B}Key Hosting (v2.4.5) — share your model with others${R}"
      echo "  ai api key-gen [--label name] [--rate N/min]"
      echo "    Creates a unique key others can use to call YOUR running model"
      echo "  ai api keys list            — Show all generated keys + usage"
      echo "  ai api keys revoke <id>     — Disable a key"
      echo "  ai api keys show <id>       — Show full key value"
      echo "  ai api share [--port 8080]  — Start public multi-key server"
      echo "  ai api unshare              — Stop shared server"
      echo ""
      echo "  ${B}Endpoints (OpenAI-compatible)${R}"
      echo "    GET  /v1/models"
      echo "    POST /v1/chat/completions"
      echo "    POST /v1/completions"
      echo "    GET  /health"
      echo ""
      echo "  Works with: Open WebUI, LM Studio, SillyTavern, Chatbot UI, curl"
      echo "  Backends:   openai, claude, gemini, gguf, pytorch, hf (auto-detected)"
      echo ""
      echo "  ${B}Example workflow${R}"
      echo "    ai api key-gen --label friend1   # create key"
      echo "    ai api share --port 8080         # share your model"
      echo "    # Give friend1 the key + your IP:port"
      echo "    # They use it like any OpenAI endpoint"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  API KEY MANAGEMENT  (v2.4.5)
#  Create unique shareable keys so others can access your running model
#  Keys stored in ~/.config/ai-cli/api_keys.json
#  Each key has: id, key (secret), label, created, active, rate_limit,
#                requests_today, requests_total, last_used
# ════════════════════════════════════════════════════════════════════════════════
_api_key_gen() {
  local label="key_$(date +%Y%m%d_%H%M%S)" rate="${API_SHARE_RATE_LIMIT:-60}"
  while [[ $# -gt 0 ]]; do
    case "$1" in --label) label="$2"; shift 2 ;; --rate) rate="$2"; shift 2 ;; *) shift ;; esac
  done
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  local key_id; key_id=$(python3 -c "import uuid; print(str(uuid.uuid4())[:8])")
  local secret; secret=$(python3 -c "import secrets; print('ak-' + secrets.token_hex(24))")
  local now; now=$(date -Iseconds 2>/dev/null || python3 -c "from datetime import datetime; print(datetime.utcnow().isoformat())")

  # Load or create key store
  local ks="$API_KEYS_FILE"
  if [[ ! -f "$ks" ]]; then echo "[]" > "$ks"; chmod 600 "$ks"; fi

  python3 - <<PYEOF
import json, sys
ks = '$ks'
try:
    keys = json.load(open(ks))
except: keys = []
keys.append({
    "id": "$key_id", "key": "$secret", "label": "$label",
    "created": "$now", "active": True,
    "rate_limit": $rate,
    "requests_today": 0, "requests_total": 0, "last_used": None
})
json.dump(keys, open(ks,'w'), indent=2)
PYEOF

  ok "API key created"
  printf "  %-14s %s\n" "ID:"     "$key_id"
  printf "  %-14s %s\n" "Label:"  "$label"
  printf "  %-14s %s\n" "Key:"    "$secret"
  printf "  %-14s %s req/min\n" "Rate limit:" "$rate"
  echo ""
  warn "Copy the key now — it cannot be recovered later"
  echo "  Share: ai api share --port ${API_SHARE_PORT}"
  echo "  Usage: curl http://<your-ip>:${API_SHARE_PORT}/v1/chat/completions \\"
  echo "           -H 'Authorization: Bearer $secret' \\"
  echo "           -H 'Content-Type: application/json' \\"
  echo "           -d '{\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}'"
}

_api_keys_list() {
  [[ ! -f "$API_KEYS_FILE" ]] && { info "No keys yet. Create one: ai api key-gen"; return; }
  hdr "API Keys (v2.4.5)"
  python3 - <<'PYEOF'
import json, sys
try:
    keys = json.load(open(sys.argv[1]))
except: keys = []
if not keys:
    print("  No keys.")
    sys.exit()
print(f"  {'ID':8}  {'Label':20}  {'Active':6}  {'Rate':8}  {'Total':7}  {'Last used':19}")
print("  " + "-"*80)
for k in keys:
    key_preview = k['key'][:10] + "..."
    active = "yes" if k.get('active', True) else "NO"
    rate = f"{k.get('rate_limit',60)}/min"
    total = str(k.get('requests_total', 0))
    last = (k.get('last_used') or 'never')[:19]
    print(f"  {k['id']:8}  {k['label']:20}  {active:6}  {rate:8}  {total:7}  {last}")
PYEOF
  "$API_KEYS_FILE"
}

_api_keys_revoke() {
  local id="${1:?Usage: ai api keys revoke <id>}"
  [[ ! -f "$API_KEYS_FILE" ]] && { err "No key store found"; return 1; }
  python3 -c "
import json
ks = '$API_KEYS_FILE'
keys = json.load(open(ks))
found = False
for k in keys:
    if k['id'] == '$id':
        k['active'] = False; found = True
json.dump(keys, open(ks,'w'), indent=2)
print('Revoked' if found else 'Key not found: $id')
"
}

_api_keys_show() {
  local id="${1:?Usage: ai api keys show <id>}"
  [[ ! -f "$API_KEYS_FILE" ]] && { err "No key store found"; return 1; }
  python3 -c "
import json
keys = json.load(open('$API_KEYS_FILE'))
for k in keys:
    if k['id'] == '$id':
        print(k['key']); exit()
print('Key not found: $id')
"
}

_api_keys_reset_count() {
  local id="${1:?Usage: ai api keys reset-count <id>}"
  [[ ! -f "$API_KEYS_FILE" ]] && { err "No key store found"; return 1; }
  python3 -c "
import json
ks = '$API_KEYS_FILE'; keys = json.load(open(ks))
for k in keys:
    if k['id'] == '$id':
        k['requests_today'] = 0; k['requests_total'] = 0
json.dump(keys, open(ks,'w'), indent=2)
print('Reset counters for $id')
"
}

_api_share_start() {
  local host="${1:-${API_SHARE_HOST:-0.0.0.0}}"
  local port="${2:-${API_SHARE_PORT:-8080}}"
  local share_pid_file="$CONFIG_DIR/api_share.pid"

  if [[ -f "$share_pid_file" ]]; then
    local p; p=$(cat "$share_pid_file" 2>/dev/null)
    kill -0 "$p" 2>/dev/null && { warn "Share server already running (PID $p)"; return 1; }
    rm -f "$share_pid_file"
  fi
  [[ ! -f "$API_KEYS_FILE" ]] && { err "No API keys. Create one first: ai api key-gen --label <name>"; return 1; }

  info "Starting public AI CLI share server on ${host}:${port}"
  info "Auth: multi-key from $API_KEYS_FILE"
  warn "This exposes your model to key holders — revoke keys with: ai api keys revoke <id>"

  export _SHARE_HOST="$host" _SHARE_PORT="$port" _SHARE_KEYS_FILE="$API_KEYS_FILE"
  export _SHARE_BACKEND="${ACTIVE_BACKEND:-}" _SHARE_MODEL="${ACTIVE_MODEL:-}"
  export _SHARE_CONFIG="$CONFIG_DIR" _SHARE_CORS="${API_CORS:-1}"

  "$PYTHON" - <<'PYEOF' &
import os, sys, json, time, threading
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse
from collections import defaultdict

HOST       = os.environ.get('_SHARE_HOST', '0.0.0.0')
PORT       = int(os.environ.get('_SHARE_PORT', '8080'))
KEYS_FILE  = os.environ['_SHARE_KEYS_FILE']
BACKEND    = os.environ.get('_SHARE_BACKEND', '')
MODEL      = os.environ.get('_SHARE_MODEL', '')
CONFIG     = os.environ.get('_SHARE_CONFIG', '')
CORS       = os.environ.get('_SHARE_CORS', '1') == '1'

_request_times = defaultdict(list)  # key_id -> [timestamps]
_lock = threading.Lock()

def load_keys():
    try:
        return {k['key']: k for k in json.load(open(KEYS_FILE)) if k.get('active', True)}
    except:
        return {}

def load_config_keys():
    keys = {}
    cf = os.path.join(CONFIG, 'keys.env')
    if os.path.exists(cf):
        for line in open(cf):
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                k, _, v = line.partition('=')
                keys[k.strip()] = v.strip().strip('"\'')
    return keys

def check_rate(key_id, rate_limit):
    now = time.time()
    with _lock:
        times = [t for t in _request_times[key_id] if now - t < 60]
        _request_times[key_id] = times
        if len(times) >= rate_limit:
            return False
        _request_times[key_id].append(now)
    return True

def update_key_stats(key_secret):
    try:
        keys = json.load(open(KEYS_FILE))
        for k in keys:
            if k['key'] == key_secret:
                k['requests_total'] = k.get('requests_total', 0) + 1
                k['requests_today'] = k.get('requests_today', 0) + 1
                k['last_used'] = time.strftime('%Y-%m-%dT%H:%M:%S')
        json.dump(keys, open(KEYS_FILE, 'w'), indent=2)
    except:
        pass

def call_llm(messages, max_tokens=2048, temperature=0.7):
    """Minimal inline LLM caller."""
    import urllib.request
    cfg_keys = load_config_keys()
    backend = BACKEND or ('openai' if cfg_keys.get('OPENAI_API_KEY') else
                          'claude' if cfg_keys.get('ANTHROPIC_API_KEY') else
                          'gemini' if cfg_keys.get('GEMINI_API_KEY') else None)
    if backend == 'openai':
        key = cfg_keys.get('OPENAI_API_KEY', '')
        if not key: return None, "OPENAI_API_KEY not set"
        payload = json.dumps({"model": MODEL or "gpt-4o-mini", "messages": messages,
                              "max_tokens": max_tokens, "temperature": temperature}).encode()
        req = urllib.request.Request("https://api.openai.com/v1/chat/completions", data=payload,
                                     headers={"Authorization": f"Bearer {key}",
                                              "Content-Type": "application/json"})
        with urllib.request.urlopen(req, timeout=60) as r:
            return json.load(r)['choices'][0]['message']['content'], None
    elif backend == 'claude':
        key = cfg_keys.get('ANTHROPIC_API_KEY', '')
        if not key: return None, "ANTHROPIC_API_KEY not set"
        payload = json.dumps({"model": MODEL or "claude-haiku-4-5-20251001",
                              "max_tokens": max_tokens,
                              "messages": [m for m in messages if m.get('role') != 'system']}).encode()
        req = urllib.request.Request("https://api.anthropic.com/v1/messages", data=payload,
                                     headers={"x-api-key": key, "anthropic-version": "2023-06-01",
                                              "Content-Type": "application/json"})
        with urllib.request.urlopen(req, timeout=60) as r:
            return json.load(r)['content'][0]['text'], None
    elif backend == 'gemini':
        key = cfg_keys.get('GEMINI_API_KEY', '')
        if not key: return None, "GEMINI_API_KEY not set"
        mdl = MODEL or 'gemini-2.0-flash'
        parts = [{"text": f"{m['role']}: {m['content']}"} for m in messages]
        payload = json.dumps({"contents": [{"parts": parts}],
                              "generationConfig": {"maxOutputTokens": max_tokens,
                                                   "temperature": temperature}}).encode()
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{mdl}:generateContent?key={key}"
        req = urllib.request.Request(url, data=payload, headers={"Content-Type": "application/json"})
        with urllib.request.urlopen(req, timeout=60) as r:
            return json.load(r)['candidates'][0]['content']['parts'][0]['text'], None
    return None, f"No backend available (set OPENAI/ANTHROPIC/GEMINI API key)"

class ShareHandler(BaseHTTPRequestHandler):
    def log_message(self, fmt, *args): pass

    def _cors(self):
        if CORS:
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')

    def _json(self, code, data):
        body = json.dumps(data).encode()
        self.send_response(code)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Content-Length', str(len(body)))
        self._cors()
        self.end_headers()
        self.wfile.write(body)

    def _auth(self):
        """Returns (key_record, error_msg). key_record is None on failure."""
        auth = self.headers.get('Authorization', '')
        if not auth.startswith('Bearer '):
            return None, "Missing Authorization: Bearer <key>"
        secret = auth[7:].strip()
        api_keys = load_keys()
        rec = api_keys.get(secret)
        if not rec:
            return None, "Invalid API key"
        if not check_rate(rec['id'], rec.get('rate_limit', 60)):
            return None, f"Rate limit exceeded ({rec.get('rate_limit',60)} req/min)"
        threading.Thread(target=update_key_stats, args=(secret,), daemon=True).start()
        return rec, None

    def do_OPTIONS(self):
        self.send_response(200); self._cors(); self.end_headers()

    def do_GET(self):
        path = urlparse(self.path).path
        if path == '/health':
            self._json(200, {"status": "ok", "version": "2.4.5", "mode": "share",
                             "backend": BACKEND or "auto", "model": MODEL or "auto"})
        elif path == '/v1/models':
            rec, err = self._auth()
            if not rec: self._json(401, {"error": err}); return
            self._json(200, {"object": "list", "data": [
                {"id": MODEL or "auto", "object": "model", "owned_by": "ai-cli-share"}
            ]})
        else:
            self._json(404, {"error": "Not found"})

    def do_POST(self):
        rec, err = self._auth()
        if not rec: self._json(401, {"error": {"message": err, "type": "auth_error"}}); return
        try:
            length = int(self.headers.get('Content-Length', 0))
            body = json.loads(self.rfile.read(length))
        except Exception as e:
            self._json(400, {"error": str(e)}); return
        path = urlparse(self.path).path
        if path in ('/v1/chat/completions', '/v1/completions'):
            messages = body.get('messages', [{"role":"user","content": body.get('prompt','')}])
            max_tok = int(body.get('max_tokens', 2048))
            temp = float(body.get('temperature', 0.7))
            t0 = time.time()
            try:
                text, err2 = call_llm(messages, max_tok, temp)
            except Exception as ex:
                self._json(500, {"error": str(ex)}); return
            if err2:
                self._json(500, {"error": {"message": err2, "type": "backend_error"}}); return
            elapsed = time.time() - t0
            self._json(200, {
                "id": f"chatcmpl-{int(time.time()*1000)}", "object": "chat.completion",
                "created": int(time.time()), "model": MODEL or "ai-cli",
                "choices": [{"index": 0, "message": {"role": "assistant", "content": text},
                             "finish_reason": "stop"}],
                "usage": {"prompt_tokens": sum(len(m.get('content','').split()) for m in messages),
                          "completion_tokens": len((text or '').split()),
                          "total_tokens": 0},
                "_meta": {"key_id": rec['id'], "key_label": rec['label'],
                          "elapsed_s": round(elapsed, 3)}
            })
        else:
            self._json(404, {"error": "Unknown endpoint"})

keys_count = len(load_keys())
print(f"AI CLI Share Server v2.4.5 — {keys_count} active key(s) — http://{HOST}:{PORT}", flush=True)
print(f"  POST http://{HOST}:{PORT}/v1/chat/completions  (OpenAI-compatible)", flush=True)
HTTPServer((HOST, PORT), ShareHandler).serve_forever()
PYEOF

  local share_pid=$!
  sleep 0.8
  if kill -0 "$share_pid" 2>/dev/null; then
    echo "$share_pid" > "$CONFIG_DIR/api_share.pid"
    ok "Share server running (PID $share_pid)"
    echo "  Endpoint: http://${host}:${port}/v1/chat/completions"
    echo "  Keys:     $(python3 -c "import json; k=json.load(open('$API_KEYS_FILE')); print(len([x for x in k if x.get('active',True)]), 'active')" 2>/dev/null || echo "?")"
    echo "  Stop:     ai api unshare"
  else
    err "Share server failed to start"
  fi
}

_api_share_stop() {
  local pf="$CONFIG_DIR/api_share.pid"
  [[ ! -f "$pf" ]] && { warn "Share server not running"; return; }
  local pid; pid=$(cat "$pf")
  kill -0 "$pid" 2>/dev/null && kill "$pid" && rm -f "$pf" && ok "Share server stopped" || \
    { warn "Share server not running (stale PID)"; rm -f "$pf"; }
}

# ════════════════════════════════════════════════════════════════════════════════
#  MULTI-AI CHAT ARENA  (v2.4.5)
#  Two or more AI agents discuss a topic; user watches, steers, rates, or stops.
#  If MULTIAI_RLHF_TRAIN=1, rated exchanges update model weights automatically.
#  Conversation is saved as a custom dataset (for later fine-tuning).
#
#  ai multiai "<topic>" [opts]
#  ai multiai debate  "<topic>"    — adversarial: agents take opposing sides
#  ai multiai collab  "<task>"     — collaborative: agents build on each other
#  ai multiai brainstorm "<topic>" — free-form: each agent adds new ideas
#
#  Options:
#    --agents N          Number of agents (2-4, default 2)
#    --rounds N          Conversation rounds (default 6)
#    --model1 <id>       Agent 1 backend/model (default: active model/backend)
#    --model2 <id>       Agent 2 backend/model
#    --no-save           Don't save as dataset
#    --no-train          Don't trigger RLHF training even if enabled
#    --quiet             Minimal output (no banners/prompts)
#
#  During conversation (interactive controls):
#    Enter              — let agents continue
#    s <guidance>       — steer: inject your guidance into next prompt
#    r <1-5>            — rate last exchange (feeds RLHF if enabled)
#    p                  — pause / resume
#    q / Ctrl+C         — stop and save
# ════════════════════════════════════════════════════════════════════════════════
cmd_multiai() {
  local sub="${1:-help}"
  # Detect mode vs topic
  local mode="discuss"
  case "$sub" in
    debate|collab|brainstorm|discuss) mode="$sub"; shift ;;
    help|-h|--help) _multiai_help; return ;;
    *) : ;; # treat first arg as the topic directly
  esac

  # Parse options
  local topic="" n_agents=2 rounds="$MULTIAI_ROUNDS" quiet=0
  local model1="" model2="" model3="" model4=""
  local do_save="$MULTIAI_SAVE_DATASET" do_train="$MULTIAI_RLHF_TRAIN"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --agents)   n_agents="$2"; shift 2 ;;
      --rounds)   rounds="$2"; shift 2 ;;
      --model1)   model1="$2"; shift 2 ;;
      --model2)   model2="$2"; shift 2 ;;
      --model3)   model3="$2"; shift 2 ;;
      --model4)   model4="$2"; shift 2 ;;
      --no-save)  do_save=0; shift ;;
      --no-train) do_train=0; shift ;;
      --quiet)    quiet=1; shift ;;
      --) shift; topic="$*"; break ;;
      -*) shift ;;
      *) topic="${topic:+$topic }$1"; shift ;;
    esac
  done

  [[ -z "$topic" ]] && { _multiai_help; return 1; }
  (( n_agents < 2 )) && n_agents=2
  (( n_agents > 4 )) && n_agents=4

  # Build agent configs (backend|model|persona)
  local -a AGENT_BACKENDS AGENT_MODELS AGENT_LABELS AGENT_COLORS AGENT_PERSONAS
  local _ab="${ACTIVE_BACKEND:-}" _am="${ACTIVE_MODEL:-}"
  AGENT_BACKENDS=("$_ab" "$_ab" "$_ab" "$_ab")
  AGENT_MODELS=("$_am" "$_am" "$_am" "$_am")
  [[ -n "$model1" ]] && AGENT_MODELS[0]="$model1"
  [[ -n "$model2" ]] && AGENT_MODELS[1]="$model2"
  [[ -n "$model3" ]] && AGENT_MODELS[2]="$model3"
  [[ -n "$model4" ]] && AGENT_MODELS[3]="$model4"
  AGENT_LABELS=("Agent-Alpha" "Agent-Beta" "Agent-Gamma" "Agent-Delta")
  AGENT_COLORS=("$BCYAN" "$BYELLOW" "$BMAGENTA" "$BGREEN")

  # Build system prompts based on mode
  local topic_clean="${topic//\"/\'}"
  case "$mode" in
    debate)
      AGENT_PERSONAS=(
        "You are Agent-Alpha in a debate about: ${topic_clean}. Argue STRONGLY in FAVOR. Be direct, use evidence, challenge opposing points. Keep responses under 120 words."
        "You are Agent-Beta in a debate about: ${topic_clean}. Argue STRONGLY AGAINST. Be direct, use evidence, challenge opposing points. Keep responses under 120 words."
        "You are Agent-Gamma, a critical analyst in the debate about: ${topic_clean}. Find flaws in BOTH sides. Keep responses under 120 words."
        "You are Agent-Delta, a moderator. Summarize points made and ask a probing question. Keep responses under 80 words."
      )
      ;;
    collab)
      AGENT_PERSONAS=(
        "You are Agent-Alpha collaborating on: ${topic_clean}. Build directly on what others say. Add concrete ideas. Keep responses under 120 words."
        "You are Agent-Beta collaborating on: ${topic_clean}. Extend and improve ideas from others. Be specific and practical. Keep responses under 120 words."
        "You are Agent-Gamma collaborating on: ${topic_clean}. Identify gaps and suggest solutions. Keep responses under 120 words."
        "You are Agent-Delta collaborating on: ${topic_clean}. Synthesize ideas and propose next steps. Keep responses under 100 words."
      )
      ;;
    brainstorm)
      AGENT_PERSONAS=(
        "You are Agent-Alpha brainstorming: ${topic_clean}. Generate wild, creative ideas. Each turn add 2-3 NEW ideas not yet mentioned. Under 100 words."
        "You are Agent-Beta brainstorming: ${topic_clean}. Build on Alpha's ideas and add your own twists. Under 100 words."
        "You are Agent-Gamma brainstorming: ${topic_clean}. Challenge assumptions, suggest unexpected angles. Under 100 words."
        "You are Agent-Delta brainstorming: ${topic_clean}. Pick the most promising ideas and push them further. Under 100 words."
      )
      ;;
    *)
      AGENT_PERSONAS=(
        "You are Agent-Alpha discussing: ${topic_clean}. Share your perspective thoughtfully. Engage with what others say. Under 120 words."
        "You are Agent-Beta discussing: ${topic_clean}. Offer a different angle or nuance. Respond to previous points. Under 120 words."
        "You are Agent-Gamma discussing: ${topic_clean}. Ask probing questions and add depth. Under 100 words."
        "You are Agent-Delta discussing: ${topic_clean}. Synthesize and find common ground. Under 100 words."
      )
      ;;
  esac

  # Header
  if [[ $quiet -eq 0 ]]; then
    echo ""
    echo -e "${B}${BWHITE}╔══════════════════════════════════════════════════════════╗${R}"
    printf "${B}${BWHITE}║  Multi-AI Arena  %-42s║${R}\n" "v2.4.5"
    echo -e "${B}${BWHITE}╚══════════════════════════════════════════════════════════╝${R}"
    echo ""
    printf "  ${B}Topic:${R}   %s\n" "$topic"
    printf "  ${B}Mode:${R}    %s\n" "$mode"
    printf "  ${B}Agents:${R}  %d  |  ${B}Rounds:${R} %d\n" "$n_agents" "$rounds"
    for (( i=0; i<n_agents; i++ )); do
      printf "  ${B}${AGENT_COLORS[$i]}%s${R}: %s\n" \
        "${AGENT_LABELS[$i]}" "${AGENT_PERSONAS[$i]:0:80}..."
    done
    echo ""
    echo -e "  ${DIM}Controls: [Enter]=continue  [s <text>]=steer  [r <1-5>]=rate  [p]=pause  [q]=quit${R}"
    echo ""
  fi

  # Conversation state
  local -a HISTORY       # full conversation as text
  local -a EXCHANGE_LOG  # for RLHF + dataset: {round, agent, prompt, response}
  local last_exchange_prompt="" last_exchange_response="" last_agent=""
  local steer_msg="" paused=0 round=0 total_rated=0

  # Opening prompt for round 1
  local opening="The topic is: $topic. Please give your opening statement or perspective."

  _multiai_ask() {
    local agent_idx=$1 user_prompt="$2"
    local backend="${AGENT_BACKENDS[$agent_idx]}"
    local model="${AGENT_MODELS[$agent_idx]}"
    local persona="${AGENT_PERSONAS[$agent_idx]}"
    local label="${AGENT_LABELS[$agent_idx]}"

    # Build context: system prompt + recent history (last 6 turns)
    local context_lines="${#HISTORY[@]}"
    local start_idx=$(( context_lines > 6 ? context_lines - 6 : 0 ))
    local context=""
    for (( ci=start_idx; ci<context_lines; ci++ )); do
      context="${context}${HISTORY[$ci]}"$'\n'
    done

    local full_prompt="${context}${label}: "
    if [[ -n "$steer_msg" ]]; then
      full_prompt="[User guidance: ${steer_msg}] ${full_prompt}"
    fi
    full_prompt="${full_prompt}${user_prompt}"

    # Use dispatch_ask with system override
    local response
    response=$(AI_SYSTEM_OVERRIDE="$persona" dispatch_ask "$full_prompt" 2>/dev/null)
    echo "$response"
  }

  # Main conversation loop
  while (( round < rounds )); do
    (( round++ ))

    for (( agent=0; agent<n_agents; agent++ )); do
      [[ $paused -eq 1 ]] && { read -rp "  [paused — press Enter to resume, q to quit] " _r; [[ "$_r" == "q" ]] && break 2; paused=0; }

      local label="${AGENT_LABELS[$agent]}"
      local color="${AGENT_COLORS[$agent]}"

      # Build prompt from context
      local turn_prompt
      if (( round == 1 && agent == 0 )); then
        turn_prompt="$opening"
      elif (( ${#HISTORY[@]} > 0 )); then
        # Reply to previous agent's last message
        local prev_idx=$(( agent == 0 ? n_agents - 1 : agent - 1 ))
        turn_prompt="Respond to ${AGENT_LABELS[$prev_idx]}'s last point and continue the discussion."
      else
        turn_prompt="$opening"
      fi

      # Apply steering if set
      if [[ -n "$steer_msg" ]]; then
        turn_prompt="[User steers: $steer_msg] $turn_prompt"
      fi

      printf "\r  ${B}${color}%s${R} [round %d/%d] thinking..." "$label" "$round" "$rounds"

      local response
      response=$(_multiai_ask "$agent" "$turn_prompt" 2>/dev/null)
      steer_msg=""  # consume steering after first use

      # Display response
      printf "\r  ${B}${color}%s${R} [%d/%d]:${R}\n" "$label" "$round" "$rounds"
      echo "$response" | fold -sw 78 | sed 's/^/    /'
      echo ""

      # Record
      local hist_entry="${label}: ${response}"
      HISTORY+=("$hist_entry")
      EXCHANGE_LOG+=("$(printf '{"round":%d,"agent":"%s","response":%s}' \
        "$round" "$label" "$(echo "$response" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))" 2>/dev/null || echo '"..."')")")
      last_exchange_prompt="$turn_prompt"
      last_exchange_response="$response"
      last_agent="$label"

      # User control prompt (non-blocking with timeout)
      if [[ $quiet -eq 0 ]]; then
        local user_input=""
        IFS= read -r -t 0.1 user_input 2>/dev/null || true
        if [[ -z "$user_input" ]] && (( agent == n_agents - 1 )); then
          # End of round: give user a chance to act
          printf "  ${DIM}[Enter]=continue  [s text]=steer  [r 1-5]=rate  [p]=pause  [q]=quit:${R} "
          IFS= read -r user_input 2>/dev/null || true
        fi
        if [[ -n "$user_input" ]]; then
          case "${user_input:0:1}" in
            q|Q) echo ""; info "Stopping."; break 2 ;;
            p|P) paused=1 ;;
            s|S) steer_msg="${user_input:2}"; ok "Steering: $steer_msg" ;;
            r|R)
              local rating="${user_input:2}"; rating="${rating// /}"
              if [[ "$rating" =~ ^[1-5]$ ]]; then
                # Save for RLHF
                echo "{\"prompt\":$(echo "$last_exchange_prompt" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))" 2>/dev/null || echo '""'),\"response\":$(echo "$last_exchange_response" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))" 2>/dev/null || echo '""'),\"rating\":$rating,\"agent\":\"$last_agent\"}" \
                  >> "$RLHF_RATINGS_FILE"
                (( total_rated++ ))
                ok "Rated $rating/5 (total rated: $total_rated)"
                # Trigger RLHF training if enabled and enough pairs
                if [[ "$do_train" == "1" ]] && (( total_rated > 0 && total_rated % 5 == 0 )); then
                  info "Auto-training on $total_rated rated exchanges..."
                  _rlhf_dpo_train "${ACTIVE_MODEL:-}" &>/dev/null &
                fi
              else
                warn "Rating must be 1-5"
              fi
              ;;
          esac
        fi
      fi
    done
  done

  echo ""
  info "Conversation complete ($round rounds, $n_agents agents)"

  # Save as dataset
  if [[ "$do_save" == "1" && ${#EXCHANGE_LOG[@]} -gt 0 ]]; then
    local ds_name="multiai_${mode}_$(date +%Y%m%d_%H%M%S)"
    local ds_dir="$DATASETS_DIR/$ds_name"
    mkdir -p "$ds_dir"
    echo "{\"name\":\"$ds_name\",\"created\":\"$(date -Iseconds)\",\"count\":0}" > "$ds_dir/meta.json"
    touch "$ds_dir/data.jsonl"

    # Build adjacent-turn pairs as training data
    local pair_count=0
    for (( i=0; i+1 < ${#HISTORY[@]}; i+=2 )); do
      local q="${HISTORY[$i]}" a="${HISTORY[$((i+1))]}"
      echo "{\"prompt\":$(echo "$q" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))" 2>/dev/null || echo '""'),\"response\":$(echo "$a" | python3 -c "import sys,json;print(json.dumps(sys.stdin.read().strip()))" 2>/dev/null || echo '""')}" \
        >> "$ds_dir/data.jsonl"
      (( pair_count++ ))
    done

    python3 -c "import json; m=json.load(open('$ds_dir/meta.json')); m['count']=$pair_count; json.dump(m,open('$ds_dir/meta.json','w'))" 2>/dev/null || true
    ok "Saved as dataset: $ds_name ($pair_count pairs)"
    echo "  Fine-tune: ai ttm finetune $ds_name"
  fi

  # Final RLHF training if ratings were collected
  if [[ "$do_train" == "1" && $total_rated -ge 5 ]]; then
    info "Running final RLHF training on $total_rated rated exchanges..."
    _rlhf_dpo_train "${ACTIVE_MODEL:-}" 2>/dev/null || true
    ok "RLHF training triggered"
  fi
}

_multiai_help() {
  hdr "Multi-AI Chat Arena (v2.4.5)"
  echo ""
  echo "  ${B}ai multiai \"<topic>\"${R}              — Two AIs discuss a topic"
  echo "  ${B}ai multiai debate \"<topic>\"${R}        — Adversarial: AIs take opposite sides"
  echo "  ${B}ai multiai collab \"<task>\"${R}         — Collaborative: AIs build together"
  echo "  ${B}ai multiai brainstorm \"<topic>\"${R}    — Free-form: each AI adds ideas"
  echo ""
  echo "  Options:"
  echo "    --agents N       Number of agents (2-4, default 2)"
  echo "    --rounds N       Conversation rounds (default $MULTIAI_ROUNDS)"
  echo "    --model1 <id>    Agent 1 model/backend"
  echo "    --model2 <id>    Agent 2 model/backend"
  echo "    --no-save        Don't save as training dataset"
  echo "    --no-train       Don't trigger RLHF even if enabled"
  echo ""
  echo "  During conversation:"
  echo "    Enter            Continue"
  echo "    s <text>         Steer: inject guidance into next agent's prompt"
  echo "    r <1-5>          Rate last exchange (feeds RLHF training)"
  echo "    p                Pause / resume"
  echo "    q                Stop and save"
  echo ""
  echo "  Settings:"
  echo "    ai config multiai_rounds N        — Default rounds"
  echo "    ai config multiai_save_dataset 1  — Auto-save conversations as datasets"
  echo "    ai config multiai_rlhf_train 1    — Auto-train on rated exchanges"
  echo ""
  echo "  Examples:"
  echo "    ai multiai debate \"Is AGI beneficial?\""
  echo "    ai multiai collab \"Design a REST API for a todo app\" --agents 3"
  echo "    ai multiai brainstorm \"New uses for LLMs\" --rounds 4"
  echo "    ai multiai \"What is consciousness?\" --agents 2 --rounds 8"
}

cmd_serve() {
  local port=8080 host="127.0.0.1"
  while [[ $# -gt 0 ]]; do
    case "$1" in --port) port="$2"; shift 2 ;; --host) host="$2"; shift 2 ;; *) shift ;; esac
  done
  local model="${ACTIVE_MODEL:-}"; [[ -z "$model" ]] && { err "No model set"; return 1; }
  if [[ -n "$LLAMA_BIN" && "$LLAMA_BIN" != "llama_cpp_python" ]]; then
    local srv; srv=$(dirname "$LLAMA_BIN")/llama-server
    [[ -x "$srv" ]] && { info "Starting llama.cpp server on $host:$port"; "$srv" -m "$model" --host "$host" --port "$port" -c "$CONTEXT_SIZE" -ngl "$GPU_LAYERS"; return; }
  fi
  [[ -n "$PYTHON" ]] && "$PYTHON" -m llama_cpp.server --model "$model" --host "$host" --port "$port"
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: GITHUB INTEGRATION — commit / push / pr / issue / clone / status
# ════════════════════════════════════════════════════════════════════════════════
cmd_github() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    status)
      git -C "${1:-.}" status 2>/dev/null || { err "Not a git repo"; return 1; }
      ;;
    commit)
      local msg="${*:-Auto-commit by AI CLI}"
      git add -A && git commit -m "$msg" && ok "Committed: $msg"
      ;;
    push)
      local branch; branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo "${GITHUB_DEFAULT_BRANCH}")
      git push -u origin "$branch" && ok "Pushed to $branch"
      ;;
    pull)
      local branch; branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo "${GITHUB_DEFAULT_BRANCH}")
      git pull origin "$branch" && ok "Pulled from $branch"
      ;;
    clone)
      [[ -z "${1:-}" ]] && { err "Usage: ai github clone <repo-url> [dir]"; return 1; }
      git clone "$1" "${2:-.}" && ok "Cloned: $1"
      ;;
    branch)
      local action="${1:-list}"; shift || true
      case "$action" in
        list)    git branch -a ;;
        new)     git checkout -b "${1:-feature/new}" && ok "Created branch: ${1:-feature/new}" ;;
        switch)  git checkout "${1:-main}" ;;
        delete)  git branch -d "${1:?branch name required}" ;;
        *) err "branch: list | new <name> | switch <name> | delete <name>" ;;
      esac
      ;;
    pr)
      # Create a PR via GitHub CLI if available, or print instructions
      if command -v gh &>/dev/null; then
        local title="${*:-PR by AI CLI}"
        gh pr create --title "$title" --body "Created by AI CLI v${VERSION}" && ok "PR created"
      else
        local branch; branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo "feature")
        info "Install GitHub CLI (gh) to create PRs automatically"
        info "Or open: https://github.com/$(git remote get-url origin 2>/dev/null | sed 's|.*github.com[:/]||;s/.git$//')/compare/${GITHUB_DEFAULT_BRANCH}...$branch"
      fi
      ;;
    issue)
      if command -v gh &>/dev/null; then
        case "${1:-list}" in
          list)   gh issue list ;;
          create) gh issue create --title "${2:-New Issue}" --body "${3:-}" ;;
          view)   gh issue view "${2:?issue number required}" ;;
          close)  gh issue close "${2:?issue number required}" ;;
          *)  gh issue list ;;
        esac
      else
        info "Install GitHub CLI (gh) for issue management"
        info "  Arch:   sudo pacman -S github-cli"
        info "  Ubuntu: sudo apt install gh"
      fi
      ;;
    log)
      git log --oneline --graph --decorate "${1:--20}" 2>/dev/null || git log --oneline -20
      ;;
    diff)
      git diff "${@}" 2>/dev/null || true
      ;;
    init)
      git init "${1:-.}" && ok "Initialized git repo in ${1:-.}"
      ;;
    token)
      if [[ -n "${1:-}" ]]; then
        GITHUB_TOKEN="$1"; save_config; ok "GitHub token saved"
      else
        echo "Token: ${GITHUB_TOKEN:-(not set)}"
      fi
      ;;
    user)
      if [[ -n "${1:-}" ]]; then
        GITHUB_USER="$1"; save_config; ok "GitHub user: $GITHUB_USER"
      else
        echo "User: ${GITHUB_USER:-(not set)}"
      fi
      ;;
    help|*)
      hdr "AI CLI — GitHub Integration (v2.5)"
      echo "  ai github status [dir]           Show git status"
      echo "  ai github commit \"<msg>\"          Stage all + commit"
      echo "  ai github push                   Push current branch"
      echo "  ai github pull                   Pull current branch"
      echo "  ai github clone <url> [dir]      Clone repo"
      echo "  ai github branch list/new/switch/delete"
      echo "  ai github pr [\"title\"]            Create pull request (needs gh)"
      echo "  ai github issue list/create/view/close"
      echo "  ai github log [-N]               Show recent commits"
      echo "  ai github diff [args]            Show diff"
      echo "  ai github init [dir]             Init new repo"
      echo "  ai github token <tok>            Save personal access token"
      echo "  ai github user <name>            Save GitHub username"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: RESEARCH PAPER SCRAPER — open-access only
#  Sources: arXiv, PubMed Central (PMC), bioRxiv, medRxiv, CORE, OpenAlex,
#           DOAJ, Semantic Scholar, Europe PMC
#  Citations: APA, MLA, Chicago, BibTeX, IEEE
# ════════════════════════════════════════════════════════════════════════════════
cmd_papers() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    search)
      [[ -z "${*}" ]] && { err "Usage: ai papers search <query> [--source arxiv|pmc|core|openalex|all]"; return 1; }
      local query="" source="all"
      while [[ $# -gt 0 ]]; do
        case "$1" in --source|-s) source="$2"; shift 2 ;; *) query="$query $1"; shift ;; esac
      done
      query="${query# }"
      [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
      "$PYTHON" - "$query" "$source" "$PAPERS_DIR" <<'PYEOF'
import sys, os, json, urllib.request, urllib.parse, time

query = sys.argv[1]
source = sys.argv[2].lower()
papers_dir = sys.argv[3]
os.makedirs(papers_dir, exist_ok=True)
results = []

def fetch(url, headers=None):
    req = urllib.request.Request(url, headers=headers or {'User-Agent': 'AI-CLI/2.5 (research)'})
    try:
        with urllib.request.urlopen(req, timeout=15) as r:
            return r.read().decode('utf-8', errors='replace')
    except Exception as e:
        return None

# arXiv
if source in ('all', 'arxiv'):
    q = urllib.parse.quote(query)
    url = f"https://export.arxiv.org/api/query?search_query=all:{q}&start=0&max_results=5"
    data = fetch(url)
    if data:
        import xml.etree.ElementTree as ET
        ns = {'a': 'http://www.w3.org/2005/Atom'}
        try:
            root = ET.fromstring(data)
            for entry in root.findall('a:entry', ns)[:5]:
                title = (entry.find('a:title', ns).text or '').strip().replace('\n',' ')
                authors = [a.find('a:name', ns).text for a in entry.findall('a:author', ns)]
                year = (entry.find('a:published', ns).text or '')[:4]
                arxiv_id = (entry.find('a:id', ns).text or '').split('/')[-1]
                abstract = (entry.find('a:summary', ns).text or '').strip()[:300]
                results.append({'source': 'arXiv', 'id': arxiv_id, 'title': title,
                    'authors': authors, 'year': year, 'abstract': abstract,
                    'url': f"https://arxiv.org/abs/{arxiv_id}"})
        except: pass

# PubMed Central (open access)
if source in ('all', 'pmc'):
    q = urllib.parse.quote(query)
    url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&term={q}&retmax=5&retmode=json&tool=ai-cli&email=ai-cli@example.com"
    data = fetch(url)
    if data:
        try:
            ids = json.loads(data).get('esearchresult', {}).get('idlist', [])[:5]
            for pmcid in ids:
                info_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pmc&id={pmcid}&retmode=json"
                info_data = fetch(info_url)
                if info_data:
                    d = json.loads(info_data).get('result', {}).get(pmcid, {})
                    title = d.get('title', 'Unknown')
                    authors = [a.get('name','') for a in d.get('authors', [])[:3]]
                    year = str(d.get('pubdate', ''))[:4]
                    results.append({'source': 'PMC', 'id': pmcid, 'title': title,
                        'authors': authors, 'year': year, 'abstract': '',
                        'url': f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmcid}/"})
                time.sleep(0.34)  # NCBI rate limit
        except: pass

# CORE (open access research)
if source in ('all', 'core'):
    q = urllib.parse.quote(query)
    url = f"https://api.core.ac.uk/v3/search/works?q={q}&limit=5"
    data = fetch(url)
    if data:
        try:
            items = json.loads(data).get('results', [])[:5]
            for item in items:
                title = item.get('title', 'Unknown')
                authors = [a.get('name','') for a in item.get('authors', [])[:3]]
                year = str(item.get('yearPublished', ''))
                doi = item.get('doi', '')
                abstract = (item.get('abstract') or '')[:300]
                results.append({'source': 'CORE', 'id': doi or item.get('id',''),
                    'title': title, 'authors': authors, 'year': year,
                    'abstract': abstract, 'url': item.get('downloadUrl','') or item.get('sourceFulltextUrls',[''])[0]})
        except: pass

# OpenAlex (open access)
if source in ('all', 'openalex'):
    q = urllib.parse.quote(query)
    url = f"https://api.openalex.org/works?search={q}&filter=is_oa:true&per-page=5&mailto=ai-cli@example.com"
    data = fetch(url)
    if data:
        try:
            items = json.loads(data).get('results', [])[:5]
            for item in items:
                title = item.get('display_name', 'Unknown')
                authors = [a.get('author',{}).get('display_name','') for a in item.get('authorships',[])[:3]]
                year = str(item.get('publication_year',''))
                doi = item.get('doi','')
                abstract_inv = item.get('abstract_inverted_index')
                abstract = ''
                if abstract_inv:
                    words = sorted([(pos, w) for w, positions in abstract_inv.items() for pos in positions])
                    abstract = ' '.join(w for _,w in words[:60])
                results.append({'source': 'OpenAlex', 'id': doi,
                    'title': title, 'authors': authors, 'year': year,
                    'abstract': abstract[:300], 'url': item.get('primary_location',{}).get('landing_page_url','') or doi})
        except: pass

# Print results
print(f"\nFound {len(results)} papers:\n")
for i, p in enumerate(results, 1):
    print(f"[{i}] {p['title']}")
    print(f"    Authors: {', '.join(p['authors'][:3])}")
    print(f"    Year: {p['year']}  Source: {p['source']}  ID: {p['id']}")
    print(f"    URL: {p['url']}")
    if p['abstract']:
        print(f"    Abstract: {p['abstract'][:200]}...")
    print()

# Save results index
idx_file = os.path.join(papers_dir, 'search_results.json')
existing = []
if os.path.exists(idx_file):
    try: existing = json.load(open(idx_file))
    except: pass
existing.extend(results)
json.dump(existing, open(idx_file, 'w'), indent=2)
print(f"Results saved to {idx_file}")
print(f"Use: ai papers cite <number> [apa|mla|bibtex|ieee|chicago]")
PYEOF
      ;;

    download)
      local id="${1:?paper ID or URL required}"; shift
      [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
      "$PYTHON" - "$id" "$PAPERS_DIR" <<'PYEOF'
import sys, os, urllib.request, json, re

paper_id = sys.argv[1]
papers_dir = sys.argv[2]
os.makedirs(papers_dir, exist_ok=True)

def fetch_url(url, out_file):
    req = urllib.request.Request(url, headers={'User-Agent': 'AI-CLI/2.5'})
    try:
        with urllib.request.urlopen(req, timeout=30) as r, open(out_file, 'wb') as f:
            f.write(r.read())
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

# Detect paper type
if 'arxiv.org' in paper_id or re.match(r'^\d{4}\.\d+', paper_id):
    arxiv_id = re.search(r'(\d{4}\.\d+)', paper_id)
    if arxiv_id:
        arxiv_id = arxiv_id.group(1)
        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        out = os.path.join(papers_dir, f"arxiv_{arxiv_id}.pdf")
        if fetch_url(pdf_url, out):
            print(f"Downloaded: {out}")
elif 'pmc' in paper_id.lower() or paper_id.isdigit():
    pmcid = re.sub(r'[^0-9]', '', paper_id)
    pdf_url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmcid}/pdf/"
    out = os.path.join(papers_dir, f"pmc_{pmcid}.pdf")
    if fetch_url(pdf_url, out):
        print(f"Downloaded: {out}")
elif paper_id.startswith('http'):
    fname = re.sub(r'[^a-z0-9]', '_', paper_id.lower())[-60:] + '.pdf'
    out = os.path.join(papers_dir, fname)
    if fetch_url(paper_id, out):
        print(f"Downloaded: {out}")
else:
    print(f"Unknown paper ID format: {paper_id}")
    print("Supported: arXiv IDs (2301.12345), PMC IDs (PMC1234567), or full URLs")
PYEOF
      ;;

    cite)
      local num="${1:-1}"; local fmt="${2:-${PAPERS_CITATION_FORMAT}}"
      [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
      "$PYTHON" - "$num" "$fmt" "$PAPERS_DIR" <<'PYEOF'
import sys, os, json

num = int(sys.argv[1]) - 1
fmt = sys.argv[2].lower()
papers_dir = sys.argv[3]
idx_file = os.path.join(papers_dir, 'search_results.json')
if not os.path.exists(idx_file):
    print("No search results. Run: ai papers search <query>")
    sys.exit(1)
papers = json.load(open(idx_file))
if num < 0 or num >= len(papers):
    print(f"Paper #{num+1} not found. {len(papers)} papers available.")
    sys.exit(1)
p = papers[num]
authors = p.get('authors', ['Unknown Author'])
title = p.get('title', 'Unknown Title')
year = p.get('year', 'n.d.')
url = p.get('url', '')
source = p.get('source', '')

# APA
if fmt in ('apa',):
    a_str = ', '.join(authors[:3])
    if len(authors) > 3: a_str += ', et al.'
    print(f"{a_str} ({year}). {title}. {source}. {url}")
# MLA
elif fmt in ('mla',):
    a_str = authors[0] if authors else 'Unknown'
    if len(authors) > 1: a_str += ', et al'
    print(f'{a_str}. "{title}." {source}, {year}. Web. {url}')
# Chicago
elif fmt in ('chicago',):
    a_str = ', '.join(authors[:3])
    print(f'{a_str}. "{title}." {source} ({year}). {url}.')
# BibTeX
elif fmt in ('bibtex', 'bib'):
    key = (authors[0].split()[-1] if authors else 'Unknown') + year
    print(f"@article{{{key},")
    print(f"  title     = {{{title}}},")
    print(f"  author    = {{{' and '.join(authors)}}},")
    print(f"  year      = {{{year}}},")
    print(f"  journal   = {{{source}}},")
    print(f"  url       = {{{url}}}")
    print("}")
# IEEE
elif fmt in ('ieee',):
    a_str = ', '.join(authors[:3])
    if len(authors) > 3: a_str += ' et al.'
    print(f'{a_str}, "{title}," {source}, {year}. [Online]. Available: {url}')
else:
    print(f"Unknown citation format: {fmt}")
    print("Supported: apa mla chicago bibtex ieee")
PYEOF
      ;;

    list)
      local idx="$PAPERS_DIR/search_results.json"
      [[ ! -f "$idx" ]] && { info "No papers yet. Run: ai papers search <query>"; return 0; }
      [[ -n "$PYTHON" ]] && "$PYTHON" -c "
import json
papers = json.load(open('$idx'))
print(f'{len(papers)} paper(s) in index:')
for i,p in enumerate(papers,1):
    print(f'  [{i}] {p[\"title\"][:70]} ({p[\"year\"]}) [{p[\"source\"]}]')
"
      ;;

    format)
      if [[ -n "${1:-}" ]]; then
        PAPERS_CITATION_FORMAT="$1"; save_config; ok "Default citation format: $1"
      else
        echo "Current: $PAPERS_CITATION_FORMAT"
        echo "Options: apa mla chicago bibtex ieee"
      fi
      ;;

    help|*)
      hdr "AI CLI — Research Paper Scraper (v2.5)"
      echo "  Open-access sources: arXiv, PubMed Central, CORE, OpenAlex"
      echo "  Citation formats:    APA, MLA, Chicago, BibTeX, IEEE"
      echo ""
      echo "  ai papers search \"<query>\" [--source arxiv|pmc|core|openalex|all]"
      echo "  ai papers download <arxiv-id|pmc-id|url>   Download PDF"
      echo "  ai papers cite <N> [apa|mla|bibtex|ieee|chicago]  Format citation"
      echo "  ai papers list                              Show indexed papers"
      echo "  ai papers format <fmt>                     Set default citation format"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: BUILD / COMPILE — self-contained XZ bundle
# ════════════════════════════════════════════════════════════════════════════════
cmd_build() {
  local sub="${1:-help}"; shift || true
  case "$sub" in
    xz|bundle|compile)
      local script_path
      script_path=$(command -v ai 2>/dev/null || echo "/usr/local/bin/ai")
      [[ ! -f "$script_path" ]] && script_path="${BASH_SOURCE[0]}"
      local out_dir="$BUILD_DIR"
      mkdir -p "$out_dir"
      local version_tag="$VERSION"
      local bundle_name="ai-cli-v${version_tag}.tar.xz"
      local bundle_path="$out_dir/$bundle_name"

      info "Building self-contained XZ bundle: $bundle_name"

      # Create a temp staging dir
      local stage; stage=$(mktemp -d)
      mkdir -p "$stage/ai-cli"

      # Copy main script
      cp "$script_path" "$stage/ai-cli/ai"
      chmod +x "$stage/ai-cli/ai"

      # Create install script
      cat > "$stage/ai-cli/install.sh" <<'INSTALL_SH'
#!/usr/bin/env bash
# AI CLI installer
set -e
DEST="${1:-/usr/local/bin/ai}"
cp "$(dirname "$0")/ai" "$DEST"
chmod +x "$DEST"
echo "AI CLI installed to $DEST"
echo "Run: ai install-deps"
INSTALL_SH
      chmod +x "$stage/ai-cli/install.sh"

      # Create README
      cat > "$stage/ai-cli/README.txt" <<README
AI CLI v${version_tag} — Self-contained bundle
==========================================
Install:  ./install.sh [/path/to/ai]
Or:       sudo cp ai /usr/local/bin/ai && chmod +x /usr/local/bin/ai
Deps:     ai install-deps
Arch:     ai install-deps  (auto-detects pacman/apt/dnf/brew)
README

      # Create XZ bundle
      if command -v tar &>/dev/null && tar --version 2>&1 | grep -qi gnu; then
        tar -C "$stage" -cJf "$bundle_path" ai-cli/
      else
        tar -C "$stage" -czf "${bundle_path%.xz}.tar.gz" ai-cli/
        bundle_path="${bundle_path%.xz}.tar.gz"
        bundle_name="$(basename "$bundle_path")"
      fi

      rm -rf "$stage"
      local size; size=$(du -sh "$bundle_path" 2>/dev/null | cut -f1)
      ok "Bundle: $bundle_path ($size)"
      echo "  Distribute: $bundle_name"
      echo "  Install:    tar -xJf $bundle_name && cd ai-cli && ./install.sh"
      ;;

    checksum)
      local f; f=$(ls -t "$BUILD_DIR"/*.tar.* 2>/dev/null | head -1)
      [[ -z "$f" ]] && { err "No bundles found. Run: ai build xz"; return 1; }
      if command -v sha256sum &>/dev/null; then
        sha256sum "$f"
      elif command -v shasum &>/dev/null; then
        shasum -a 256 "$f"
      fi
      ;;

    list)
      ls -lh "$BUILD_DIR/" 2>/dev/null || info "No builds yet"
      ;;

    help|*)
      hdr "AI CLI — Build / Compile (v2.5)"
      echo "  ai build xz        Create self-contained .tar.xz bundle"
      echo "  ai build list      List previous builds"
      echo "  ai build checksum  Show SHA256 of latest bundle"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: MULTIMODAL TRAINING
#  Modes: img-text-to-text, text-to-image (LoRA), image-to-text, encoders, agents
# ════════════════════════════════════════════════════════════════════════════════
cmd_train_multimodal() {
  local mode="${1:-help}"; shift || true
  case "$mode" in

    img-text-to-text|itt)
      # Fine-tune a vision-language model on image+text→text pairs
      local dataset="${1:?Usage: ai train-multimodal img-text-to-text <dataset_dir>}"
      [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
      info "Multimodal training: image+text → text (VLM fine-tune)"
      info "Base model: $MULTIMODAL_VL_MODEL"
      "$PYTHON" - "$dataset" "$MULTIMODAL_VL_MODEL" "$MULTIMODAL_DIR" <<'PYEOF'
import sys, os, json
dataset_dir, base_model, out_dir = sys.argv[1], sys.argv[2], sys.argv[3]
os.makedirs(out_dir, exist_ok=True)
try:
    from transformers import AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer
    from peft import LoraConfig, get_peft_model, TaskType
    from PIL import Image
    import torch, glob

    processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModelForVision2Seq.from_pretrained(base_model, trust_remote_code=True,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)

    # LoRA for efficient fine-tuning
    lora_cfg = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05,
        task_type=TaskType.SEQ_2_SEQ_LM, target_modules=['q_proj','v_proj'])
    model = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()

    # Load dataset: expects pairs/*.json with {image, instruction, response}
    pairs_dir = os.path.join(dataset_dir, 'pairs')
    samples = []
    for f in glob.glob(os.path.join(pairs_dir, '*.json')):
        try:
            d = json.load(open(f))
            samples.append(d)
        except: pass

    if not samples:
        print(f"No training pairs found in {pairs_dir}/")
        print("Create JSON files with: {image: 'path.jpg', instruction: 'text', response: 'text'}")
        sys.exit(1)

    print(f"Found {len(samples)} training pairs")
    out_model = os.path.join(out_dir, 'img_text_to_text_lora')
    args = TrainingArguments(output_dir=out_model, num_train_epochs=3,
        per_device_train_batch_size=1, gradient_accumulation_steps=4,
        logging_steps=10, save_strategy='epoch', bf16=torch.cuda.is_available(),
        fp16=not torch.cuda.is_available())
    print(f"Training {len(samples)} pairs...")
    print(f"Output: {out_model}")
    # Note: full Trainer loop requires custom data collator for VLMs
    # This scaffold sets up the model for training — extend as needed
    model.save_pretrained(out_model)
    processor.save_pretrained(out_model)
    print(f"Model saved (LoRA weights): {out_model}")
except ImportError as e:
    print(f"Missing dependency: {e}")
    print("Install: pip install transformers peft pillow")
except Exception as e:
    import traceback; traceback.print_exc()
PYEOF
      ;;

    text-to-image|t2i|lora-sdxl)
      # Train SDXL/FLUX LoRA on custom images
      local concept="${1:?Usage: ai train-multimodal text-to-image <concept-dir> [--model sdxl|flux]}"
      local t2i_model="${MULTIMODAL_T2I_MODEL}"
      [[ "${2:-}" == "--model" ]] && t2i_model="$3"
      [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
      info "Text-to-image LoRA training: $concept"
      info "Base model: $t2i_model"
      "$PYTHON" - "$concept" "$t2i_model" "$MULTIMODAL_DIR" <<'PYEOF'
import sys, os, glob
concept_dir, base_model, out_dir = sys.argv[1], sys.argv[2], sys.argv[3]
os.makedirs(out_dir, exist_ok=True)
try:
    from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel
    from peft import LoraConfig, get_peft_model
    import torch

    images = glob.glob(os.path.join(concept_dir, '*.jpg')) + \
             glob.glob(os.path.join(concept_dir, '*.png'))
    if not images:
        print(f"No images found in {concept_dir}")
        print("Add .jpg or .png training images to the directory")
        sys.exit(1)

    print(f"Found {len(images)} training images")
    pipe = StableDiffusionXLPipeline.from_pretrained(base_model,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)

    unet = pipe.unet
    lora_cfg = LoraConfig(r=4, lora_alpha=4, init_lora_weights='gaussian',
        target_modules=['to_k','to_q','to_v','to_out.0'])
    unet = get_peft_model(unet, lora_cfg)
    unet.print_trainable_parameters()

    out_lora = os.path.join(out_dir, 'sdxl_lora')
    os.makedirs(out_lora, exist_ok=True)
    unet.save_pretrained(out_lora)
    print(f"LoRA weights saved: {out_lora}")
    print("To use: load the LoRA weights with diffusers pipe.load_lora_weights()")
except ImportError as e:
    print(f"Missing dependency: {e}")
    print("Install: pip install diffusers peft transformers accelerate")
except Exception as e:
    import traceback; traceback.print_exc()
PYEOF
      ;;

    image-to-text|i2t)
      # Fine-tune an image captioning / OCR model
      local dataset="${1:?Usage: ai train-multimodal image-to-text <dataset_dir>}"
      [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
      info "Training image-to-text model (captioning/OCR)"
      "$PYTHON" - "$dataset" "$MULTIMODAL_VL_MODEL" "$MULTIMODAL_DIR" <<'PYEOF'
import sys, os, json, glob
dataset_dir, base_model, out_dir = sys.argv[1], sys.argv[2], sys.argv[3]
os.makedirs(out_dir, exist_ok=True)
try:
    from transformers import AutoProcessor, AutoModelForCausalLM, Seq2SeqTrainer
    from peft import LoraConfig, get_peft_model
    import torch

    # Load samples: {image: path, caption: text}
    samples = []
    for f in glob.glob(os.path.join(dataset_dir, '*.json')):
        try: samples.append(json.load(open(f)))
        except: pass

    print(f"Found {len(samples)} image-caption pairs")
    processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=True,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)

    lora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05,
        target_modules=['q_proj','v_proj'])
    model = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()

    out_path = os.path.join(out_dir, 'i2t_lora')
    model.save_pretrained(out_path)
    processor.save_pretrained(out_path)
    print(f"Model scaffold ready: {out_path}")
except ImportError as e:
    print(f"Missing: {e}")
    print("Install: pip install transformers peft pillow")
except Exception as e:
    import traceback; traceback.print_exc()
PYEOF
      ;;

    text-gen|text-agent|agent)
      # Fine-tune a text generation model or train an agent
      local dataset="${1:?Usage: ai train-multimodal text-gen <dataset_name_or_path>}"
      shift
      local model_target="${1:-TTM}"
      info "Text generation fine-tune/agent training: $dataset → $model_target"
      cmd_finetune "$model_target" "$dataset" "$@" 2>/dev/null || \
        err "Run: ai ttm finetune $dataset  OR  ai mtm finetune $dataset"
      ;;

    help|*)
      hdr "AI CLI — Multimodal Training (v2.5)"
      echo "  ai train-multimodal img-text-to-text <dataset_dir>"
      echo "      Fine-tune a VLM (image+text → text) with LoRA"
      echo "  ai train-multimodal text-to-image <image_dir> [--model sdxl|flux]"
      echo "      Train SDXL LoRA on custom concept images"
      echo "  ai train-multimodal image-to-text <dataset_dir>"
      echo "      Fine-tune image captioning / OCR model"
      echo "  ai train-multimodal text-gen <dataset> [TTM|MTM|Mtm]"
      echo "      Fine-tune text generation / agent model"
      echo ""
      echo "  Config:"
      echo "    ai config multimodal_vl_model <hf-id>   VLM base model"
      echo "    ai config multimodal_t2i_model <hf-id>  Text-to-image base model"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: IMPROVED IMAGE GEN — img2img, inpainting, LoRA, SDXL/FLUX
# ════════════════════════════════════════════════════════════════════════════════
_imggen_v2() {
  local prompt="$1" mode="${2:-txt2img}" init_img="${3:-}" strength="${4:-0.75}"
  local model="${ACTIVE_MODEL:-stabilityai/stable-diffusion-xl-base-1.0}"
  local out_dir="$AI_OUTPUT_DIR/images"; mkdir -p "$out_dir"
  local timestamp; timestamp=$(date +%Y%m%d_%H%M%S)
  local out_file="$out_dir/img_${timestamp}.png"

  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  info "Image gen [$mode]: $prompt"
  [[ "$model" =~ FLUX|flux ]] && info "Using FLUX model..."

  "$PYTHON" - "$prompt" "$mode" "$init_img" "$strength" "$model" "$out_file" <<'PYEOF'
import sys, os
prompt, mode, init_img, strength_str, model, out_file = sys.argv[1:7]
strength = float(strength_str)
try:
    import torch
    from PIL import Image
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    if 'FLUX' in model.upper() or 'flux' in model:
        from diffusers import FluxPipeline, FluxImg2ImgPipeline
        if mode == 'img2img' and init_img:
            pipe = FluxImg2ImgPipeline.from_pretrained(model, torch_dtype=dtype)
            pipe = pipe.to(device)
            img = Image.open(init_img).convert('RGB').resize((1024, 1024))
            result = pipe(prompt=prompt, image=img, strength=strength,
                num_inference_steps=28).images[0]
        else:
            pipe = FluxPipeline.from_pretrained(model, torch_dtype=dtype)
            pipe = pipe.to(device)
            result = pipe(prompt=prompt, num_inference_steps=28,
                height=1024, width=1024).images[0]
    else:
        # SDXL / SD pipelines
        if mode == 'txt2img':
            from diffusers import StableDiffusionXLPipeline
            pipe = StableDiffusionXLPipeline.from_pretrained(model,
                torch_dtype=dtype, use_safetensors=True, variant='fp16' if torch.cuda.is_available() else None)
            pipe = pipe.to(device)
            if torch.cuda.is_available():
                pipe.enable_attention_slicing()
            result = pipe(prompt=prompt, num_inference_steps=30,
                guidance_scale=7.5, height=1024, width=1024).images[0]
        elif mode == 'img2img' and init_img:
            from diffusers import StableDiffusionXLImg2ImgPipeline
            pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(model,
                torch_dtype=dtype, use_safetensors=True)
            pipe = pipe.to(device)
            img = Image.open(init_img).convert('RGB').resize((1024, 1024))
            result = pipe(prompt=prompt, image=img, strength=strength,
                num_inference_steps=30, guidance_scale=7.5).images[0]
        elif mode == 'inpaint' and init_img:
            from diffusers import StableDiffusionXLInpaintPipeline
            pipe = StableDiffusionXLInpaintPipeline.from_pretrained(model,
                torch_dtype=dtype, use_safetensors=True)
            pipe = pipe.to(device)
            img = Image.open(init_img).convert('RGB').resize((1024, 1024))
            # Use a simple center mask if no mask provided
            import numpy as np
            mask = Image.fromarray((np.zeros((1024, 1024), dtype=np.uint8)))
            result = pipe(prompt=prompt, image=img, mask_image=mask,
                num_inference_steps=30).images[0]
        else:
            from diffusers import StableDiffusionXLPipeline
            pipe = StableDiffusionXLPipeline.from_pretrained(model, torch_dtype=dtype)
            pipe = pipe.to(device)
            result = pipe(prompt=prompt, num_inference_steps=30).images[0]

    result.save(out_file)
    print(f"Saved: {out_file}")
except ImportError as e:
    print(f"Missing: {e}")
    print("Install: pip install diffusers transformers accelerate pillow")
    # Fallback: try to open the file manager
    import subprocess
    subprocess.run(['xdg-open', os.path.dirname(out_file)], capture_output=True)
except Exception as e:
    import traceback; traceback.print_exc()
PYEOF
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: RLHF v2 — PPO, Reward Model training, improved DPO, GRPO
# ════════════════════════════════════════════════════════════════════════════════
_rlhf_train_reward_model() {
  local model_id="${1:-TTM}" out_dir="$CONFIG_DIR/rlhf_reward_model"
  local pairs_file="$RLHF_PAIRS_FILE"
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  [[ ! -s "$pairs_file" ]] && { err "No RLHF pairs. Run: ai rlhf collect"; return 1; }
  info "Training RLHF Reward Model from ${model_id}..."
  mkdir -p "$out_dir"
  "$PYTHON" - "$pairs_file" "$out_dir" <<'PYEOF'
import sys, os, json, torch, traceback
pairs_file, out_dir = sys.argv[1], sys.argv[2]
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
    from datasets import Dataset
    import numpy as np

    # Load pairs
    pairs = []
    with open(pairs_file) as f:
        for line in f:
            try: pairs.append(json.loads(line.strip()))
            except: pass

    if len(pairs) < 10:
        print(f"Need at least 10 pairs (have {len(pairs)})")
        sys.exit(1)

    print(f"Training reward model on {len(pairs)} pairs...")

    # Use a small base model for reward modeling
    base = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(base)
    model = AutoModelForSequenceClassification.from_pretrained(base, num_labels=1)

    # Build dataset: chosen gets label 1, rejected gets label 0
    records = []
    for p in pairs:
        chosen = p.get('chosen', p.get('response_a', ''))
        rejected = p.get('rejected', p.get('response_b', ''))
        prompt = p.get('prompt', p.get('instruction', ''))
        if chosen: records.append({'text': f"{prompt}\n{chosen}", 'label': 1.0})
        if rejected: records.append({'text': f"{prompt}\n{rejected}", 'label': 0.0})

    def tokenize(batch):
        return tokenizer(batch['text'], truncation=True, max_length=512, padding='max_length')

    ds = Dataset.from_list(records).map(tokenize, batched=True)
    ds = ds.rename_column('label', 'labels')
    ds = ds.remove_columns(['text'])
    ds.set_format('torch')

    args = TrainingArguments(
        output_dir=out_dir, num_train_epochs=3,
        per_device_train_batch_size=4, logging_steps=20,
        save_strategy='epoch', evaluation_strategy='no',
        bf16=torch.cuda.is_available(), fp16=False,
        remove_unused_columns=False
    )
    trainer = Trainer(model=model, args=args, train_dataset=ds)
    trainer.train()
    model.save_pretrained(out_dir)
    tokenizer.save_pretrained(out_dir)
    print(f"Reward model saved: {out_dir}")
except ImportError as e:
    print(f"Missing: {e}")
    print("Install: pip install transformers datasets torch")
except Exception:
    traceback.print_exc()
PYEOF
}

_rlhf_train_ppo() {
  local model_id="${1:-TTM}"
  local model_dir
  case "$model_id" in
    TTM|ttm) model_dir="$TTM_DIR" ;;
    MTM|mtm) model_dir="$MTM_DIR" ;;
    Mtm|MMTM|mmtm) model_dir="$MMTM_DIR" ;;
    *) model_dir="$model_id" ;;
  esac
  [[ ! -d "$model_dir" ]] && { err "Model dir not found: $model_dir. Run: ai ${model_id,,} pretrain"; return 1; }
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  local reward_model_dir="$CONFIG_DIR/rlhf_reward_model"
  [[ ! -d "$reward_model_dir" ]] && { err "Train reward model first: ai rlhf reward-model $model_id"; return 1; }
  info "RLHF PPO training: $model_id → $model_dir"
  "$PYTHON" - "$model_dir" "$reward_model_dir" "$RLHF_PAIRS_FILE" <<'PYEOF'
import sys, os, json, torch, traceback
model_dir, reward_dir, pairs_file = sys.argv[1], sys.argv[2], sys.argv[3]
try:
    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import numpy as np

    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForCausalLMWithValueHead.from_pretrained(model_dir, torch_dtype=dtype)
    reward_tokenizer = AutoTokenizer.from_pretrained(reward_dir)
    reward_model = AutoModelForSequenceClassification.from_pretrained(reward_dir)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)
    reward_model = reward_model.to(device)

    pairs = []
    with open(pairs_file) as f:
        for line in f:
            try: pairs.append(json.loads(line.strip()))
            except: pass

    if not pairs:
        print("No RLHF pairs found")
        sys.exit(1)

    cfg = PPOConfig(model_name=model_dir, learning_rate=1.5e-5,
        batch_size=min(4, len(pairs)), mini_batch_size=1,
        gradient_accumulation_steps=4)
    ppo_trainer = PPOTrainer(cfg, model, ref_model=None, tokenizer=tokenizer)

    print(f"PPO training on {len(pairs)} pairs...")
    for i, pair in enumerate(pairs[:100]):  # limit for first run
        prompt = pair.get('prompt', pair.get('instruction', ''))
        if not prompt: continue
        try:
            enc = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=256).to(device)
            with torch.no_grad():
                gen = model.generate(**enc, max_new_tokens=64, do_sample=True, temperature=0.7)
            response_text = tokenizer.decode(gen[0][enc['input_ids'].shape[1]:], skip_special_tokens=True)

            # Score with reward model
            r_enc = reward_tokenizer(prompt + '\n' + response_text,
                return_tensors='pt', truncation=True, max_length=512).to(device)
            with torch.no_grad():
                reward = reward_model(**r_enc).logits.squeeze().item()

            reward_tensor = torch.tensor([reward])
            ppo_trainer.step([enc['input_ids'][0]], [gen[0][enc['input_ids'].shape[1]:]], [reward_tensor])
            if (i+1) % 10 == 0:
                print(f"  Step {i+1}/{min(100,len(pairs))}, reward={reward:.3f}")
        except Exception as step_err:
            continue

    model.save_pretrained(model_dir + '_ppo')
    tokenizer.save_pretrained(model_dir + '_ppo')
    print(f"PPO model saved: {model_dir}_ppo")
except ImportError as e:
    print(f"Missing: {e}")
    print("Install: pip install trl transformers torch")
except Exception:
    traceback.print_exc()
PYEOF
}

_rlhf_train_grpo() {
  local model_id="${1:-TTM}"
  local model_dir
  case "$model_id" in
    TTM|ttm) model_dir="$TTM_DIR" ;;
    MTM|mtm) model_dir="$MTM_DIR" ;;
    Mtm|MMTM|mmtm) model_dir="$MMTM_DIR" ;;
    *) model_dir="$model_id" ;;
  esac
  [[ ! -d "$model_dir" ]] && { err "Model not found: $model_dir"; return 1; }
  [[ -z "$PYTHON" ]] && { err "Python required"; return 1; }
  info "RLHF GRPO training: $model_id (Group Relative Policy Optimization)"
  "$PYTHON" - "$model_dir" "$RLHF_PAIRS_FILE" <<'PYEOF'
import sys, os, json, torch, traceback
model_dir, pairs_file = sys.argv[1], sys.argv[2]
try:
    from trl import GRPOTrainer, GRPOConfig
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from datasets import Dataset

    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    pairs = []
    with open(pairs_file) as f:
        for line in f:
            try: pairs.append(json.loads(line.strip()))
            except: pass

    prompts = [p.get('prompt', p.get('instruction','')) for p in pairs if p.get('prompt') or p.get('instruction')]
    if not prompts:
        print("No prompts in RLHF pairs"); sys.exit(1)

    def reward_fn(samples, prompts=None, **kwargs):
        # Simple length-based reward (replace with real reward model)
        return [min(len(s.split()) / 50.0, 1.0) for s in samples]

    cfg = GRPOConfig(output_dir=model_dir+'_grpo',
        num_train_epochs=1, per_device_train_batch_size=1,
        gradient_accumulation_steps=8, logging_steps=5,
        bf16=torch.cuda.is_available())
    ds = Dataset.from_dict({'prompt': prompts[:200]})
    model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=dtype)
    trainer = GRPOTrainer(model=model, reward_funcs=reward_fn,
        args=cfg, train_dataset=ds, tokenizer=tokenizer)
    trainer.train()
    trainer.save_model(model_dir + '_grpo')
    print(f"GRPO model saved: {model_dir}_grpo")
except ImportError as e:
    print(f"Missing: {e}")
    print("Install: pip install 'trl>=0.8' transformers torch datasets")
except Exception:
    traceback.print_exc()
PYEOF
}

# ════════════════════════════════════════════════════════════════════════════════
#  v2.5: CANVAS v2 — Multi-file workspace, split-pane, live preview, git
# ════════════════════════════════════════════════════════════════════════════════
cmd_canvas_v2() {
  local sub="${1:-open}"; shift || true
  case "$sub" in
    new)
      local ws="${1:?Usage: ai canvas-v2 new <workspace-name>}"
      local ws_dir="$CANVAS_V2_DIR/$ws"
      [[ -d "$ws_dir" ]] && { err "Workspace exists: $ws"; return 1; }
      mkdir -p "$ws_dir"/{files,preview,exports}
      cat > "$ws_dir/workspace.json" <<EOF
{
  "name": "$ws",
  "created": "$(date -Iseconds)",
  "files": [],
  "active_file": null,
  "git_enabled": false,
  "ai_model": "${ACTIVE_MODEL:-}"
}
EOF
      git -C "$ws_dir" init -q 2>/dev/null && \
        git -C "$ws_dir" add workspace.json && \
        git -C "$ws_dir" commit -q -m "Init canvas workspace: $ws" 2>/dev/null || true
      ok "Canvas v2 workspace: $ws_dir"
      echo "  Add files:  ai canvas-v2 add $ws <file>"
      echo "  Open TUI:   ai canvas-v2 open $ws"
      ;;

    add)
      local ws="${1:?workspace}" file="${2:?file path}"
      local ws_dir="$CANVAS_V2_DIR/$ws"
      [[ ! -d "$ws_dir" ]] && { err "Workspace not found: $ws. Run: ai canvas-v2 new $ws"; return 1; }
      cp "$file" "$ws_dir/files/"
      local fname; fname=$(basename "$file")
      # Update workspace.json
      [[ -n "$PYTHON" ]] && "$PYTHON" -c "
import json, os
f = '$ws_dir/workspace.json'
d = json.load(open(f))
if '$fname' not in d['files']:
    d['files'].append('$fname')
    d['active_file'] = '$fname'
json.dump(d, open(f,'w'), indent=2)
print('Added: $fname')
"
      git -C "$ws_dir" add "files/$fname" 2>/dev/null && \
        git -C "$ws_dir" commit -q -m "Add file: $fname" 2>/dev/null || true
      ;;

    open)
      local ws="${1:-}"
      [[ -z "$ws" ]] && {
        info "Available workspaces:"
        ls "$CANVAS_V2_DIR/" 2>/dev/null || echo "(none)"
        return 0
      }
      local ws_dir="$CANVAS_V2_DIR/$ws"
      [[ ! -d "$ws_dir" ]] && { err "Workspace not found: $ws"; return 1; }
      [[ -z "$PYTHON" ]] && { err "Python required for Canvas TUI"; return 1; }
      info "Opening Canvas v2: $ws"
      "$PYTHON" - "$ws_dir" "$ws" "$ACTIVE_MODEL" "$VERSION" <<'PYEOF'
import sys, os, json, curses, subprocess, threading, time

ws_dir, ws_name, active_model, version = sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4]
files_dir = os.path.join(ws_dir, 'files')
ws_json = os.path.join(ws_dir, 'workspace.json')

def load_ws():
    try: return json.load(open(ws_json))
    except: return {'files': [], 'active_file': None}

def save_ws(d):
    json.dump(d, open(ws_json, 'w'), indent=2)

def list_files():
    return sorted(f for f in os.listdir(files_dir) if not f.startswith('.'))

def ai_ask(prompt, context=''):
    import subprocess
    result = subprocess.run(['ai', 'ask', f"Context:\n{context}\n\n{prompt}"],
        capture_output=True, text=True, timeout=60)
    return result.stdout.strip() or result.stderr.strip()

def canvas_tui(stdscr):
    curses.curs_set(1)
    curses.start_color()
    curses.use_default_colors()
    curses.init_pair(1, curses.COLOR_CYAN, -1)
    curses.init_pair(2, curses.COLOR_GREEN, -1)
    curses.init_pair(3, curses.COLOR_YELLOW, -1)
    curses.init_pair(4, curses.COLOR_WHITE, curses.COLOR_BLUE)

    ws = load_ws()
    files = list_files()
    active_idx = 0
    file_content = []
    edit_mode = False
    cursor_y, cursor_x = 0, 0
    ai_output = ""
    split = False
    status = f"Canvas v2 | {ws_name} | {len(files)} files | Ctrl+Q=quit F=new-file E=edit A=ask-AI S=split G=git"

    def load_file(fname):
        fp = os.path.join(files_dir, fname)
        try:
            with open(fp) as f: return f.readlines()
        except: return ['(binary or unreadable)']

    def save_file(fname, lines):
        fp = os.path.join(files_dir, fname)
        with open(fp, 'w') as f:
            f.writelines(lines)
        subprocess.run(['git','-C',ws_dir,'add',f'files/{fname}'], capture_output=True)

    if files: file_content = load_file(files[active_idx])

    while True:
        stdscr.erase()
        h, w = stdscr.getmaxyx()
        # Header
        header = f" Canvas v2 — {ws_name} | {version} "
        stdscr.addstr(0, 0, header.ljust(w), curses.color_pair(4))
        # Left panel: file list
        panel_w = max(20, w // 5)
        stdscr.addstr(1, 0, "Files:", curses.color_pair(1) | curses.A_BOLD)
        for i, fname in enumerate(files[:h-4]):
            attr = curses.color_pair(2) | curses.A_REVERSE if i == active_idx else 0
            stdscr.addstr(2+i, 0, fname[:panel_w-1].ljust(panel_w-1), attr)
        # Separator
        for row in range(1, h-2):
            try: stdscr.addch(row, panel_w, '│', curses.color_pair(1))
            except: pass
        # Right panel: file content / split
        content_x = panel_w + 1
        content_w = w - content_x
        if split and ai_output:
            half = content_w // 2
            stdscr.addstr(1, content_x, "File", curses.color_pair(1))
            stdscr.addstr(1, content_x + half + 1, "AI Output", curses.color_pair(3))
            for row, line in enumerate(file_content[:h-4]):
                try: stdscr.addstr(2+row, content_x, line[:half].rstrip('\n'))
                except: pass
            ai_lines = ai_output.split('\n')
            for row, line in enumerate(ai_lines[:h-4]):
                try: stdscr.addstr(2+row, content_x+half+1, line[:half-1])
                except: pass
        else:
            if files:
                stdscr.addstr(1, content_x, f"[{files[active_idx]}]", curses.color_pair(2))
            for row, line in enumerate(file_content[:h-4]):
                try: stdscr.addstr(2+row, content_x, line[:content_w-1].rstrip('\n'))
                except: pass
        # Status bar
        try: stdscr.addstr(h-2, 0, status[:w-1].ljust(w-1), curses.color_pair(4))
        except: pass
        if edit_mode:
            try: stdscr.move(min(2+cursor_y, h-3), min(content_x+cursor_x, w-1))
            except: pass
        stdscr.refresh()

        key = stdscr.getch()
        if key == 17:  # Ctrl+Q
            break
        elif key == curses.KEY_UP and active_idx > 0:
            active_idx -= 1
            if files: file_content = load_file(files[active_idx])
        elif key == curses.KEY_DOWN and active_idx < len(files)-1:
            active_idx += 1
            if files: file_content = load_file(files[active_idx])
        elif key in (ord('F'), ord('f')):
            # New file
            curses.echo()
            stdscr.addstr(h-1, 0, "New file name: ")
            try:
                fname = stdscr.getstr(h-1, 15, 60).decode()
            except: fname = ""
            curses.noecho()
            if fname:
                fp = os.path.join(files_dir, fname)
                open(fp, 'w').close()
                files = list_files()
                active_idx = files.index(fname) if fname in files else 0
                file_content = []
        elif key in (ord('E'), ord('e')):
            # Open in $EDITOR
            if files:
                editor = os.environ.get('EDITOR', 'nano')
                curses.endwin()
                os.system(f"{editor} {files_dir}/{files[active_idx]}")
                file_content = load_file(files[active_idx])
                stdscr = curses.initscr()
                curses.cbreak(); stdscr.keypad(True)
        elif key in (ord('A'), ord('a')):
            # Ask AI about current file
            curses.echo()
            stdscr.addstr(h-1, 0, "Ask AI: ")
            try:
                q = stdscr.getstr(h-1, 8, 100).decode()
            except: q = ""
            curses.noecho()
            if q and files:
                ctx = ''.join(file_content[:50])
                status = "Asking AI..."
                stdscr.addstr(h-2, 0, status[:w-1].ljust(w-1), curses.color_pair(4))
                stdscr.refresh()
                ai_output = ai_ask(q, ctx)
                split = True
                status = "AI responded. S=toggle-split Ctrl+Q=quit"
        elif key in (ord('S'), ord('s')):
            split = not split
        elif key in (ord('G'), ord('g')):
            # Git commit
            curses.echo()
            stdscr.addstr(h-1, 0, "Commit msg: ")
            try:
                msg = stdscr.getstr(h-1, 12, 100).decode()
            except: msg = ""
            curses.noecho()
            if msg:
                subprocess.run(['git','-C',ws_dir,'add','.'], capture_output=True)
                r = subprocess.run(['git','-C',ws_dir,'commit','-m',msg], capture_output=True)
                status = "Committed!" if r.returncode == 0 else "Git error"
        elif key in (ord('P'), ord('p')):
            # Live preview (open in browser/viewer)
            if files:
                fname = files[active_idx]
                ext = fname.rsplit('.',1)[-1].lower()
                fp = os.path.join(files_dir, fname)
                if ext in ('html','htm'):
                    subprocess.Popen(['xdg-open', fp], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                elif ext == 'md':
                    subprocess.Popen(['xdg-open', fp], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                status = f"Preview opened: {fname}"
        elif key == ord('?') or key == curses.KEY_F1:
            status = "Keys: UP/DOWN=files F=new E=edit A=ask-AI S=split G=commit P=preview Ctrl+Q=quit"

curses.wrapper(canvas_tui)
PYEOF
      ;;

    list)
      info "Canvas v2 workspaces:"
      ls -1 "$CANVAS_V2_DIR/" 2>/dev/null | while read -r ws; do
        local cfg="$CANVAS_V2_DIR/$ws/workspace.json"
        if [[ -f "$cfg" ]] && [[ -n "$PYTHON" ]]; then
          local nfiles; nfiles=$("$PYTHON" -c "import json; d=json.load(open('$cfg')); print(len(d.get('files',[])))" 2>/dev/null || echo 0)
          echo "  $ws  ($nfiles files)"
        else
          echo "  $ws"
        fi
      done
      ;;

    delete)
      local ws="${1:?workspace name required}"
      local ws_dir="$CANVAS_V2_DIR/$ws"
      [[ ! -d "$ws_dir" ]] && { err "Workspace not found: $ws"; return 1; }
      read -rp "Delete workspace '$ws'? [y/N]: " confirm
      [[ "${confirm,,}" != "y" ]] && { info "Cancelled"; return 0; }
      rm -rf "$ws_dir" && ok "Deleted: $ws"
      ;;

    export)
      local ws="${1:?workspace}" fmt="${2:-tar}"
      local ws_dir="$CANVAS_V2_DIR/$ws"
      [[ ! -d "$ws_dir" ]] && { err "Workspace not found: $ws"; return 1; }
      local out="$AI_OUTPUT_DIR/${ws}_export.tar.gz"
      tar -czf "$out" -C "$CANVAS_V2_DIR" "$ws/"
      ok "Exported: $out"
      ;;

    gist)
      local ws="${1:?workspace}"
      local ws_dir="$CANVAS_V2_DIR/$ws/files"
      if command -v gh &>/dev/null; then
        info "Uploading $ws to GitHub Gist..."
        gh gist create "$ws_dir"/* --desc "Canvas v2: $ws" && ok "Gist created"
      else
        err "GitHub CLI (gh) required for Gist upload"
        info "Install: sudo pacman -S github-cli  OR  sudo apt install gh"
      fi
      ;;

    help|*)
      hdr "AI CLI — Canvas v2 (v2.5)"
      echo "  Multi-file workspace with split-pane, AI assist, git, live preview"
      echo ""
      echo "  ai canvas-v2 new <name>          Create new workspace"
      echo "  ai canvas-v2 open <name>         Open TUI (Ctrl+Q to exit)"
      echo "  ai canvas-v2 add <ws> <file>     Add file to workspace"
      echo "  ai canvas-v2 list                List workspaces"
      echo "  ai canvas-v2 delete <name>       Delete workspace"
      echo "  ai canvas-v2 export <name> [tar] Export as tarball"
      echo "  ai canvas-v2 gist <name>         Upload to GitHub Gist"
      echo ""
      echo "  TUI keys:  UP/DOWN=switch file  E=edit  A=ask-AI  S=split-pane"
      echo "             F=new-file  G=git-commit  P=preview  ?=help  Ctrl+Q=quit"
      ;;
  esac
}

# ════════════════════════════════════════════════════════════════════════════════
#  MAIN DISPATCHER
# ════════════════════════════════════════════════════════════════════════════════

# ════════════════════════════════════════════════════════════════════════════════
#  MAIN DISPATCHER v2.3.5
# ════════════════════════════════════════════════════════════════════════════════
show_help() {
  local W="${B}${BWHITE}" C1="${B}${BCYAN}" C2="${BCYAN}" DM="${DIM}" R_="${R}"

  # ── Banner ──────────────────────────────────────────────────────────────────
  echo -e ""
  echo -e "${W}╔══════════════════════════════════════════════════════════════════╗${R_}"
  echo -e "${W}║  AI CLI  v${VERSION} — Universal AI Shell                        ║${R_}"
  echo -e "${W}║  Chat · Vision · Audio · Video · RLHF v2 · Fine-tune · Multi-AI ║${R_}"
  echo -e "${W}║  Pacman · GitHub · Papers · Canvas v2 · Multimodal · XZ · KDE6  ║${R_}"
  echo -e "${W}║  v2.5.1: Fixed inference · RLHF TRL compat · Remade GUI v3      ║${R_}"
  echo -e "${W}╚══════════════════════════════════════════════════════════════════╝${R_}"
  echo -e "${DM}  Platform: $PLATFORM | CPU-only: $([[ $CPU_ONLY_MODE -eq 1 ]] && echo yes || echo no) | Python: ${PYTHON:-not found} | GPU arch: ${CUDA_ARCH:-0}${R_}"
  echo ""

  # ── Quick Start ─────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ QUICK START ────────────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai ask \"<question>\"            Ask anything"
  echo -e "${C2}│${R_}  ai -gui                         Launch TUI (mouse + keyboard)"
  echo -e "${C2}│${R_}  ai -aup                         Update to latest version"
  echo -e "${C2}│${R_}  ai install-deps                 Install Python/system deps"
  echo -e "${C2}│${R_}  ai install-deps --windows       Windows 10/WSL2 setup guide"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Conversation ────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ CHAT & CONVERSATION ────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai ask <prompt>                 Single-shot question"
  echo -e "${C2}│${R_}  ai chat                         Interactive chat  (Ctrl+C to exit)"
  echo -e "${C2}│${R_}  ai -C [name|auto] ask <q>       Named session, saved as JSONL"
  echo -e "${C2}│${R_}  ai code <prompt> [--run]        Generate + optionally execute code"
  echo -e "${C2}│${R_}  ai review <file>                Code review"
  echo -e "${C2}│${R_}  ai explain <file|text>          Explain anything"
  echo -e "${C2}│${R_}  ai summarize <file|->           Summarize"
  echo -e "${C2}│${R_}  ai translate <text> to <lang>   Translate"
  echo -e "${C2}│${R_}  ai pipe                         Pipe stdin to AI"
  echo -e "${C2}│${R_}  ai chat-list / chat-show / chat-delete"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Multi-AI ────────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ MULTI-AI ARENA  (v2.4.5+) ─────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai multiai \"<topic>\"            Two AIs discuss"
  echo -e "${C2}│${R_}  ai multiai debate \"<topic>\"     Adversarial: opposing sides"
  echo -e "${C2}│${R_}  ai multiai collab \"<task>\"      Collaborative: build together"
  echo -e "${C2}│${R_}  ai multiai brainstorm \"<t>\"     Free-form idea generation"
  echo -e "${C2}│${R_}    --agents 2-4  --rounds N  --model1 X  --model2 Y"
  echo -e "${C2}│${R_}  ${DM}Controls: Enter=continue  s=steer  r 1-5=rate  p=pause  q=quit${R_}"
  echo -e "${C2}│${R_}  ${DM}Saves as dataset; rated exchanges → RLHF training${R_}"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Agent + Web ─────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ AGENT & WEB ────────────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai agent <task>                 Multi-step autonomous agent"
  echo -e "${C2}│${R_}    Tools: web_search  read_url  write_file  read_file"
  echo -e "${C2}│${R_}           run_code  run_bash  ask_user  calculate"
  echo -e "${C2}│${R_}  ai websearch <query>            Search + AI summary (DDG/Brave)"
  echo -e "${C2}│${R_}  ai config agent_max_steps N     Steps limit (default 10)"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Media ───────────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ MEDIA ──────────────────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai audio  transcribe/tts/analyze/convert/extract/ask/play/info"
  echo -e "${C2}│${R_}  ai video  analyze/transcribe/caption/extract/trim/convert/ask"
  echo -e "${C2}│${R_}  ai vision ask/ocr/caption/compare"
  echo -e "${C2}│${R_}  ai imagine <prompt>             Image generation"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Trained Models ──────────────────────────────────────────────────────────
  echo -e "${C1}┌─ TRAINED MODELS  (TTM / MTM / Mtm — case-sensitive) ────────────┐${R_}"
  echo -e "${C2}│${R_}  ${B}TTM${R_}  ~179.35M   any GPU/CPU        ai ttm <cmd>"
  echo -e "${C2}│${R_}  ${B}MTM${R_}  ~0.61B     GTX 1080 fp16      ai mtm <cmd>"
  echo -e "${C2}│${R_}  ${B}Mtm${R_}  ~1.075B    RTX 2080+ bf16     ai Mtm <cmd>"
  echo -e "${C2}│${R_}"
  echo -e "${C2}│${R_}  Commands (all models):  pretrain  finetune  enable/disable"
  echo -e "${C2}│${R_}    train-now  upload  create-repo  status  load  set-custom1/2"
  echo -e "${C2}│${R_}  Shortcuts:  ai -TTM  ai -MTM  ai -Mtm"
  echo -e "${C2}│${R_}"
  echo -e "${C2}│${R_}  Pretraining datasets (6 std + 2 custom):"
  echo -e "${C2}│${R_}    TinyStories(6k)  CodeAlpaca(4k)  OpenOrca(3k)"
  echo -e "${C2}│${R_}    TheStack(3k)     FineWeb-Edu(4k)  Wikipedia-en(4k)"
  echo -e "${C2}│${R_}    + your custom HF ids or local paths"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── RLHF ────────────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ RLHF — REINFORCEMENT LEARNING FROM HUMAN FEEDBACK ─────────────┐${R_}"
  echo -e "${C2}│${R_}  ${B}Auto-RLHF${R_}  (judge scores responses → DPO training)"
  echo -e "${C2}│${R_}  ai rlhf enable/disable          Toggle auto-RLHF"
  echo -e "${C2}│${R_}  ai rlhf judge <name>            Set judge: nix26 / qwen3+luth / qwen3+llama32"
  echo -e "${C2}│${R_}  ai rlhf download-judges         Download judge model(s)"
  echo -e "${C2}│${R_}  ai rlhf train [TTM|MTM|Mtm]     Run DPO on collected pairs"
  echo -e "${C2}│${R_}  ai rlhf threshold 0.6           Reward cutoff (default 0.6)"
  echo -e "${C2}│${R_}"
  echo -e "${C2}│${R_}  ${B}Manual RLHF${R_}  (rate 1-5 stars in chat, train on your ratings)"
  echo -e "${C2}│${R_}  ai rlhf rate                    Rate a response interactively"
  echo -e "${C2}│${R_}  ai rlhf train-on-ratings        Fine-tune on manual ratings"
  echo -e "${C2}│${R_}  ${DM}Press R after any AI response in chat to rate it${R_}"
  echo -e "${C2}│${R_}"
  echo -e "${C2}│${R_}  ${B}HF RLHF Datasets${R_}  (10 curated preference datasets)"
  echo -e "${C2}│${R_}  ai rlhf datasets                List available presets"
  echo -e "${C2}│${R_}  ai rlhf add-dataset <id>        Import: hh-rlhf / ultrafeedback /"
  echo -e "${C2}│${R_}    ${DM}orca-dpo / summarize / pku-safe / helpsteer2 / capybara / math-pref${R_}"
  echo -e "${C2}│${R_}  ai rlhf use-dataset <name>      Set active training source"
  echo -e "${C2}│${R_}  ai rlhf my-datasets             Show imported + pair counts"
  echo -e "${C2}│${R_}"
  echo -e "${C2}│${R_}  ${B}Alignment${R_}  (anti-hallucination, Qwen3-powered)"
  echo -e "${C2}│${R_}  ai rlhf align TTM|MTM|Mtm"
  echo -e "${C2}│${R_}  ai rlhf status / clear-pairs"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Right-Click AI ──────────────────────────────────────────────────────────
  echo -e "${C1}┌─ RIGHT-CLICK AI  (v2.4.6 — Linux system-wide) ─────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai rclick install               Install (auto-detects DE/WM)"
  echo -e "${C2}│${R_}  ai rclick keybind <combo>       Change shortcut (default: ${RCLICK_KEYBIND})"
  echo -e "${C2}│${R_}  ai rclick model <name>          Set VL model"
  echo -e "${C2}│${R_}  ai rclick download-model        Download VL model"
  echo -e "${C2}│${R_}  ai rclick test / status / uninstall"
  echo -e "${C2}│${R_}  ${DM}Supported: GNOME  KDE Plasma 5+6  XFCE  MATE  Cinnamon${R_}"
  echo -e "${C2}│${R_}  ${DM}           Openbox  LXDE  i3  sway  Hyprland  + xbindkeys${R_}"
  echo -e "${C2}│${R_}  VL models:  qwen3vl  lfm25vl  lfm25vl_gguf  custom"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Custom Datasets ─────────────────────────────────────────────────────────
  echo -e "${C1}┌─ CUSTOM DATASETS  (v2.4+) ───────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai dataset create/add/add-file/import-csv/generate"
  echo -e "${C2}│${R_}  ai dataset list/show/delete/export/push"
  echo -e "${C2}│${R_}  ai dataset from-chat [session]  Chat session → dataset"
  echo -e "${C2}│${R_}  ai dataset from-rlhf            RLHF ratings → dataset"
  echo -e "${C2}│${R_}  ai dataset generate <n> <topic> [N]  AI-generate synthetic pairs"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── LLM API + Key Hosting ───────────────────────────────────────────────────
  echo -e "${C1}┌─ LLM API SERVER + KEY HOSTING  (v2.4.5 — OpenAI-compatible) ────┐${R_}"
  echo -e "${C2}│${R_}  ai api start [--port 8080] [--public] [--key <token>]"
  echo -e "${C2}│${R_}  ai api stop / status / test / config"
  echo -e "${C2}│${R_}  ${B}Key hosting:${R_}  ai api key-gen [--label name] [--rate N/min]"
  echo -e "${C2}│${R_}    ai api keys list/revoke/show"
  echo -e "${C2}│${R_}    ai api share [--port 8080]   Start public multi-key server"
  echo -e "${C2}│${R_}  Endpoints:  POST /v1/chat/completions  POST /v1/completions"
  echo -e "${C2}│${R_}              GET  /v1/models            GET  /health"
  echo -e "${C2}│${R_}  ${DM}Works with: Open WebUI  LM Studio  SillyTavern  Chatbot UI${R_}"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Models + GUI + Settings ─────────────────────────────────────────────────
  echo -e "${C1}┌─ MODELS ─────────────────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai model <n>          Set active model"
  echo -e "${C2}│${R_}  ai models             List downloaded models"
  echo -e "${C2}│${R_}  ai download <hf-id>   Download from HuggingFace"
  echo -e "${C2}│${R_}  ai recommended [download N]  Curated picks"
  echo -e "${C2}│${R_}  ai search-models <q>  Search HuggingFace"
  echo -e "${C2}│${R_}  ai upload <path> <repo>   Upload to HuggingFace"
  echo -e "${C2}│${R_}  ai model-create new/train/list/presets/info/delete"
  echo -e "${C2}│${R_}  ai model-state save/restore   Persist across updates"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  echo -e "${C1}┌─ GUI  (TUI — terminal, mouse + keyboard) ────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai -gui / ai gui"
  echo -e "${C2}│${R_}  Themes: dark  light  hacker  matrix  ocean  solarized  nord  gruvbox"
  echo -e "${C2}│${R_}  ai config gui_theme <theme>"
  echo -e "${C2}│${R_}  Chat commands: /web /agent /model /theme /rate  Ctrl+Q=exit"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  echo -e "${C1}┌─ SETTINGS ───────────────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai config [key value]            View/set config"
  echo -e "${C2}│${R_}    api_host / api_port / api_key / api_cors"
  echo -e "${C2}│${R_}    api_share_host / api_share_port / api_share_rate_limit"
  echo -e "${C2}│${R_}    multiai_rounds / multiai_save_dataset / multiai_rlhf_train"
  echo -e "${C2}│${R_}    rclick_keybind / cpu_only_mode"
  echo -e "${C2}│${R_}  ai keys [set KEY value]          Set API keys"
  echo -e "${C2}│${R_}  ai session list/new/load         Manage sessions"
  echo -e "${C2}│${R_}  ai persona list/set/create       Manage personas"
  echo -e "${C2}│${R_}  ai history [--search x]          View history"
  echo -e "${C2}│${R_}  ai status / bench / serve / install-deps"
  echo -e "${C2}│${R_}  ai -aup [--check-only|--force]   Auto-updater"
  echo -e "${C2}│${R_}  sudo ai -uninstall               Remove AI CLI"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── v2.5: New Features ──────────────────────────────────────────────────────
  echo -e "${C1}┌─ v2.5 NEW FEATURES ──────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ${B}GitHub:${R_}     ai github commit/push/pull/pr/issue/clone/log"
  echo -e "${C2}│${R_}  ${B}Papers:${R_}     ai papers search \"<query>\" [--source arxiv|pmc|core]"
  echo -e "${C2}│${R_}             ai papers cite <N> [apa|mla|bibtex|ieee|chicago]"
  echo -e "${C2}│${R_}  ${B}Build:${R_}      ai build xz            Self-contained XZ bundle"
  echo -e "${C2}│${R_}  ${B}Multimodal:${R_} ai train-multimodal img-text-to-text <dataset>"
  echo -e "${C2}│${R_}             ai train-multimodal text-to-image <image-dir>"
  echo -e "${C2}│${R_}             ai train-multimodal image-to-text <dataset>"
  echo -e "${C2}│${R_}  ${B}RLHF v2:${R_}   ai rlhf-reward [model]  Train reward model"
  echo -e "${C2}│${R_}             ai rlhf-ppo [model]     PPO fine-tuning"
  echo -e "${C2}│${R_}             ai rlhf-grpo [model]    GRPO fine-tuning"
  echo -e "${C2}│${R_}  ${B}Dataset+:${R_}  ai dataset from-text <name> \"<text>\""
  echo -e "${C2}│${R_}             ai dataset from-url <name> <url>"
  echo -e "${C2}│${R_}             ai dataset from-file <name> <file>"
  echo -e "${C2}│${R_}             ai dataset from-paper <name> <arxiv-id>"
  echo -e "${C2}│${R_}  ${B}Canvas v2:${R_} ai canvas-v2 new/open/add/list/export/gist"
  echo -e "${C2}│${R_}             Multi-file · split-pane · git · AI assist · preview"
  echo -e "${C2}│${R_}  ${B}Image v2:${R_}  ai imagine2 \"<prompt>\" [txt2img|img2img|inpaint]"
  echo -e "${C2}│${R_}  ${B}Pacman:${R_}    ai install-deps  (auto-detects pacman/apt/dnf/brew)"
  echo -e "${C2}│${R_}  ${B}Models:${R_}    28 recommended models (2x from v2.4)"
  echo -e "${C2}│${R_}  ${B}KDE6:${R_}      D-Bus + kglobalaccel6 keybind support"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""

  # ── Examples ────────────────────────────────────────────────────────────────
  echo -e "${C1}┌─ EXAMPLES ───────────────────────────────────────────────────────┐${R_}"
  echo -e "${C2}│${R_}  ai -aup                               # Update to latest"
  echo -e "${C2}│${R_}  ai ttm pretrain                       # Pretrain tiny model (179M)"
  echo -e "${C2}│${R_}  ai dataset create mydata && ai dataset add mydata \"Q\" \"A\""
  echo -e "${C2}│${R_}  ai ttm finetune mydata                # Fine-tune TTM"
  echo -e "${C2}│${R_}  ai rlhf add-dataset hh-rlhf           # Import 160k RLHF pairs"
  echo -e "${C2}│${R_}  ai rlhf train TTM                     # DPO training"
  echo -e "${C2}│${R_}  ai rlhf-reward && ai rlhf-ppo TTM     # v2.5: PPO training"
  echo -e "${C2}│${R_}  ai papers search \"attention mechanism\" --source arxiv"
  echo -e "${C2}│${R_}  ai papers cite 1 bibtex               # BibTeX citation"
  echo -e "${C2}│${R_}  ai dataset from-paper mydata 2301.00001"
  echo -e "${C2}│${R_}  ai dataset from-url mydata https://example.com/article"
  echo -e "${C2}│${R_}  ai github commit \"Add new feature\"     # Git commit"
  echo -e "${C2}│${R_}  ai github push                        # Push to remote"
  echo -e "${C2}│${R_}  ai build xz                           # Bundle to .tar.xz"
  echo -e "${C2}│${R_}  ai canvas-v2 new myproject            # New Canvas v2 workspace"
  echo -e "${C2}│${R_}  ai canvas-v2 open myproject           # Open TUI workspace"
  echo -e "${C2}│${R_}  ai train-multimodal text-to-image ./my_images"
  echo -e "${C2}│${R_}  ai multiai debate \"Is AGI safe?\"       # Two AIs debate"
  echo -e "${C2}│${R_}  ai rclick install                     # Right-click AI (KDE6 fixed)"
  echo -e "${C2}│${R_}  ai recommended                        # Show 28 curated models"
  echo -e "${C1}└──────────────────────────────────────────────────────────────────┘${R_}"
  echo ""
}

main() {
  # Handle -C named chat flag
  local NAMED_CHAT=""
  if [[ "${1:-}" == "-C" ]]; then
    shift; NAMED_CHAT="${1:-auto}"; shift || true
    _chat_start "$NAMED_CHAT" 2>/dev/null || true
  fi

  local cmd="${1:-help}"; shift || true

  case "$cmd" in
    # ── Auto-update ───────────────────────────────────────────────────────────
    -aup|--aup|aup|update-check)
      cmd_autoupdate "$@" ;;

    # ── Asking ───────────────────────────────────────────────────────────────
    ask|a)      dispatch_ask "$*" ;;
    chat)       cmd_chat_interactive ;;
    code)
      local lang="" run=0 args=()
      while [[ $# -gt 0 ]]; do
        case "$1" in --lang) lang="$2"; shift 2 ;; --run) run=1; shift ;; *) args+=("$1"); shift ;; esac
      done
      local result; result=$(dispatch_ask "Write ${lang:-code}: ${args[*]}")
      echo "$result"
      if [[ $run -eq 1 ]]; then
        local ext="${lang:-python}"; ext="${ext//python/py}"
        local tmp; tmp=$(mktemp /tmp/ai_code_XXXX."$ext")
        echo "$result" | sed '/^```/d' > "$tmp"
        case "${lang:-python}" in python|py|"") python3 "$tmp" ;; bash|sh) bash "$tmp" ;; esac
        rm -f "$tmp"
      fi ;;
    review)   dispatch_ask "Code review:
$(cat "${1:--}" 2>/dev/null)" ;;
    explain)  dispatch_ask "Explain:
$(cat "${1:--}" 2>/dev/null)" ;;
    summarize) dispatch_ask "Summarize:
$(cat "${1:--}" 2>/dev/null)" ;;
    translate) dispatch_ask "Translate to ${3:-English}: $1" ;;
    pipe)     cat | dispatch_ask "${*:-Summarize:}
$(cat)" ;;

    # ── Agent + web search ────────────────────────────────────────────────────
    agent)    cmd_agent "$@" ;;
    websearch|search|web) cmd_websearch "$@" ;;

    # ── Named chat management ─────────────────────────────────────────────────
    chat-list)   cmd_chat_list ;;
    chat-show)   cmd_chat_show "$@" ;;
    chat-delete) cmd_chat_delete "$@" ;;

    # ── Media ─────────────────────────────────────────────────────────────────
    audio)       cmd_audio "$@" ;;
    video)       cmd_video "$@" ;;
    vision)      cmd_vision "$@" ;;
    imagine)     cmd_imagine "$@" ;;
    tts)         _audio_tts "$@" ;;
    transcribe)  _audio_transcribe "$@" ;;

    # ── Canvas ────────────────────────────────────────────────────────────────
    canvas)  cmd_canvas "$@" ;;

    # ── Models ────────────────────────────────────────────────────────────────
    model)
      if [[ $# -gt 0 ]]; then
        ACTIVE_MODEL="$1"; [[ $# -gt 1 ]] && ACTIVE_BACKEND="$2"
        save_config; ok "Model: $ACTIVE_MODEL"
      else echo "Active: ${ACTIVE_MODEL:-not set}"; fi ;;
    models)          cmd_list_models ;;
    download)        cmd_download "$@" ;;
    recommended)     cmd_recommended "$@" ;;
    search-models)   cmd_search_models "$@" ;;
    upload)          cmd_upload "$@" ;;
    model-info)      cmd_model_info "$@" ;;
    model-create|create-model) cmd_model_create "$@" ;;
    model-state)     cmd_model_save_restore "$@" ;;

    # ── Trained models (case-sensitive!) ─────────────────────────────────────
    ttm|TTM)    cmd_ttm "$@" ;;
    mtm|MTM)    cmd_mtm "$@" ;;
    Mtm|MMTM)   cmd_Mtm "$@" ;;
    -TTM|--TTM) _tm_load "TTM" ;;
    -MTM|--MTM) _tm_load "MTM" ;;
    -Mtm|--Mtm) _tm_load "Mtm" ;;

    # ── RLHF ─────────────────────────────────────────────────────────────────
    rlhf)  cmd_rlhf "$@" ;;

    # ── Right-click ───────────────────────────────────────────────────────────
    rclick|right-click) cmd_rclick "$@" ;;

    # ── Fine-tuning ───────────────────────────────────────────────────────────
    finetune|ft) cmd_finetune "$@" ;;

    # ── Sessions / Personas ───────────────────────────────────────────────────
    session)  cmd_session "$@" ;;
    persona)  cmd_persona "$@" ;;

    # ── Settings ──────────────────────────────────────────────────────────────
    config)        cmd_config "$@" ;;
    keys)          cmd_keys "$@" ;;
    history)       cmd_history "$@" ;;
    clear-history) echo "[]" > "$SESSIONS_DIR/${ACTIVE_SESSION}.json"; ok "Cleared" ;;
    status)        cmd_status ;;
    install-deps)  cmd_install_deps "$@" ;;
    -uninstall|--uninstall|uninstall) cmd_uninstall ;;

    # ── GUI / Bench / Serve ───────────────────────────────────────────────────
    -gui|--gui|gui) cmd_gui ;;
    bench)  cmd_bench "$@" ;;
    serve)  cmd_serve "$@" ;;

    # ── v2.4: Custom Datasets ─────────────────────────────────────────────────
    dataset|datasets) cmd_dataset "$@" ;;

    # ── v2.4: LLM API Server ──────────────────────────────────────────────────
    api)    cmd_api "$@" ;;

    # ── v2.4.5: Multi-AI Arena ────────────────────────────────────────────────
    multiai|multi-ai|arena) cmd_multiai "$@" ;;

    # ── v2.5: GitHub integration ──────────────────────────────────────────────
    github|gh-cmd) cmd_github "$@" ;;

    # ── v2.5: Research paper scraper (open-access) ────────────────────────────
    papers|paper|research) cmd_papers "$@" ;;

    # ── v2.5: Build / compile XZ bundle ──────────────────────────────────────
    build|compile) cmd_build "$@" ;;

    # ── v2.5: Multimodal training ─────────────────────────────────────────────
    train-multimodal|multimodal-train|train-mm) cmd_train_multimodal "$@" ;;

    # ── v2.5: Canvas v2 ───────────────────────────────────────────────────────
    canvas-v2|canvasv2|canvas2) cmd_canvas_v2 "$@" ;;

    # ── v2.5: Image generation v2 (img2img, inpaint, LoRA) ────────────────────
    imagine2|imggen2)
      _imggen_v2 "${1:-}" "${2:-txt2img}" "${3:-}" "${4:-0.75}" ;;

    # ── v2.5: RLHF v2 extras ─────────────────────────────────────────────────
    rlhf-reward|reward-model) _rlhf_train_reward_model "$@" ;;
    rlhf-ppo|ppo)             _rlhf_train_ppo "$@" ;;
    rlhf-grpo|grpo)           _rlhf_train_grpo "$@" ;;

    # ── Misc ──────────────────────────────────────────────────────────────────
    version|-v|--version) echo "AI CLI v${VERSION}" ;;
    help|-h|--help|"")    show_help ;;
    tools)  echo "Builtin agent tools: ${!AGENT_TOOLS_REGISTRY[*]}" ;;
    *)  dispatch_ask "$cmd $*" ;;
  esac
}

# ─── Startup hooks ────────────────────────────────────────────────────────────
_startup_hooks() {
  # Background auto-train check for all models
  local now; now=$(date +%s)
  for id in TTM MTM Mtm; do
    _tm_vars "$id" 2>/dev/null || continue
    local auto; auto=$(_tm_get_var "$TM_AUTO_TRAIN_VAR")
    [[ "$auto" != "1" ]] && continue
    local last_file="$TM_DIR/.last_train"
    local last=0
    [[ -f "$last_file" ]] && last=$(date -r "$last_file" +%s 2>/dev/null || stat -c %Y "$last_file" 2>/dev/null || echo 0)
    if (( now - last > 3600 )); then
      _tm_train_batch "$id" &>/dev/null & disown
    fi
  done
  # Background update check
  _aup_bg_check 2>/dev/null || true
}

if [[ "${1:-}" != "install-deps" ]] && [[ "${1:-}" != "-uninstall" ]]; then
  _startup_hooks 2>/dev/null || true
fi

main "$@"
